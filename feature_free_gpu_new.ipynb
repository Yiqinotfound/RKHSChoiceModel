{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import json \n",
    "import pandas as pd \n",
    "from scipy.special import gamma, kv\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters \n",
    "instance_id = 5\n",
    "\n",
    "# kernel_type = 'matern'\n",
    "# kernel_params = {'length_scale': 1.0, 'nu':2.0, 'sigma': 1.0}\n",
    "\n",
    "# kernel_type = 'gaussian'\n",
    "# kernel_params = {'length_scale': 1.0, 'sigma': 1.0}\n",
    "\n",
    "\n",
    "# kernel_type = 'sigmoid'\n",
    "# kernel_params = {'alpha': 0.5, 'c': 1.0, 'sigma':1.0}\n",
    "\n",
    "\n",
    "# kernel_type = 'laplace'\n",
    "# kernel_params = {'length_scale': 1.0}\n",
    "\n",
    "kernel_type = 'poly'\n",
    "kernel_params = {'c': 1, 'degree': 2}\n",
    "\n",
    "# loss_type = 'mse'\n",
    "loss_type = 'cross_entrophy'\n",
    "\n",
    "kernel_params_str = \"_\".join([f\"{key}={value}\" for key, value in kernel_params.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read Data, Convert to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_json_to_df(json_file_path:str):\n",
    "    with open(json_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    in_sample_transactions = data[\"transactions\"][\"in_sample_transactions\"]\n",
    "    out_sample_transactions = data[\"transactions\"][\"out_of_sample_transactions\"]\n",
    "    product_labels = data['product_labels']\n",
    "\n",
    "    in_sample_transactions = pd.DataFrame(in_sample_transactions)\n",
    "    out_sample_transactions = pd.DataFrame(out_sample_transactions)\n",
    "    \n",
    "    # rename 'prodcut' to 'choice' \n",
    "    in_sample_transactions.rename(columns={'product':'choice'}, inplace=True)\n",
    "    out_sample_transactions.rename(columns={'product':'choice'}, inplace=True)\n",
    "    product_labels = pd.DataFrame(\n",
    "        list(product_labels.items()), columns=[\"product_id\", \"product_name\"]\n",
    "    )\n",
    "    return in_sample_transactions, out_sample_transactions,product_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_list_to_one_hot(transaction:list,d):\n",
    "    one_hot = np.zeros(d)\n",
    "    for item in transaction:\n",
    "        one_hot[item] = 1\n",
    "    return one_hot\n",
    "\n",
    "def convert_to_one_hot(transactions:pd.DataFrame,d):\n",
    "    transactions[\"offered_product_one_hot\"] = transactions['offered_products'].apply(lambda x : convert_list_to_one_hot(x,d))\n",
    "    transactions['choice_one_hot'] = transactions['choice'].apply(lambda x: convert_list_to_one_hot([x],d))\n",
    "    return transactions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offered_products</th>\n",
       "      <th>choice</th>\n",
       "      <th>offered_product_one_hot</th>\n",
       "      <th>choice_one_hot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6]</td>\n",
       "      <td>5</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6]</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6]</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6]</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6]</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>[0, 2, 4, 5]</td>\n",
       "      <td>2</td>\n",
       "      <td>[1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>[0, 2, 4, 5]</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>[0, 2, 4, 5]</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>[0, 2, 4, 5]</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>[0, 2, 4, 5]</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          offered_products  choice              offered_product_one_hot  \\\n",
       "0    [0, 1, 2, 3, 4, 5, 6]       5  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "1    [0, 1, 2, 3, 4, 5, 6]       0  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "2    [0, 1, 2, 3, 4, 5, 6]       0  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "3    [0, 1, 2, 3, 4, 5, 6]       0  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "4    [0, 1, 2, 3, 4, 5, 6]       0  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "..                     ...     ...                                  ...   \n",
       "995           [0, 2, 4, 5]       2  [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]   \n",
       "996           [0, 2, 4, 5]       0  [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]   \n",
       "997           [0, 2, 4, 5]       0  [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]   \n",
       "998           [0, 2, 4, 5]       0  [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]   \n",
       "999           [0, 2, 4, 5]       0  [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]   \n",
       "\n",
       "                          choice_one_hot  \n",
       "0    [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]  \n",
       "1    [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  \n",
       "2    [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  \n",
       "3    [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  \n",
       "4    [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  \n",
       "..                                   ...  \n",
       "995  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]  \n",
       "996  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  \n",
       "997  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  \n",
       "998  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  \n",
       "999  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_sample_transactions, out_sample_transactions ,items= convert_json_to_df(f\"hotel_json/instance_{instance_id}.json\")\n",
    "\n",
    "d = len(items) + 1 # consider the 0 as no-purchase\n",
    "\n",
    "train_datasize = len(in_sample_transactions)\n",
    "test_datasize = len(out_sample_transactions)\n",
    "in_sample_transactions = convert_to_one_hot(in_sample_transactions,d)\n",
    "out_sample_transactions = convert_to_one_hot(out_sample_transactions,d)\n",
    "\n",
    "in_sample_transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_train = torch.stack([torch.tensor(sample, dtype=torch.float16,device=device) for sample in in_sample_transactions[\"offered_product_one_hot\"]])\n",
    "S_test = torch.stack([torch.tensor(sample, dtype=torch.float16,device=device) for sample in out_sample_transactions[\"offered_product_one_hot\"]])\n",
    "y_train = torch.stack([torch.tensor(sample, dtype=torch.float16,device=device) for sample in in_sample_transactions[\"choice_one_hot\"]])\n",
    "y_test = torch.stack([torch.tensor(sample, dtype=torch.float16,device=device) for sample in out_sample_transactions[\"choice_one_hot\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Kernel Implementation\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix-valued **Matern kernel** $\\tilde{\\boldsymbol{\\mathsf{k}}}$ can be \n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\mathsf{k}}(\\boldsymbol{e}_{S},\\boldsymbol{e}_{S'}) = \\boldsymbol{K} \\otimes \\mathsf{k}_{m}(\\boldsymbol{e}_{S}, \\boldsymbol{e}_{S'})\n",
    "$$\n",
    "where $\\boldsymbol{K}$ is a positive semi-definite matrix. Then \n",
    "\n",
    "$$\n",
    "\\tilde{\\mathsf{k}}^{ij}(\\boldsymbol{e}_{S},\\boldsymbol{e}_{S'}) = K_{ij} \\times\\sigma^{2} \\frac{2^{1-\\nu}}{\\Gamma(\\nu)} \\left( \\sqrt{ 2\\nu }  \\frac{\\left\\| \\boldsymbol{e}_{S} - \\boldsymbol{e}_{S'} \\right\\|_{2}  }{\\ell} \\right) K_{\\nu} \\left( \\sqrt{ 2\\nu } \\frac{\\left\\| \\boldsymbol{\\boldsymbol{e}_{S}-\\boldsymbol{e}_{S'}} \\right\\|_{2}  }{\\ell} \\right)\n",
    "$$\n",
    "Add constraint, \n",
    "\n",
    "$$\n",
    "\\mathsf{k}^{ij}(S, S') = \\mathbb{1}(i \\in \\boldsymbol{e}_{S})\\cdot \\mathbb{1}(j \\in \\boldsymbol{e}_{S'}) \\cdot   \\tilde{\\mathsf{k}}^{ij}(\\boldsymbol{e}_{S}, \\boldsymbol{e}_{S'})\n",
    "$$\n",
    "\n",
    "\n",
    "Same with Gaussian kernel.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = torch.eye(d).to(torch.float16) # for the moment we consider the identity matrix as the covariance matrix\n",
    "K = K.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Solve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Precalcualte Kernel Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_kernel(S1_set:torch.Tensor,S2_set:torch.Tensor,c:float=1.0, degree:int=1):\n",
    "    \n",
    "    K_g = torch.mm(S1_set, S2_set.T)\n",
    "    K_g += c\n",
    "\n",
    "    K_g = torch.pow(K_g, degree)\n",
    "\n",
    "    K_g /= torch.max(K_g)\n",
    "    return K_g\n",
    "\n",
    "def gaussian_kernel(S1_set:torch.Tensor,S2_set:torch.Tensor, length_scale:float=1.0, sigma:float=1.0):\n",
    "    sq_dist = torch.cdist(S1_set, S2_set, p=2)**2 \n",
    "    K_g = sigma**2 * torch.exp(-sq_dist / (2 * length_scale**2))\n",
    "\n",
    "    return K_g\n",
    "\n",
    "\n",
    "def matern_kernel(S1_set:torch.Tensor,S2_set:torch.Tensor, length_scale:float, nu:float, sigma:float):\n",
    "    dist = torch.cdist(S1_set, S2_set,p=2)\n",
    "    dist = torch.where(dist == 0, torch.tensor(1e-6, device=S1_set.device), dist)\n",
    "    scaled_dist = np.sqrt(2 * nu) * dist / length_scale\n",
    "    factor = (2 **(1-nu)) / gamma(nu)\n",
    "    K_g = sigma**2 * factor * (scaled_dist**nu) * torch.tensor(kv(nu, scaled_dist.cpu().numpy()), device=S1_set.device)\n",
    "    \n",
    "    return K_g\n",
    "\n",
    "def sigmoid_kernel(S1_set:torch.Tensor, S2_set:torch.Tensor, alpha:float,c:float, sigma:float):\n",
    "    inner_product = torch.mm(S1_set, S2_set.T)\n",
    "    K_g = sigma**2 * torch.tanh(alpha * inner_product + c)\n",
    "    return K_g\n",
    "    \n",
    "\n",
    "def laplace_kernel(S1_set:torch.Tensor, S2_set:torch.Tensor, length_scale:float=1.0):\n",
    "    dist = torch.cdist(S1_set, S2_set,p=1)\n",
    "    K_g = torch.exp(-dist / length_scale)\n",
    "    return K_g\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kg_train_train = None\n",
    "Kg_test_train = None\n",
    "if kernel_type == 'matern':\n",
    "    Kg_train_train = matern_kernel(S_train,S_train,kernel_params['length_scale'], kernel_params['nu'], kernel_params['sigma'])\n",
    "    Kg_test_train = matern_kernel(S_test,S_train,kernel_params['length_scale'], kernel_params['nu'], kernel_params['sigma'])\n",
    "elif kernel_type == 'gaussian':\n",
    "    Kg_train_train = gaussian_kernel(S_train, S_train,kernel_params['length_scale'], kernel_params['sigma'])\n",
    "    Kg_test_train = gaussian_kernel(S_test, S_train,kernel_params['length_scale'], kernel_params['sigma'])\n",
    "elif kernel_type == 'sigmoid':\n",
    "    Kg_train_train = sigmoid_kernel(S_train,S_train, kernel_params['alpha'], kernel_params['c'], kernel_params['sigma'])\n",
    "    Kg_test_train = sigmoid_kernel(S_test,S_train, kernel_params['alpha'], kernel_params['c'], kernel_params['sigma'])\n",
    "elif kernel_type == 'laplace':\n",
    "    Kg_train_train = laplace_kernel(S_train,S_train, kernel_params['length_scale'])\n",
    "    Kg_test_train = laplace_kernel(S_test,S_train, kernel_params['length_scale'])\n",
    "elif kernel_type == 'poly':\n",
    "    Kg_train_train = polynomial_kernel(S_train,S_train,kernel_params['c'],kernel_params['degree'])\n",
    "    Kg_test_train = polynomial_kernel(S_test,S_train,kernel_params['c'],kernel_params['degree'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cal_kernel_tensor_old(K_g:torch.Tensor, S_1:torch.Tensor, S_2:torch.Tensor):\n",
    "#     print(K_g.shape)\n",
    "#     K_kernel = torch.zeros((K_g.shape[0],K_g.shape[1], d, d),dtype = torch.float16)\n",
    "#     print(K_kernel.shape)\n",
    "#     K_g = K_g.to('cpu') \n",
    "#     total_iterations = K_g.shape[0] * K_g.shape[1]\n",
    "#     with tqdm(total=total_iterations, desc=\"Kernel Tensor Calculating...\", unit=\"iteration\") as pbar:\n",
    "#         for i in range(K_g.shape[0]):\n",
    "#             mask_x1 = (S_train[i]!=0).view(-1, 1).to('cpu')\n",
    "#             for j in range(K_g.shape[1]):\n",
    "#                 mask_x2 = (S_train[j]!=0).view(1, -1).to('cpu')\n",
    "#                 mask = mask_x1*mask_x2\n",
    "#                 K_kernel[i, j] = K_g[i,j]  * K * mask\n",
    "#                 pbar.update(1)\n",
    "#     return K_kernel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_kernel_tensor_new(Kg:torch.Tensor, S_1:torch.Tensor,S_2:torch.Tensor):\n",
    "    mask1 = (S_1 != 0).unsqueeze(1).unsqueeze(3)\n",
    "    mask2 = (S_2 != 0).unsqueeze(0).unsqueeze(2)\n",
    "    mask = (mask1 & mask2 )\n",
    "    Kg_expanded = Kg.unsqueeze(-1).unsqueeze(-1)\n",
    "    gc.collect()\n",
    "    return (Kg_expanded * K * mask)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = f\"results/feature_free/hotel_{instance_id}/kernel_data/train_train_{kernel_type}({kernel_params_str})_kernel_data.pt\"\n",
    "test_file = f\"results/feature_free/hotel_{instance_id}/kernel_data/test_train_{kernel_type}({kernel_params_str})_kernel_data.pt\"\n",
    "\n",
    "precompute_time  = 0\n",
    "if os.path.exists(train_file) and os.path.exists(test_file):\n",
    "    K_kernel_train_train = torch.load(train_file,map_location=device)\n",
    "    K_kernel_test_train = torch.load(test_file,map_location=device)\n",
    "else:\n",
    "    time1 = time.time()\n",
    "    K_kernel_train_train = cal_kernel_tensor_new(Kg_train_train,S_train, S_train)\n",
    "    time2 = time.time()\n",
    "    precompute_time = time2-time1\n",
    "    \n",
    "    K_kernel_train_train = K_kernel_train_train.to(dtype = torch.float16,device=device)\n",
    "    \n",
    "    K_kernel_test_train = cal_kernel_tensor_new(Kg_test_train, S_test, S_train)\n",
    "    K_kernel_test_train = K_kernel_test_train.to(dtype =torch.float16,device=device)\n",
    "    \n",
    "    os.makedirs(os.path.dirname(train_file), exist_ok=True)\n",
    "    torch.save(\n",
    "        K_kernel_train_train, train_file\n",
    "    )\n",
    "\n",
    "    os.makedirs(os.path.dirname(test_file), exist_ok=True)\n",
    "    torch.save(\n",
    "        K_kernel_test_train,\n",
    "        test_file,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Solve Using `torch.optimin.Adam`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Allocated Memory: 0.13 GB | Reserved Memory: 0.21 GB\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "allocated_memory = torch.cuda.memory_allocated(device) / (1024 ** 3)  \n",
    "reserved_memory = torch.cuda.memory_reserved(device) / (1024 ** 3)  \n",
    "print(f\" Allocated Memory: {allocated_memory:.2f} GB | Reserved Memory: {reserved_memory:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Best Loss: 1.649713397026062\n",
      "Epoch 10, Best Loss: 0.823919415473938\n",
      "Epoch 20, Best Loss: 0.823919415473938\n",
      "Epoch 30, Best Loss: 0.7907113432884216\n",
      "Epoch 40, Best Loss: 0.7822631001472473\n",
      "Epoch 50, Best Loss: 0.7771490216255188\n",
      "Epoch 60, Best Loss: 0.7757511138916016\n",
      "Epoch 70, Best Loss: 0.7730705142021179\n",
      "Epoch 80, Best Loss: 0.771404504776001\n",
      "Epoch 90, Best Loss: 0.7700221538543701\n",
      "Epoch 100, Best Loss: 0.7686238884925842\n",
      "Epoch 110, Best Loss: 0.7674310803413391\n",
      "Epoch 120, Best Loss: 0.7662979364395142\n",
      "Epoch 130, Best Loss: 0.7653632164001465\n",
      "Epoch 140, Best Loss: 0.7644644975662231\n",
      "Epoch 150, Best Loss: 0.7636728286743164\n",
      "Epoch 160, Best Loss: 0.7629429697990417\n",
      "Epoch 170, Best Loss: 0.7622671723365784\n",
      "Epoch 180, Best Loss: 0.7616795301437378\n",
      "Epoch 190, Best Loss: 0.7611636519432068\n",
      "Epoch 200, Best Loss: 0.7606747150421143\n",
      "Epoch 210, Best Loss: 0.7602311372756958\n",
      "Epoch 220, Best Loss: 0.7598104476928711\n",
      "Epoch 230, Best Loss: 0.759435772895813\n",
      "Epoch 240, Best Loss: 0.7590911388397217\n",
      "Epoch 250, Best Loss: 0.7587752938270569\n",
      "Epoch 260, Best Loss: 0.7584626078605652\n",
      "Epoch 270, Best Loss: 0.7581944465637207\n",
      "Epoch 280, Best Loss: 0.7579479813575745\n",
      "Epoch 290, Best Loss: 0.757688581943512\n",
      " Allocated Memory: 0.13 GB | Reserved Memory: 0.21 GB\n"
     ]
    }
   ],
   "source": [
    "alphaset = torch.rand((train_datasize, d), dtype=torch.float32, device=device) \n",
    "alphaset = (alphaset * 0.01).detach().requires_grad_(True)\n",
    "lambda_ = 1e-4\n",
    "grad_clip_threshold = 2.0\n",
    "patience = 25\n",
    "best_loss = float('inf')  \n",
    "epochs_since_improvement = 0\n",
    "best_alphaset = None\n",
    "grad_norm_threshold = 1e-2\n",
    "# grad_norm_threshold = 1e-6\n",
    "\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "\n",
    "def objective(alphaset: torch.Tensor):\n",
    "    # U = torch.zeros((datasize, d), dtype=torch.float32, device=device)\n",
    "    \n",
    "    # batch_size=64\n",
    "    # for start in range(0, datasize, batch_size):\n",
    "    #     end = min(start + batch_size, datasize)\n",
    "    #     U[start:end] = torch.einsum(\"jikl, il -> jk\", K_kernel[start:end], alphaset)\n",
    "    #     torch.cuda.empty_cache()\n",
    "    \n",
    "    U = torch.einsum(\"jikl, il -> jk\", K_kernel_train_train, alphaset)\n",
    "\n",
    "        \n",
    "    if torch.any(torch.isnan(U)):\n",
    "        print(\"NaN detected in U\")\n",
    "\n",
    "    l = cross_entrophy_loss(U)\n",
    "    # print('l=',l)\n",
    "    r = reg(alphaset)\n",
    "    # print('r=',r) \n",
    "     \n",
    "    del U  \n",
    "    gc.collect()\n",
    "    return l + lambda_ * r\n",
    "\n",
    "\n",
    "\n",
    "def cross_entrophy_loss(U):\n",
    "    U_max = torch.max(U, dim=1, keepdim=True).values\n",
    "    U_stable = U - U_max\n",
    "    exp_utility = torch.exp(U_stable)\n",
    "    \n",
    "    if torch.any(torch.isnan(exp_utility)):\n",
    "        print(\"NaN detected in exp(U)\",epoch)\n",
    "    \n",
    "    exp_utility_masked = exp_utility * S_train\n",
    "\n",
    "    sum_exp_utility = torch.sum(exp_utility_masked, dim=1, keepdim=True)\n",
    "\n",
    "    p_matrix = exp_utility_masked / (sum_exp_utility)\n",
    "    if torch.any(torch.isnan(p_matrix)):\n",
    "        print(\"NaN detected in p\",epoch)\n",
    "        \n",
    "    log_p = torch.log(p_matrix + 1e-30)\n",
    "    if torch.any(torch.isnan(log_p)):\n",
    "        print(\"NaN detected in log_p\",epoch)\n",
    "    loss_matrix = -y_train * log_p\n",
    "    if torch.any(torch.isnan(loss_matrix)):\n",
    "        print(\"NaN detected in loss matrix\",epoch)\n",
    "    loss_value = torch.sum(loss_matrix) / train_datasize\n",
    "    return loss_value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def squared_loss(p_vec: torch.Tensor, y_vec: torch.Tensor):\n",
    "    return torch.sum((p_vec - y_vec) ** 2)\n",
    "\n",
    "\n",
    "def reg(alphaset: torch.Tensor):\n",
    "\n",
    "\n",
    "    return torch.einsum(\"id,ijdl,jl->\", alphaset, K_kernel_train_train, alphaset)\n",
    "\n",
    "optimizer = torch.optim.Adam([alphaset], lr=1e-3)\n",
    "\n",
    "time3 = time.time()\n",
    "for epoch in range(300):\n",
    "    optimizer.zero_grad(set_to_none=True)  \n",
    "    \n",
    "    with torch.amp.autocast(device_type='cuda'):\n",
    "        loss_value = objective(alphaset)\n",
    "\n",
    "\n",
    "    \n",
    "    if loss_value.item() < best_loss:\n",
    "        best_loss = loss_value.item()\n",
    "        best_alphaset = alphaset.clone().detach()\n",
    "\n",
    "        \n",
    " \n",
    "    scaler.scale(loss_value).backward()  \n",
    "\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Best Loss: {best_loss}\")\n",
    "    \n",
    "    scaler.unscale_(optimizer)\n",
    "    scaler.step(optimizer)\n",
    "    torch.nn.utils.clip_grad_norm_([alphaset], max_norm=1.0)\n",
    "    \n",
    "    grad_norm = torch.norm(alphaset.grad).item()\n",
    "    # print(grad_norm)\n",
    "    if grad_norm < grad_norm_threshold:\n",
    "        print(f\"Early stopping at epoch {epoch}, Gradient Norm: {grad_norm}, Best Loss: {best_loss}\")\n",
    "        break \n",
    "    \n",
    "    scaler.update()\n",
    "    \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "optimizer.state.clear()\n",
    "del alphaset\n",
    "del optimizer\n",
    "del scaler\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "time4 = time.time()\n",
    "train_time = time4 - time3\n",
    "\n",
    "allocated_memory = torch.cuda.memory_allocated(device) / (1024 ** 3)  # è½¬ä¸ºGB\n",
    "reserved_memory = torch.cuda.memory_reserved(device) / (1024 ** 3)  # è½¬ä¸ºGB\n",
    "print(f\" Allocated Memory: {allocated_memory:.2f} GB | Reserved Memory: {reserved_memory:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor: <class 'torch.Tensor'>, Shape: torch.Size([1000, 1000]), Memory: 1.91 MB\n",
      "Tensor: <class 'torch.Tensor'>, Shape: torch.Size([255, 1000]), Memory: 0.49 MB\n",
      "Tensor: <class 'torch.Tensor'>, Shape: torch.Size([1000, 7]), Memory: 0.01 MB\n",
      "Tensor: <class 'torch.Tensor'>, Shape: torch.Size([255, 7]), Memory: 0.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, Shape: torch.Size([1000, 7]), Memory: 0.01 MB\n",
      "Tensor: <class 'torch.Tensor'>, Shape: torch.Size([255, 7]), Memory: 0.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, Shape: torch.Size([7, 7]), Memory: 0.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, Shape: torch.Size([1000, 1000, 7, 7]), Memory: 93.46 MB\n",
      "Tensor: <class 'torch.Tensor'>, Shape: torch.Size([255, 1000, 7, 7]), Memory: 23.83 MB\n",
      "Tensor: <class 'torch.Tensor'>, Shape: torch.Size([1000, 7]), Memory: 0.03 MB\n",
      "Tensor: <class 'torch.Tensor'>, Shape: torch.Size([]), Memory: 0.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, Shape: torch.Size([1000, 7]), Memory: 0.03 MB\n",
      "Tensor: <class 'torch.Tensor'>, Shape: torch.Size([1000, 7]), Memory: 0.03 MB\n",
      "Total GPU memory used by tensors: 119.80 MB\n",
      " Allocated Memory: 0.13 GB | Reserved Memory: 0.21 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/yiqipytorch/lib/python3.9/site-packages/torch/__init__.py:1021: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
      "  return isinstance(obj, torch.Tensor)\n",
      "/tmp/ipykernel_262229/464894091.py:10: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
      "  if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "def find_large_tensors():\n",
    "    total_mem = 0\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
    "                if obj.is_cuda:\n",
    "                    tensor_mem = obj.numel() * obj.element_size() / 1024**2  # MB\n",
    "                    total_mem += tensor_mem\n",
    "                    print(f\"Tensor: {type(obj)}, Shape: {obj.shape}, Memory: {tensor_mem:.2f} MB\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    print(f\"Total GPU memory used by tensors: {total_mem:.2f} MB\")\n",
    "\n",
    "find_large_tensors()\n",
    "allocated_memory = torch.cuda.memory_allocated(device) / (1024 ** 3)  # è½¬ä¸ºGB\n",
    "reserved_memory = torch.cuda.memory_reserved(device) / (1024 ** 3)  # è½¬ä¸ºGB\n",
    "print(f\" Allocated Memory: {allocated_memory:.2f} GB | Reserved Memory: {reserved_memory:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_matrix(U:torch.Tensor,S:torch.Tensor):\n",
    "    exp_utility = torch.exp(U)\n",
    "    exp_utility_masked = exp_utility * S\n",
    "    sum_exp_utility = torch.sum(exp_utility_masked, dim=1, keepdim=True)\n",
    "    p_matrix = exp_utility_masked / sum_exp_utility\n",
    "    return p_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float16\n"
     ]
    }
   ],
   "source": [
    "print(K_kernel_test_train.dtype)\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "U_test = torch.einsum(\"jikl,il->jk\", K_kernel_test_train, best_alphaset.to(torch.float16))\n",
    "P_test = get_prob_matrix(U_test,S_test)\n",
    "\n",
    "U_train = torch.einsum(\"jikl,il->jk\", K_kernel_train_train, best_alphaset.to(torch.float16))\n",
    "P_train = get_prob_matrix(U_train,S_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.22432896899001922, 0.2230328503690656)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mse = torch.mean((P_test- y_test) ** 2).item()\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "train_mse = torch.mean((P_train- y_train) ** 2).item()\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "test_rmse,train_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7955882352941176, 0.7545)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epsilon  = 1e-7\n",
    "test_nnl = -torch.sum(y_test * torch.log(P_test + epsilon)).item() / y_test.shape[0]\n",
    "train_nnl = -torch.sum(y_train * torch.log(P_train + epsilon)).item() / y_train.shape[0]\n",
    "test_nnl, train_nnl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv,os\n",
    "results = [\n",
    "    [instance_id, test_rmse, train_rmse, test_nnl, train_nnl, lambda_,kernel_type , kernel_params_str,loss_type,precompute_time, train_time]\n",
    "]\n",
    "file_path = f'results/feature_free/results.csv'\n",
    "file_exists = os.path.exists(file_path)\n",
    "with open(file_path, mode=\"a\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    if not file_exists or os.stat(file_path).st_size == 0:\n",
    "        writer.writerow([\"instance_id\", \"test_rmse\", \"train_rmse\",\"test_nnl\",\"train_nnl\", \"lambda_\", \"kernel_type\", \"kernel_params\",\"loss_type\",\"precompute_time\",\"train_time\"])\n",
    "    writer.writerows(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yiqipytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
