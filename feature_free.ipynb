{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json \n",
    "import pandas as pd \n",
    "from scipy.special import gamma, kv\n",
    "from typing import List, Dict, Any\n",
    "from scipy.optimize import minimize\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read Data, Convert to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_json_to_df(json_file_path:str):\n",
    "    with open(json_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    in_sample_transactions = data[\"transactions\"][\"in_sample_transactions\"]\n",
    "    out_sample_transactions = data[\"transactions\"][\"out_of_sample_transactions\"]\n",
    "    product_labels = data['product_labels']\n",
    "\n",
    "    in_sample_transactions = pd.DataFrame(in_sample_transactions)\n",
    "    out_sample_transactions = pd.DataFrame(out_sample_transactions)\n",
    "    \n",
    "    # rename 'prodcut' to 'choice' \n",
    "    in_sample_transactions.rename(columns={'product':'choice'}, inplace=True)\n",
    "    out_sample_transactions.rename(columns={'product':'choice'}, inplace=True)\n",
    "    product_labels = pd.DataFrame(\n",
    "        list(product_labels.items()), columns=[\"product_id\", \"product_name\"]\n",
    "    )\n",
    "    return in_sample_transactions, out_sample_transactions,product_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_list_to_one_hot(transaction:list,d):\n",
    "    one_hot = np.zeros(d)\n",
    "    for item in transaction:\n",
    "        one_hot[item-1] = 1\n",
    "    return one_hot\n",
    "\n",
    "def convert_to_one_hot(transactions:pd.DataFrame,d):\n",
    "    transactions[\"offered_product_one_hot\"] = transactions['offered_products'].apply(lambda x : convert_list_to_one_hot(x,d))\n",
    "    transactions['choice_one_hot'] = transactions['choice'].apply(lambda x: np.zeros(d) if x == 0 else  convert_list_to_one_hot([x],d))\n",
    "    return transactions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offered_products</th>\n",
       "      <th>choice</th>\n",
       "      <th>offered_product_one_hot</th>\n",
       "      <th>choice_one_hot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6]</td>\n",
       "      <td>5</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6]</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6]</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6]</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6]</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>[0, 2, 4, 5]</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 1.0, 0.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>[0, 2, 4, 5]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 1.0, 0.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>[0, 2, 4, 5]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 1.0, 0.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>[0, 2, 4, 5]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 1.0, 0.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>[0, 2, 4, 5]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 1.0, 0.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          offered_products  choice         offered_product_one_hot  \\\n",
       "0    [0, 1, 2, 3, 4, 5, 6]       5  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "1    [0, 1, 2, 3, 4, 5, 6]       0  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "2    [0, 1, 2, 3, 4, 5, 6]       0  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "3    [0, 1, 2, 3, 4, 5, 6]       0  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "4    [0, 1, 2, 3, 4, 5, 6]       0  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "..                     ...     ...                             ...   \n",
       "995           [0, 2, 4, 5]       2  [0.0, 1.0, 0.0, 1.0, 1.0, 1.0]   \n",
       "996           [0, 2, 4, 5]       0  [0.0, 1.0, 0.0, 1.0, 1.0, 1.0]   \n",
       "997           [0, 2, 4, 5]       0  [0.0, 1.0, 0.0, 1.0, 1.0, 1.0]   \n",
       "998           [0, 2, 4, 5]       0  [0.0, 1.0, 0.0, 1.0, 1.0, 1.0]   \n",
       "999           [0, 2, 4, 5]       0  [0.0, 1.0, 0.0, 1.0, 1.0, 1.0]   \n",
       "\n",
       "                     choice_one_hot  \n",
       "0    [0.0, 0.0, 0.0, 0.0, 1.0, 0.0]  \n",
       "1    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  \n",
       "2    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  \n",
       "3    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  \n",
       "4    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  \n",
       "..                              ...  \n",
       "995  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]  \n",
       "996  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  \n",
       "997  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  \n",
       "998  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  \n",
       "999  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance_id = 5\n",
    "in_sample_transactions, out_sample_transactions ,items= convert_json_to_df(f\"hotel_json/instance_{instance_id}.json\")\n",
    "d = len(items)\n",
    "datasize = len(in_sample_transactions)\n",
    "in_sample_transactions = convert_to_one_hot(in_sample_transactions,d)\n",
    "out_sample_transactions = convert_to_one_hot(out_sample_transactions,d)\n",
    "\n",
    "in_sample_transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Kernel Implementation\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix-valued **Matern kernel** $\\tilde{\\boldsymbol{\\mathsf{k}}}$ can be \n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\mathsf{k}}(\\boldsymbol{e}_{S},\\boldsymbol{e}_{S'}) = \\boldsymbol{K} \\otimes \\mathsf{k}_{m}(\\boldsymbol{e}_{S}, \\boldsymbol{e}_{S'})\n",
    "$$\n",
    "where $\\boldsymbol{K}$ is a positive semi-definite matrix. Then \n",
    "\n",
    "$$\n",
    "\\tilde{\\mathsf{k}}^{ij}(\\boldsymbol{e}_{S},\\boldsymbol{e}_{S'}) = K_{ij} \\times\\sigma^{2} \\frac{2^{1-\\nu}}{\\Gamma(\\nu)} \\left( \\sqrt{ 2\\nu }  \\frac{\\left\\| \\boldsymbol{e}_{S} - \\boldsymbol{e}_{S'} \\right\\|_{2}  }{\\ell} \\right) K_{\\nu} \\left( \\sqrt{ 2\\nu } \\frac{\\left\\| \\boldsymbol{\\boldsymbol{e}_{S}-\\boldsymbol{e}_{S'}} \\right\\|_{2}  }{\\ell} \\right)\n",
    "$$\n",
    "Add constraint, \n",
    "\n",
    "$$\n",
    "\\mathsf{k}^{ij}(S, S') = \\mathbb{1}(i \\in \\boldsymbol{e}_{S})\\cdot \\mathbb{1}(j \\in \\boldsymbol{e}_{S'}) \\cdot   \\tilde{\\mathsf{k}}^{ij}(\\boldsymbol{e}_{S}, \\boldsymbol{e}_{S'})\n",
    "$$\n",
    "\n",
    "\n",
    "Same with Gaussian kernel.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "K = np.ones((d,d))\n",
    "\n",
    "def generate_scalar_matern_kernel(length_scale:float, nu:float, sigma:float):\n",
    "    \"\"\"\n",
    "    Input: kernel parameters and index (i,j)\n",
    "    generate base scalar-valued Matern kernel at (i,j) \n",
    "    return {k_m}_ij\n",
    "    \"\"\"\n",
    "    def kernel(x1:np.ndarray, x2:np.ndarray):\n",
    "\n",
    "        dist = np.linalg.norm(x1 - x2)\n",
    "        \n",
    "        if dist == 0:\n",
    "            return sigma**2\n",
    "\n",
    "        # calculate the factor\n",
    "        factor = (2 ** (1 - nu)) / gamma(nu)\n",
    "        scaled_dist  = np.sqrt(2 * nu) * dist / length_scale\n",
    "        result = sigma**2 * factor * (scaled_dist**nu) * kv(nu, scaled_dist)\n",
    "        return result\n",
    "    return kernel\n",
    "\n",
    "def generate_scalar_gaussian_kernel(length_scale:float, sigma:float):\n",
    "    \"\"\"\n",
    "    Input: kernel parameters and index (i,j)\n",
    "    generate base scalar-valued Gaussian kernel at (i,j)\n",
    "    return {k_g}_ij\n",
    "    \"\"\"\n",
    "    def kernel(x1:np.ndarray, x2:np.ndarray):\n",
    "        \n",
    "        dist = np.linalg.norm(x1 - x2)\n",
    "        return sigma**2 * np.exp(-dist**2 / (2 * length_scale**2))\n",
    "    return kernel\n",
    "\n",
    "def generate_matrix_matern_kernel(length_scale, nu, sigma):\n",
    "    \"\"\"\n",
    "    generate a matrix-valued Matern kernel \n",
    "    \"\"\"\n",
    "    scalar_matern_kernel = generate_scalar_matern_kernel(length_scale, nu, sigma)\n",
    "    def kernel(x1:np.ndarray,x2:np.ndarray):\n",
    "        dim = x1.shape[0]\n",
    "        result = np.zeros((dim,dim))\n",
    "        for i in range(dim):\n",
    "            for j in range(dim):\n",
    "                if x1[i] == 0 or x2[j] == 0:\n",
    "                    result[i,j] = 0\n",
    "                else:\n",
    "                    result[i,j] = scalar_matern_kernel(x1,x2) * K[i,j]\n",
    "        return result\n",
    "    return kernel\n",
    "\n",
    "\n",
    "def generate_matrix_gaussian_kernel(length_scale, sigma):\n",
    "    \"\"\"\n",
    "    generate a matrix-valued Gaussian kernel \n",
    "    \"\"\"\n",
    "    scalar_gaussian_kernel = generate_scalar_gaussian_kernel(length_scale, sigma)\n",
    "    def kernel(x1:np.ndarray,x2:np.ndarray):\n",
    "        dim = x1.shape[0]\n",
    "        result = np.zeros((dim,dim))\n",
    "        for i in range(dim):\n",
    "            for j in range(dim):\n",
    "                if x1[i] == 0 or x2[j] == 0:\n",
    "                    result[i,j] = 0\n",
    "                else:\n",
    "                    result[i,j] = scalar_gaussian_kernel(x1, x2) * K[i,j]\n",
    "        return result\n",
    "    return kernel\n",
    "\n",
    "\n",
    "scalar_matern_kernel = generate_scalar_matern_kernel(1, 1, 1)\n",
    "scalar_gaussian_kernel = generate_scalar_gaussian_kernel(1, 1)\n",
    "matrix_matern_kernel = generate_matrix_matern_kernel(1, 1, 1)\n",
    "matrix_gaussian_kernel = generate_matrix_gaussian_kernel(1, 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress: 100%|██████████| 1000000/1000000 [02:31<00:00, 6593.25iteration/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "total_iterations = datasize * datasize\n",
    "with tqdm(total=total_iterations, desc=\"Overall Progress\", unit=\"iteration\") as pbar:\n",
    "    K_kernel = np.zeros((datasize, datasize, d,d))  \n",
    "\n",
    "    for i in range(datasize):\n",
    "        for j in range(datasize):\n",
    "\n",
    "            K_kernel[i, j] = matrix_matern_kernel(\n",
    "                in_sample_transactions[\"offered_product_one_hot\"][i],\n",
    "                in_sample_transactions[\"offered_product_one_hot\"][j],\n",
    "            )\n",
    "\n",
    "            pbar.update(1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 1000, 6, 6])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K_kernel = torch.tensor(K_kernel,dtype=torch.float32)\n",
    "K_kernel.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 3.8568854331970215\n",
      "Epoch 1, Loss: 3.4104156494140625\n",
      "Epoch 2, Loss: 2.3018760681152344\n",
      "Epoch 3, Loss: 1.984501600265503\n",
      "Epoch 4, Loss: 2.287419319152832\n",
      "Epoch 5, Loss: 1.674678921699524\n",
      "Epoch 6, Loss: 1.88460111618042\n",
      "Epoch 7, Loss: 2.1403815746307373\n",
      "Epoch 8, Loss: 2.08719539642334\n",
      "Epoch 9, Loss: 1.771384358406067\n",
      "Epoch 10, Loss: 1.4352787733078003\n",
      "Epoch 11, Loss: 1.3189496994018555\n",
      "Epoch 12, Loss: 1.7327287197113037\n",
      "Epoch 13, Loss: 1.3278980255126953\n",
      "Epoch 14, Loss: 1.219597578048706\n",
      "Epoch 15, Loss: 1.2232204675674438\n",
      "Epoch 16, Loss: 1.272365927696228\n",
      "Epoch 17, Loss: 1.2892640829086304\n",
      "Epoch 18, Loss: 1.2386466264724731\n",
      "Epoch 19, Loss: 1.1419211626052856\n",
      "Epoch 20, Loss: 1.046735167503357\n",
      "Epoch 21, Loss: 0.9826368689537048\n",
      "Epoch 22, Loss: 0.9481101036071777\n",
      "Epoch 23, Loss: 0.9336551427841187\n",
      "Epoch 24, Loss: 0.9191620349884033\n",
      "Epoch 25, Loss: 0.8764467239379883\n",
      "Epoch 26, Loss: 0.8262518644332886\n",
      "Epoch 27, Loss: 0.7780554890632629\n",
      "Epoch 28, Loss: 0.7367410659790039\n",
      "Epoch 29, Loss: 0.7071200013160706\n",
      "Epoch 30, Loss: 0.6909059286117554\n",
      "Epoch 31, Loss: 0.678384006023407\n",
      "Epoch 32, Loss: 0.6568595767021179\n",
      "Epoch 33, Loss: 0.6241796612739563\n",
      "Epoch 34, Loss: 0.5893018841743469\n",
      "Epoch 35, Loss: 0.56516033411026\n",
      "Epoch 36, Loss: 0.55609530210495\n",
      "Epoch 37, Loss: 0.5520583391189575\n",
      "Epoch 38, Loss: 0.5390164852142334\n",
      "Epoch 39, Loss: 0.5137690305709839\n",
      "Epoch 40, Loss: 0.4875948429107666\n",
      "Epoch 41, Loss: 0.47263529896736145\n",
      "Epoch 42, Loss: 0.4691098630428314\n",
      "Epoch 43, Loss: 0.4665224552154541\n",
      "Epoch 44, Loss: 0.45625975728034973\n",
      "Epoch 45, Loss: 0.4400891661643982\n",
      "Epoch 46, Loss: 0.42781880497932434\n",
      "Epoch 47, Loss: 0.4257746934890747\n",
      "Epoch 48, Loss: 0.4251249134540558\n",
      "Epoch 49, Loss: 0.4166860282421112\n",
      "Epoch 50, Loss: 0.4080224633216858\n",
      "Epoch 51, Loss: 0.4035494327545166\n",
      "Epoch 52, Loss: 0.4030303359031677\n",
      "Epoch 53, Loss: 0.40275466442108154\n",
      "Epoch 54, Loss: 0.3993633985519409\n",
      "Epoch 55, Loss: 0.39361992478370667\n",
      "Epoch 56, Loss: 0.3889514207839966\n",
      "Epoch 57, Loss: 0.38773736357688904\n",
      "Epoch 58, Loss: 0.38757842779159546\n",
      "Epoch 59, Loss: 0.38437706232070923\n",
      "Epoch 60, Loss: 0.37981724739074707\n",
      "Epoch 61, Loss: 0.37803566455841064\n",
      "Epoch 62, Loss: 0.3776426911354065\n",
      "Epoch 63, Loss: 0.3749345541000366\n",
      "Epoch 64, Loss: 0.37101539969444275\n",
      "Epoch 65, Loss: 0.3688329756259918\n",
      "Epoch 66, Loss: 0.367942750453949\n",
      "Epoch 67, Loss: 0.3663904666900635\n",
      "Epoch 68, Loss: 0.36395278573036194\n",
      "Epoch 69, Loss: 0.3613162338733673\n",
      "Epoch 70, Loss: 0.359406054019928\n",
      "Epoch 71, Loss: 0.35810714960098267\n",
      "Epoch 72, Loss: 0.3562479615211487\n",
      "Epoch 73, Loss: 0.3538832366466522\n",
      "Epoch 74, Loss: 0.3520776033401489\n",
      "Epoch 75, Loss: 0.3509366810321808\n",
      "Epoch 76, Loss: 0.3493148982524872\n",
      "Epoch 77, Loss: 0.34711894392967224\n",
      "Epoch 78, Loss: 0.3454020321369171\n",
      "Epoch 79, Loss: 0.344208300113678\n",
      "Epoch 80, Loss: 0.34285208582878113\n",
      "Epoch 81, Loss: 0.34121376276016235\n",
      "Epoch 82, Loss: 0.33973973989486694\n",
      "Epoch 83, Loss: 0.338534414768219\n",
      "Epoch 84, Loss: 0.33734309673309326\n",
      "Epoch 85, Loss: 0.33601945638656616\n",
      "Epoch 86, Loss: 0.33470138907432556\n",
      "Epoch 87, Loss: 0.33362334966659546\n",
      "Epoch 88, Loss: 0.3326398730278015\n",
      "Epoch 89, Loss: 0.3314676582813263\n",
      "Epoch 90, Loss: 0.3302556574344635\n",
      "Epoch 91, Loss: 0.3292507827281952\n",
      "Epoch 92, Loss: 0.3282971680164337\n",
      "Epoch 93, Loss: 0.32722383737564087\n",
      "Epoch 94, Loss: 0.3261762261390686\n",
      "Epoch 95, Loss: 0.3252476453781128\n",
      "Epoch 96, Loss: 0.3243175446987152\n",
      "Epoch 97, Loss: 0.32333120703697205\n",
      "Epoch 98, Loss: 0.3223748505115509\n",
      "Epoch 99, Loss: 0.32149580121040344\n",
      "Epoch 100, Loss: 0.32064610719680786\n",
      "Epoch 101, Loss: 0.3197764754295349\n",
      "Epoch 102, Loss: 0.3189108669757843\n",
      "Epoch 103, Loss: 0.31810280680656433\n",
      "Epoch 104, Loss: 0.3173222839832306\n",
      "Epoch 105, Loss: 0.3165223002433777\n",
      "Epoch 106, Loss: 0.315743625164032\n",
      "Epoch 107, Loss: 0.31502053141593933\n",
      "Epoch 108, Loss: 0.3143030107021332\n",
      "Epoch 109, Loss: 0.313571035861969\n",
      "Epoch 110, Loss: 0.31286904215812683\n",
      "Epoch 111, Loss: 0.31220245361328125\n",
      "Epoch 112, Loss: 0.31153783202171326\n",
      "Epoch 113, Loss: 0.31087926030158997\n",
      "Epoch 114, Loss: 0.31024500727653503\n",
      "Epoch 115, Loss: 0.30962932109832764\n",
      "Epoch 116, Loss: 0.3090195059776306\n",
      "Epoch 117, Loss: 0.3084179759025574\n",
      "Epoch 118, Loss: 0.30783748626708984\n",
      "Epoch 119, Loss: 0.3072737753391266\n",
      "Epoch 120, Loss: 0.3067135512828827\n",
      "Epoch 121, Loss: 0.3061610162258148\n",
      "Epoch 122, Loss: 0.30562740564346313\n",
      "Epoch 123, Loss: 0.3051031827926636\n",
      "Epoch 124, Loss: 0.3045835494995117\n",
      "Epoch 125, Loss: 0.3040759265422821\n",
      "Epoch 126, Loss: 0.3035831153392792\n",
      "Epoch 127, Loss: 0.3030949831008911\n",
      "Epoch 128, Loss: 0.3026135265827179\n",
      "Epoch 129, Loss: 0.30214405059814453\n",
      "Epoch 130, Loss: 0.3016839623451233\n",
      "Epoch 131, Loss: 0.30123037099838257\n",
      "Epoch 132, Loss: 0.30078455805778503\n",
      "Epoch 133, Loss: 0.3003474473953247\n",
      "Epoch 134, Loss: 0.2999182641506195\n",
      "Epoch 135, Loss: 0.2994951605796814\n",
      "Epoch 136, Loss: 0.2990788221359253\n",
      "Epoch 137, Loss: 0.29867175221443176\n",
      "Epoch 138, Loss: 0.298270046710968\n",
      "Epoch 139, Loss: 0.297873854637146\n",
      "Epoch 140, Loss: 0.2974851131439209\n",
      "Epoch 141, Loss: 0.29710260033607483\n",
      "Epoch 142, Loss: 0.29672566056251526\n",
      "Epoch 143, Loss: 0.2963544726371765\n",
      "Epoch 144, Loss: 0.295989453792572\n",
      "Epoch 145, Loss: 0.29562994837760925\n",
      "Epoch 146, Loss: 0.29527556896209717\n",
      "Epoch 147, Loss: 0.29492634534835815\n",
      "Epoch 148, Loss: 0.29458290338516235\n",
      "Epoch 149, Loss: 0.2942439317703247\n",
      "Epoch 150, Loss: 0.29391005635261536\n",
      "Epoch 151, Loss: 0.2935808002948761\n",
      "Epoch 152, Loss: 0.2932561933994293\n",
      "Epoch 153, Loss: 0.2929360568523407\n",
      "Epoch 154, Loss: 0.29262036085128784\n",
      "Epoch 155, Loss: 0.29230934381484985\n",
      "Epoch 156, Loss: 0.2920020520687103\n",
      "Epoch 157, Loss: 0.29169929027557373\n",
      "Epoch 158, Loss: 0.29140013456344604\n",
      "Epoch 159, Loss: 0.2911055386066437\n",
      "Epoch 160, Loss: 0.29081425070762634\n",
      "Epoch 161, Loss: 0.2905268669128418\n",
      "Epoch 162, Loss: 0.29024356603622437\n",
      "Epoch 163, Loss: 0.2899637520313263\n",
      "Epoch 164, Loss: 0.28968748450279236\n",
      "Epoch 165, Loss: 0.2894148528575897\n",
      "Epoch 166, Loss: 0.2891450524330139\n",
      "Epoch 167, Loss: 0.28887906670570374\n",
      "Epoch 168, Loss: 0.2886163294315338\n",
      "Epoch 169, Loss: 0.2883566915988922\n",
      "Epoch 170, Loss: 0.2881004214286804\n",
      "Epoch 171, Loss: 0.28784704208374023\n",
      "Epoch 172, Loss: 0.2875967025756836\n",
      "Epoch 173, Loss: 0.287349671125412\n",
      "Epoch 174, Loss: 0.28710517287254333\n",
      "Epoch 175, Loss: 0.28686392307281494\n",
      "Epoch 176, Loss: 0.2866254150867462\n",
      "Epoch 177, Loss: 0.28638988733291626\n",
      "Epoch 178, Loss: 0.2861570417881012\n",
      "Epoch 179, Loss: 0.2859269678592682\n",
      "Epoch 180, Loss: 0.2856990694999695\n",
      "Epoch 181, Loss: 0.2854742109775543\n",
      "Epoch 182, Loss: 0.28525179624557495\n",
      "Epoch 183, Loss: 0.2850321829319\n",
      "Epoch 184, Loss: 0.2848149240016937\n",
      "Epoch 185, Loss: 0.28459998965263367\n",
      "Epoch 186, Loss: 0.2843877971172333\n",
      "Epoch 187, Loss: 0.28417766094207764\n",
      "Epoch 188, Loss: 0.28397008776664734\n",
      "Epoch 189, Loss: 0.28376492857933044\n",
      "Epoch 190, Loss: 0.2835620939731598\n",
      "Epoch 191, Loss: 0.28336143493652344\n",
      "Epoch 192, Loss: 0.2831628620624542\n",
      "Epoch 193, Loss: 0.28296664357185364\n",
      "Epoch 194, Loss: 0.2827724516391754\n",
      "Epoch 195, Loss: 0.2825803756713867\n",
      "Epoch 196, Loss: 0.2823908030986786\n",
      "Epoch 197, Loss: 0.2822030484676361\n",
      "Epoch 198, Loss: 0.28201723098754883\n",
      "Epoch 199, Loss: 0.28183379769325256\n",
      "Epoch 200, Loss: 0.2816520929336548\n",
      "Epoch 201, Loss: 0.28147247433662415\n",
      "Epoch 202, Loss: 0.28129515051841736\n",
      "Epoch 203, Loss: 0.2811194956302643\n",
      "Epoch 204, Loss: 0.2809455692768097\n",
      "Epoch 205, Loss: 0.28077372908592224\n",
      "Epoch 206, Loss: 0.2806037366390228\n",
      "Epoch 207, Loss: 0.2804359495639801\n",
      "Epoch 208, Loss: 0.2802697420120239\n",
      "Epoch 209, Loss: 0.2801051735877991\n",
      "Epoch 210, Loss: 0.2799423933029175\n",
      "Epoch 211, Loss: 0.279781699180603\n",
      "Epoch 212, Loss: 0.2796226143836975\n",
      "Epoch 213, Loss: 0.2794651985168457\n",
      "Epoch 214, Loss: 0.27930960059165955\n",
      "Epoch 215, Loss: 0.2791556119918823\n",
      "Epoch 216, Loss: 0.279003381729126\n",
      "Epoch 217, Loss: 0.2788522243499756\n",
      "Epoch 218, Loss: 0.27870357036590576\n",
      "Epoch 219, Loss: 0.2785559296607971\n",
      "Epoch 220, Loss: 0.2784104645252228\n",
      "Epoch 221, Loss: 0.27826622128486633\n",
      "Epoch 222, Loss: 0.2781233787536621\n",
      "Epoch 223, Loss: 0.2779821455478668\n",
      "Epoch 224, Loss: 0.2778429388999939\n",
      "Epoch 225, Loss: 0.2777045667171478\n",
      "Epoch 226, Loss: 0.27756786346435547\n",
      "Epoch 227, Loss: 0.2774331271648407\n",
      "Epoch 228, Loss: 0.27729910612106323\n",
      "Epoch 229, Loss: 0.27716708183288574\n",
      "Epoch 230, Loss: 0.27703651785850525\n",
      "Epoch 231, Loss: 0.2769075632095337\n",
      "Epoch 232, Loss: 0.2767795920372009\n",
      "Epoch 233, Loss: 0.2766530215740204\n",
      "Epoch 234, Loss: 0.2765279710292816\n",
      "Epoch 235, Loss: 0.2764042019844055\n",
      "Epoch 236, Loss: 0.27628156542778015\n",
      "Epoch 237, Loss: 0.27616068720817566\n",
      "Epoch 238, Loss: 0.2760411202907562\n",
      "Epoch 239, Loss: 0.27592262625694275\n",
      "Epoch 240, Loss: 0.27580511569976807\n",
      "Epoch 241, Loss: 0.2756894528865814\n",
      "Epoch 242, Loss: 0.27557438611984253\n",
      "Epoch 243, Loss: 0.2754611670970917\n",
      "Epoch 244, Loss: 0.2753492295742035\n",
      "Epoch 245, Loss: 0.27523788809776306\n",
      "Epoch 246, Loss: 0.2751285135746002\n",
      "Epoch 247, Loss: 0.27501964569091797\n",
      "Epoch 248, Loss: 0.2749125063419342\n",
      "Epoch 249, Loss: 0.2748062312602997\n",
      "Epoch 250, Loss: 0.2747010290622711\n",
      "Epoch 251, Loss: 0.2745968997478485\n",
      "Epoch 252, Loss: 0.2744944393634796\n",
      "Epoch 253, Loss: 0.27439266443252563\n",
      "Epoch 254, Loss: 0.2742920517921448\n",
      "Epoch 255, Loss: 0.27419233322143555\n",
      "Epoch 256, Loss: 0.2740936875343323\n",
      "Epoch 257, Loss: 0.27399665117263794\n",
      "Epoch 258, Loss: 0.2739003896713257\n",
      "Epoch 259, Loss: 0.27380484342575073\n",
      "Epoch 260, Loss: 0.2737109065055847\n",
      "Epoch 261, Loss: 0.2736171782016754\n",
      "Epoch 262, Loss: 0.2735251784324646\n",
      "Epoch 263, Loss: 0.2734338045120239\n",
      "Epoch 264, Loss: 0.2733435034751892\n",
      "Epoch 265, Loss: 0.2732540965080261\n",
      "Epoch 266, Loss: 0.2731659412384033\n",
      "Epoch 267, Loss: 0.27307841181755066\n",
      "Epoch 268, Loss: 0.27299174666404724\n",
      "Epoch 269, Loss: 0.27290624380111694\n",
      "Epoch 270, Loss: 0.2728215157985687\n",
      "Epoch 271, Loss: 0.27273789048194885\n",
      "Epoch 272, Loss: 0.27265456318855286\n",
      "Epoch 273, Loss: 0.2725730240345001\n",
      "Epoch 274, Loss: 0.2724919617176056\n",
      "Epoch 275, Loss: 0.27241164445877075\n",
      "Epoch 276, Loss: 0.27233168482780457\n",
      "Epoch 277, Loss: 0.2722530663013458\n",
      "Epoch 278, Loss: 0.27217555046081543\n",
      "Epoch 279, Loss: 0.27209845185279846\n",
      "Epoch 280, Loss: 0.2720226049423218\n",
      "Epoch 281, Loss: 0.2719467282295227\n",
      "Epoch 282, Loss: 0.2718725800514221\n",
      "Epoch 283, Loss: 0.27179858088493347\n",
      "Epoch 284, Loss: 0.2717251479625702\n",
      "Epoch 285, Loss: 0.27165335416793823\n",
      "Epoch 286, Loss: 0.271581768989563\n",
      "Epoch 287, Loss: 0.2715107500553131\n",
      "Epoch 288, Loss: 0.27144068479537964\n",
      "Epoch 289, Loss: 0.27137163281440735\n",
      "Epoch 290, Loss: 0.27130281925201416\n",
      "Epoch 291, Loss: 0.2712348699569702\n",
      "Epoch 292, Loss: 0.2711681127548218\n",
      "Epoch 293, Loss: 0.2711014449596405\n",
      "Epoch 294, Loss: 0.27103546261787415\n",
      "Epoch 295, Loss: 0.27097031474113464\n",
      "Epoch 296, Loss: 0.27090558409690857\n",
      "Epoch 297, Loss: 0.27084195613861084\n",
      "Epoch 298, Loss: 0.27077850699424744\n",
      "Epoch 299, Loss: 0.27071574330329895\n",
      "Epoch 300, Loss: 0.2706538736820221\n",
      "Epoch 301, Loss: 0.27059251070022583\n",
      "Epoch 302, Loss: 0.27053210139274597\n",
      "Epoch 303, Loss: 0.2704716622829437\n",
      "Epoch 304, Loss: 0.27041226625442505\n",
      "Epoch 305, Loss: 0.27035319805145264\n",
      "Epoch 306, Loss: 0.27029475569725037\n",
      "Epoch 307, Loss: 0.27023714780807495\n",
      "Epoch 308, Loss: 0.27017977833747864\n",
      "Epoch 309, Loss: 0.27012312412261963\n",
      "Epoch 310, Loss: 0.2700669765472412\n",
      "Epoch 311, Loss: 0.27001121640205383\n",
      "Epoch 312, Loss: 0.2699565887451172\n",
      "Epoch 313, Loss: 0.2699020504951477\n",
      "Epoch 314, Loss: 0.2698478102684021\n",
      "Epoch 315, Loss: 0.26979437470436096\n",
      "Epoch 316, Loss: 0.26974135637283325\n",
      "Epoch 317, Loss: 0.26968902349472046\n",
      "Epoch 318, Loss: 0.26963719725608826\n",
      "Epoch 319, Loss: 0.26958566904067993\n",
      "Epoch 320, Loss: 0.26953479647636414\n",
      "Epoch 321, Loss: 0.2694842517375946\n",
      "Epoch 322, Loss: 0.26943424344062805\n",
      "Epoch 323, Loss: 0.2693846523761749\n",
      "Epoch 324, Loss: 0.26933541893959045\n",
      "Epoch 325, Loss: 0.2692871391773224\n",
      "Epoch 326, Loss: 0.26923879981040955\n",
      "Epoch 327, Loss: 0.26919102668762207\n",
      "Epoch 328, Loss: 0.269144207239151\n",
      "Epoch 329, Loss: 0.26909708976745605\n",
      "Epoch 330, Loss: 0.2690509855747223\n",
      "Epoch 331, Loss: 0.26900485157966614\n",
      "Epoch 332, Loss: 0.2689591348171234\n",
      "Epoch 333, Loss: 0.26891425251960754\n",
      "Epoch 334, Loss: 0.26886919140815735\n",
      "Epoch 335, Loss: 0.2688246965408325\n",
      "Epoch 336, Loss: 0.26878100633621216\n",
      "Epoch 337, Loss: 0.2687376141548157\n",
      "Epoch 338, Loss: 0.2686941921710968\n",
      "Epoch 339, Loss: 0.2686513364315033\n",
      "Epoch 340, Loss: 0.2686094641685486\n",
      "Epoch 341, Loss: 0.2685673236846924\n",
      "Epoch 342, Loss: 0.2685253620147705\n",
      "Epoch 343, Loss: 0.2684843838214874\n",
      "Epoch 344, Loss: 0.26844340562820435\n",
      "Epoch 345, Loss: 0.26840296387672424\n",
      "Epoch 346, Loss: 0.2683623731136322\n",
      "Epoch 347, Loss: 0.26832258701324463\n",
      "Epoch 348, Loss: 0.2682832181453705\n",
      "Epoch 349, Loss: 0.2682439982891083\n",
      "Epoch 350, Loss: 0.2682047188282013\n",
      "Epoch 351, Loss: 0.26816630363464355\n",
      "Epoch 352, Loss: 0.26812833547592163\n",
      "Epoch 353, Loss: 0.26809048652648926\n",
      "Epoch 354, Loss: 0.26805269718170166\n",
      "Epoch 355, Loss: 0.2680153548717499\n",
      "Epoch 356, Loss: 0.2679787874221802\n",
      "Epoch 357, Loss: 0.2679421305656433\n",
      "Epoch 358, Loss: 0.26790550351142883\n",
      "Epoch 359, Loss: 0.2678696811199188\n",
      "Epoch 360, Loss: 0.2678338885307312\n",
      "Epoch 361, Loss: 0.2677982449531555\n",
      "Epoch 362, Loss: 0.26776331663131714\n",
      "Epoch 363, Loss: 0.26772812008857727\n",
      "Epoch 364, Loss: 0.2676938772201538\n",
      "Epoch 365, Loss: 0.2676595151424408\n",
      "Epoch 366, Loss: 0.2676256000995636\n",
      "Epoch 367, Loss: 0.2675916254520416\n",
      "Epoch 368, Loss: 0.26755836606025696\n",
      "Epoch 369, Loss: 0.2675250172615051\n",
      "Epoch 370, Loss: 0.26749175786972046\n",
      "Epoch 371, Loss: 0.26745885610580444\n",
      "Epoch 372, Loss: 0.267426460981369\n",
      "Epoch 373, Loss: 0.267394095659256\n",
      "Epoch 374, Loss: 0.2673622667789459\n",
      "Epoch 375, Loss: 0.26733043789863586\n",
      "Epoch 376, Loss: 0.26729917526245117\n",
      "Epoch 377, Loss: 0.26726776361465454\n",
      "Epoch 378, Loss: 0.2672367990016937\n",
      "Epoch 379, Loss: 0.2672061324119568\n",
      "Epoch 380, Loss: 0.2671752870082855\n",
      "Epoch 381, Loss: 0.2671452462673187\n",
      "Epoch 382, Loss: 0.2671148180961609\n",
      "Epoch 383, Loss: 0.2670850455760956\n",
      "Epoch 384, Loss: 0.26705509424209595\n",
      "Epoch 385, Loss: 0.26702603697776794\n",
      "Epoch 386, Loss: 0.26699674129486084\n",
      "Epoch 387, Loss: 0.2669679522514343\n",
      "Epoch 388, Loss: 0.266938716173172\n",
      "Epoch 389, Loss: 0.26691025495529175\n",
      "Epoch 390, Loss: 0.2668818235397339\n",
      "Epoch 391, Loss: 0.2668536901473999\n",
      "Epoch 392, Loss: 0.2668256163597107\n",
      "Epoch 393, Loss: 0.2667977511882782\n",
      "Epoch 394, Loss: 0.26677024364471436\n",
      "Epoch 395, Loss: 0.2667427957057953\n",
      "Epoch 396, Loss: 0.26671555638313293\n",
      "Epoch 397, Loss: 0.2666884958744049\n",
      "Epoch 398, Loss: 0.26666146516799927\n",
      "Epoch 399, Loss: 0.2666352093219757\n",
      "Epoch 400, Loss: 0.2666085362434387\n",
      "Epoch 401, Loss: 0.26658207178115845\n",
      "Epoch 402, Loss: 0.26655587553977966\n",
      "Epoch 403, Loss: 0.2665298581123352\n",
      "Epoch 404, Loss: 0.26650428771972656\n",
      "Epoch 405, Loss: 0.266478568315506\n",
      "Epoch 406, Loss: 0.2664533853530884\n",
      "Epoch 407, Loss: 0.2664279043674469\n",
      "Epoch 408, Loss: 0.26640263199806213\n",
      "Epoch 409, Loss: 0.2663775682449341\n",
      "Epoch 410, Loss: 0.2663528323173523\n",
      "Epoch 411, Loss: 0.2663280665874481\n",
      "Epoch 412, Loss: 0.2663036584854126\n",
      "Epoch 413, Loss: 0.2662793695926666\n",
      "Epoch 414, Loss: 0.2662550210952759\n",
      "Epoch 415, Loss: 0.2662312686443329\n",
      "Epoch 416, Loss: 0.26620742678642273\n",
      "Epoch 417, Loss: 0.2661837041378021\n",
      "Epoch 418, Loss: 0.26615986227989197\n",
      "Epoch 419, Loss: 0.2661365568637848\n",
      "Epoch 420, Loss: 0.26611316204071045\n",
      "Epoch 421, Loss: 0.26609012484550476\n",
      "Epoch 422, Loss: 0.266067236661911\n",
      "Epoch 423, Loss: 0.2660442888736725\n",
      "Epoch 424, Loss: 0.26602137088775635\n",
      "Epoch 425, Loss: 0.26599904894828796\n",
      "Epoch 426, Loss: 0.26597630977630615\n",
      "Epoch 427, Loss: 0.26595431566238403\n",
      "Epoch 428, Loss: 0.26593196392059326\n",
      "Epoch 429, Loss: 0.26590967178344727\n",
      "Epoch 430, Loss: 0.265887975692749\n",
      "Epoch 431, Loss: 0.26586616039276123\n",
      "Epoch 432, Loss: 0.265844464302063\n",
      "Epoch 433, Loss: 0.2658229470252991\n",
      "Epoch 434, Loss: 0.26580122113227844\n",
      "Epoch 435, Loss: 0.26578015089035034\n",
      "Epoch 436, Loss: 0.2657589912414551\n",
      "Epoch 437, Loss: 0.26573795080184937\n",
      "Epoch 438, Loss: 0.26571688055992126\n",
      "Epoch 439, Loss: 0.2656960189342499\n",
      "Epoch 440, Loss: 0.26567521691322327\n",
      "Epoch 441, Loss: 0.26565462350845337\n",
      "Epoch 442, Loss: 0.26563405990600586\n",
      "Epoch 443, Loss: 0.26561376452445984\n",
      "Epoch 444, Loss: 0.2655932903289795\n",
      "Epoch 445, Loss: 0.26557275652885437\n",
      "Epoch 446, Loss: 0.2655530869960785\n",
      "Epoch 447, Loss: 0.2655331492424011\n",
      "Epoch 448, Loss: 0.2655128538608551\n",
      "Epoch 449, Loss: 0.26549336314201355\n",
      "Epoch 450, Loss: 0.26547369360923767\n",
      "Epoch 451, Loss: 0.2654542922973633\n",
      "Epoch 452, Loss: 0.2654344439506531\n",
      "Epoch 453, Loss: 0.26541537046432495\n",
      "Epoch 454, Loss: 0.26539579033851624\n",
      "Epoch 455, Loss: 0.2653769254684448\n",
      "Epoch 456, Loss: 0.26535752415657043\n",
      "Epoch 457, Loss: 0.26533883810043335\n",
      "Epoch 458, Loss: 0.26531982421875\n",
      "Epoch 459, Loss: 0.2653011679649353\n",
      "Epoch 460, Loss: 0.2652822732925415\n",
      "Epoch 461, Loss: 0.26526373624801636\n",
      "Epoch 462, Loss: 0.2652452290058136\n",
      "Epoch 463, Loss: 0.2652267515659332\n",
      "Epoch 464, Loss: 0.26520827412605286\n",
      "Epoch 465, Loss: 0.26519012451171875\n",
      "Epoch 466, Loss: 0.26517173647880554\n",
      "Epoch 467, Loss: 0.26515382528305054\n",
      "Epoch 468, Loss: 0.26513588428497314\n",
      "Epoch 469, Loss: 0.26511815190315247\n",
      "Epoch 470, Loss: 0.26510000228881836\n",
      "Epoch 471, Loss: 0.26508232951164246\n",
      "Epoch 472, Loss: 0.2650643587112427\n",
      "Epoch 473, Loss: 0.2650468051433563\n",
      "Epoch 474, Loss: 0.26502910256385803\n",
      "Epoch 475, Loss: 0.2650120258331299\n",
      "Epoch 476, Loss: 0.26499471068382263\n",
      "Epoch 477, Loss: 0.26497742533683777\n",
      "Epoch 478, Loss: 0.2649601995944977\n",
      "Epoch 479, Loss: 0.26494312286376953\n",
      "Epoch 480, Loss: 0.26492539048194885\n",
      "Epoch 481, Loss: 0.2649086117744446\n",
      "Epoch 482, Loss: 0.26489195227622986\n",
      "Epoch 483, Loss: 0.2648749053478241\n",
      "Epoch 484, Loss: 0.2648584544658661\n",
      "Epoch 485, Loss: 0.26484188437461853\n",
      "Epoch 486, Loss: 0.2648250162601471\n",
      "Epoch 487, Loss: 0.2648085057735443\n",
      "Epoch 488, Loss: 0.2647918164730072\n",
      "Epoch 489, Loss: 0.2647753059864044\n",
      "Epoch 490, Loss: 0.26475873589515686\n",
      "Epoch 491, Loss: 0.2647426724433899\n",
      "Epoch 492, Loss: 0.264726459980011\n",
      "Epoch 493, Loss: 0.26471027731895447\n",
      "Epoch 494, Loss: 0.26469412446022034\n",
      "Epoch 495, Loss: 0.2646780014038086\n",
      "Epoch 496, Loss: 0.26466232538223267\n",
      "Epoch 497, Loss: 0.2646460235118866\n",
      "Epoch 498, Loss: 0.26463010907173157\n",
      "Epoch 499, Loss: 0.26461461186408997\n",
      "Epoch 500, Loss: 0.26459863781929016\n",
      "Epoch 501, Loss: 0.26458293199539185\n",
      "Epoch 502, Loss: 0.26456722617149353\n",
      "Epoch 503, Loss: 0.26455214619636536\n",
      "Epoch 504, Loss: 0.26453641057014465\n",
      "Epoch 505, Loss: 0.264521062374115\n",
      "Epoch 506, Loss: 0.2645053565502167\n",
      "Epoch 507, Loss: 0.26449042558670044\n",
      "Epoch 508, Loss: 0.2644747495651245\n",
      "Epoch 509, Loss: 0.26445960998535156\n",
      "Epoch 510, Loss: 0.26444417238235474\n",
      "Epoch 511, Loss: 0.2644292712211609\n",
      "Epoch 512, Loss: 0.2644142806529999\n",
      "Epoch 513, Loss: 0.26439937949180603\n",
      "Epoch 514, Loss: 0.2643844783306122\n",
      "Epoch 515, Loss: 0.264369398355484\n",
      "Epoch 516, Loss: 0.2643547058105469\n",
      "Epoch 517, Loss: 0.26433974504470825\n",
      "Epoch 518, Loss: 0.26432502269744873\n",
      "Epoch 519, Loss: 0.2643105983734131\n",
      "Epoch 520, Loss: 0.26429563760757446\n",
      "Epoch 521, Loss: 0.26428109407424927\n",
      "Epoch 522, Loss: 0.2642667293548584\n",
      "Epoch 523, Loss: 0.2642521262168884\n",
      "Epoch 524, Loss: 0.2642374336719513\n",
      "Epoch 525, Loss: 0.2642231583595276\n",
      "Epoch 526, Loss: 0.26420867443084717\n",
      "Epoch 527, Loss: 0.26419463753700256\n",
      "Epoch 528, Loss: 0.26418009400367737\n",
      "Epoch 529, Loss: 0.26416584849357605\n",
      "Epoch 530, Loss: 0.2641516625881195\n",
      "Epoch 531, Loss: 0.2641375660896301\n",
      "Epoch 532, Loss: 0.26412349939346313\n",
      "Epoch 533, Loss: 0.26410937309265137\n",
      "Epoch 534, Loss: 0.2640954256057739\n",
      "Epoch 535, Loss: 0.2640816271305084\n",
      "Epoch 536, Loss: 0.26406753063201904\n",
      "Epoch 537, Loss: 0.26405373215675354\n",
      "Epoch 538, Loss: 0.26403987407684326\n",
      "Epoch 539, Loss: 0.26402604579925537\n",
      "Epoch 540, Loss: 0.2640126645565033\n",
      "Epoch 541, Loss: 0.2639986574649811\n",
      "Epoch 542, Loss: 0.263985276222229\n",
      "Epoch 543, Loss: 0.26397135853767395\n",
      "Epoch 544, Loss: 0.2639578580856323\n",
      "Epoch 545, Loss: 0.263944149017334\n",
      "Epoch 546, Loss: 0.2639308571815491\n",
      "Epoch 547, Loss: 0.26391729712486267\n",
      "Epoch 548, Loss: 0.263903945684433\n",
      "Epoch 549, Loss: 0.2638903260231018\n",
      "Epoch 550, Loss: 0.26387715339660645\n",
      "Epoch 551, Loss: 0.26386362314224243\n",
      "Epoch 552, Loss: 0.26385051012039185\n",
      "Epoch 553, Loss: 0.26383739709854126\n",
      "Epoch 554, Loss: 0.2638244330883026\n",
      "Epoch 555, Loss: 0.2638108730316162\n",
      "Epoch 556, Loss: 0.2637980878353119\n",
      "Epoch 557, Loss: 0.26378464698791504\n",
      "Epoch 558, Loss: 0.26377159357070923\n",
      "Epoch 559, Loss: 0.26375865936279297\n",
      "Epoch 560, Loss: 0.2637456953525543\n",
      "Epoch 561, Loss: 0.26373255252838135\n",
      "Epoch 562, Loss: 0.2637197971343994\n",
      "Epoch 563, Loss: 0.2637070119380951\n",
      "Epoch 564, Loss: 0.26369398832321167\n",
      "Epoch 565, Loss: 0.2636813223361969\n",
      "Epoch 566, Loss: 0.26366859674453735\n",
      "Epoch 567, Loss: 0.2636556029319763\n",
      "Epoch 568, Loss: 0.26364293694496155\n",
      "Epoch 569, Loss: 0.2636304199695587\n",
      "Epoch 570, Loss: 0.2636176645755768\n",
      "Epoch 571, Loss: 0.2636052072048187\n",
      "Epoch 572, Loss: 0.2635924220085144\n",
      "Epoch 573, Loss: 0.26357993483543396\n",
      "Epoch 574, Loss: 0.263567179441452\n",
      "Epoch 575, Loss: 0.2635546326637268\n",
      "Epoch 576, Loss: 0.2635425925254822\n",
      "Epoch 577, Loss: 0.2635301649570465\n",
      "Epoch 578, Loss: 0.26351767778396606\n",
      "Epoch 579, Loss: 0.26350539922714233\n",
      "Epoch 580, Loss: 0.2634928226470947\n",
      "Epoch 581, Loss: 0.26348039507865906\n",
      "Epoch 582, Loss: 0.2634682357311249\n",
      "Epoch 583, Loss: 0.2634558081626892\n",
      "Epoch 584, Loss: 0.26344355940818787\n",
      "Epoch 585, Loss: 0.2634316384792328\n",
      "Epoch 586, Loss: 0.26341938972473145\n",
      "Epoch 587, Loss: 0.26340723037719727\n",
      "Epoch 588, Loss: 0.2633951008319855\n",
      "Epoch 589, Loss: 0.2633829712867737\n",
      "Epoch 590, Loss: 0.2633708417415619\n",
      "Epoch 591, Loss: 0.2633589208126068\n",
      "Epoch 592, Loss: 0.2633468210697174\n",
      "Epoch 593, Loss: 0.2633349299430847\n",
      "Epoch 594, Loss: 0.26332294940948486\n",
      "Epoch 595, Loss: 0.2633111774921417\n",
      "Epoch 596, Loss: 0.2632989287376404\n",
      "Epoch 597, Loss: 0.26328739523887634\n",
      "Epoch 598, Loss: 0.2632753849029541\n",
      "Epoch 599, Loss: 0.26326367259025574\n",
      "Epoch 600, Loss: 0.263251930475235\n",
      "Epoch 601, Loss: 0.26323971152305603\n",
      "Epoch 602, Loss: 0.2632283568382263\n",
      "Epoch 603, Loss: 0.26321667432785034\n",
      "Epoch 604, Loss: 0.26320481300354004\n",
      "Epoch 605, Loss: 0.2631934881210327\n",
      "Epoch 606, Loss: 0.2631819248199463\n",
      "Epoch 607, Loss: 0.26317018270492554\n",
      "Epoch 608, Loss: 0.2631584107875824\n",
      "Epoch 609, Loss: 0.26314663887023926\n",
      "Epoch 610, Loss: 0.26313504576683044\n",
      "Epoch 611, Loss: 0.26312366127967834\n",
      "Epoch 612, Loss: 0.26311221718788147\n",
      "Epoch 613, Loss: 0.26310083270072937\n",
      "Epoch 614, Loss: 0.26308953762054443\n",
      "Epoch 615, Loss: 0.2630781829357147\n",
      "Epoch 616, Loss: 0.26306673884391785\n",
      "Epoch 617, Loss: 0.26305514574050903\n",
      "Epoch 618, Loss: 0.26304367184638977\n",
      "Epoch 619, Loss: 0.2630327045917511\n",
      "Epoch 620, Loss: 0.2630210220813751\n",
      "Epoch 621, Loss: 0.26301002502441406\n",
      "Epoch 622, Loss: 0.26299870014190674\n",
      "Epoch 623, Loss: 0.26298701763153076\n",
      "Epoch 624, Loss: 0.2629760503768921\n",
      "Epoch 625, Loss: 0.26296499371528625\n",
      "Epoch 626, Loss: 0.2629539966583252\n",
      "Epoch 627, Loss: 0.2629426121711731\n",
      "Epoch 628, Loss: 0.26293128728866577\n",
      "Epoch 629, Loss: 0.2629201412200928\n",
      "Epoch 630, Loss: 0.26290929317474365\n",
      "Epoch 631, Loss: 0.262898325920105\n",
      "Epoch 632, Loss: 0.262887179851532\n",
      "Epoch 633, Loss: 0.2628762125968933\n",
      "Epoch 634, Loss: 0.26286518573760986\n",
      "Epoch 635, Loss: 0.2628537118434906\n",
      "Epoch 636, Loss: 0.2628428339958191\n",
      "Epoch 637, Loss: 0.26283198595046997\n",
      "Epoch 638, Loss: 0.2628210783004761\n",
      "Epoch 639, Loss: 0.2628103196620941\n",
      "Epoch 640, Loss: 0.26279953122138977\n",
      "Epoch 641, Loss: 0.26278871297836304\n",
      "Epoch 642, Loss: 0.2627775967121124\n",
      "Epoch 643, Loss: 0.2627664804458618\n",
      "Epoch 644, Loss: 0.26275578141212463\n",
      "Epoch 645, Loss: 0.2627452313899994\n",
      "Epoch 646, Loss: 0.26273468136787415\n",
      "Epoch 647, Loss: 0.2627238929271698\n",
      "Epoch 648, Loss: 0.2627125084400177\n",
      "Epoch 649, Loss: 0.2627022862434387\n",
      "Epoch 650, Loss: 0.26269131898880005\n",
      "Epoch 651, Loss: 0.26268064975738525\n",
      "Epoch 652, Loss: 0.2626698613166809\n",
      "Epoch 653, Loss: 0.26265937089920044\n",
      "Epoch 654, Loss: 0.26264873147010803\n",
      "Epoch 655, Loss: 0.26263827085494995\n",
      "Epoch 656, Loss: 0.26262784004211426\n",
      "Epoch 657, Loss: 0.262617290019989\n",
      "Epoch 658, Loss: 0.2626066505908966\n",
      "Epoch 659, Loss: 0.2625960409641266\n",
      "Epoch 660, Loss: 0.26258552074432373\n",
      "Epoch 661, Loss: 0.2625750005245209\n",
      "Epoch 662, Loss: 0.26256483793258667\n",
      "Epoch 663, Loss: 0.26255398988723755\n",
      "Epoch 664, Loss: 0.2625434696674347\n",
      "Epoch 665, Loss: 0.2625330984592438\n",
      "Epoch 666, Loss: 0.2625231146812439\n",
      "Epoch 667, Loss: 0.26251283288002014\n",
      "Epoch 668, Loss: 0.26250213384628296\n",
      "Epoch 669, Loss: 0.26249170303344727\n",
      "Epoch 670, Loss: 0.2624814510345459\n",
      "Epoch 671, Loss: 0.26247113943099976\n",
      "Epoch 672, Loss: 0.2624605596065521\n",
      "Epoch 673, Loss: 0.26245054602622986\n",
      "Epoch 674, Loss: 0.26244035363197327\n",
      "Epoch 675, Loss: 0.2624299228191376\n",
      "Epoch 676, Loss: 0.26241976022720337\n",
      "Epoch 677, Loss: 0.2624092102050781\n",
      "Epoch 678, Loss: 0.2623991370201111\n",
      "Epoch 679, Loss: 0.26238882541656494\n",
      "Epoch 680, Loss: 0.26237890124320984\n",
      "Epoch 681, Loss: 0.26236844062805176\n",
      "Epoch 682, Loss: 0.2623586058616638\n",
      "Epoch 683, Loss: 0.26234859228134155\n",
      "Epoch 684, Loss: 0.2623383104801178\n",
      "Epoch 685, Loss: 0.26232847571372986\n",
      "Epoch 686, Loss: 0.2623181641101837\n",
      "Epoch 687, Loss: 0.26230794191360474\n",
      "Epoch 688, Loss: 0.26229825615882874\n",
      "Epoch 689, Loss: 0.2622879445552826\n",
      "Epoch 690, Loss: 0.2622780501842499\n",
      "Epoch 691, Loss: 0.262267529964447\n",
      "Epoch 692, Loss: 0.26225802302360535\n",
      "Epoch 693, Loss: 0.26224789023399353\n",
      "Epoch 694, Loss: 0.2622378170490265\n",
      "Epoch 695, Loss: 0.26222777366638184\n",
      "Epoch 696, Loss: 0.2622179687023163\n",
      "Epoch 697, Loss: 0.2622080445289612\n",
      "Epoch 698, Loss: 0.26219838857650757\n",
      "Epoch 699, Loss: 0.2621884346008301\n",
      "Epoch 700, Loss: 0.26217859983444214\n",
      "Epoch 701, Loss: 0.2621687352657318\n",
      "Epoch 702, Loss: 0.2621586322784424\n",
      "Epoch 703, Loss: 0.26214906573295593\n",
      "Epoch 704, Loss: 0.2621387839317322\n",
      "Epoch 705, Loss: 0.2621295750141144\n",
      "Epoch 706, Loss: 0.2621196210384369\n",
      "Epoch 707, Loss: 0.2621096670627594\n",
      "Epoch 708, Loss: 0.2621001899242401\n",
      "Epoch 709, Loss: 0.26209017634391785\n",
      "Epoch 710, Loss: 0.2620806097984314\n",
      "Epoch 711, Loss: 0.2620709538459778\n",
      "Epoch 712, Loss: 0.2620612680912018\n",
      "Epoch 713, Loss: 0.26205140352249146\n",
      "Epoch 714, Loss: 0.262041836977005\n",
      "Epoch 715, Loss: 0.2620321214199066\n",
      "Epoch 716, Loss: 0.2620222568511963\n",
      "Epoch 717, Loss: 0.2620130181312561\n",
      "Epoch 718, Loss: 0.2620030641555786\n",
      "Epoch 719, Loss: 0.2619935870170593\n",
      "Epoch 720, Loss: 0.26198408007621765\n",
      "Epoch 721, Loss: 0.261974573135376\n",
      "Epoch 722, Loss: 0.26196449995040894\n",
      "Epoch 723, Loss: 0.261955201625824\n",
      "Epoch 724, Loss: 0.26194557547569275\n",
      "Epoch 725, Loss: 0.2619365453720093\n",
      "Epoch 726, Loss: 0.26192647218704224\n",
      "Epoch 727, Loss: 0.2619169354438782\n",
      "Epoch 728, Loss: 0.261907696723938\n",
      "Epoch 729, Loss: 0.26189836859703064\n",
      "Epoch 730, Loss: 0.2618887722492218\n",
      "Epoch 731, Loss: 0.26187896728515625\n",
      "Epoch 732, Loss: 0.26187002658843994\n",
      "Epoch 733, Loss: 0.2618604898452759\n",
      "Epoch 734, Loss: 0.26185110211372375\n",
      "Epoch 735, Loss: 0.26184147596359253\n",
      "Epoch 736, Loss: 0.26183202862739563\n",
      "Epoch 737, Loss: 0.26182276010513306\n",
      "Epoch 738, Loss: 0.2618134915828705\n",
      "Epoch 739, Loss: 0.26180416345596313\n",
      "Epoch 740, Loss: 0.26179465651512146\n",
      "Epoch 741, Loss: 0.26178544759750366\n",
      "Epoch 742, Loss: 0.26177629828453064\n",
      "Epoch 743, Loss: 0.261766642332077\n",
      "Epoch 744, Loss: 0.26175767183303833\n",
      "Epoch 745, Loss: 0.2617480754852295\n",
      "Epoch 746, Loss: 0.2617388963699341\n",
      "Epoch 747, Loss: 0.2617298662662506\n",
      "Epoch 748, Loss: 0.2617204189300537\n",
      "Epoch 749, Loss: 0.2617114186286926\n",
      "Epoch 750, Loss: 0.26170191168785095\n",
      "Epoch 751, Loss: 0.26169276237487793\n",
      "Epoch 752, Loss: 0.2616836428642273\n",
      "Epoch 753, Loss: 0.26167476177215576\n",
      "Epoch 754, Loss: 0.26166510581970215\n",
      "Epoch 755, Loss: 0.26165634393692017\n",
      "Epoch 756, Loss: 0.26164668798446655\n",
      "Epoch 757, Loss: 0.26163771748542786\n",
      "Epoch 758, Loss: 0.2616285979747772\n",
      "Epoch 759, Loss: 0.2616197168827057\n",
      "Epoch 760, Loss: 0.2616104483604431\n",
      "Epoch 761, Loss: 0.26160117983818054\n",
      "Epoch 762, Loss: 0.2615920603275299\n",
      "Epoch 763, Loss: 0.2615830898284912\n",
      "Epoch 764, Loss: 0.2615744173526764\n",
      "Epoch 765, Loss: 0.26156508922576904\n",
      "Epoch 766, Loss: 0.2615562975406647\n",
      "Epoch 767, Loss: 0.26154688000679016\n",
      "Epoch 768, Loss: 0.26153793931007385\n",
      "Epoch 769, Loss: 0.26152899861335754\n",
      "Epoch 770, Loss: 0.2615199685096741\n",
      "Epoch 771, Loss: 0.2615111768245697\n",
      "Epoch 772, Loss: 0.2615022659301758\n",
      "Epoch 773, Loss: 0.2614929974079132\n",
      "Epoch 774, Loss: 0.26148414611816406\n",
      "Epoch 775, Loss: 0.2614752948284149\n",
      "Epoch 776, Loss: 0.2614661753177643\n",
      "Epoch 777, Loss: 0.2614575922489166\n",
      "Epoch 778, Loss: 0.2614486813545227\n",
      "Epoch 779, Loss: 0.26143980026245117\n",
      "Epoch 780, Loss: 0.2614307403564453\n",
      "Epoch 781, Loss: 0.26142194867134094\n",
      "Epoch 782, Loss: 0.2614131569862366\n",
      "Epoch 783, Loss: 0.2614041268825531\n",
      "Epoch 784, Loss: 0.2613951563835144\n",
      "Epoch 785, Loss: 0.26138633489608765\n",
      "Epoch 786, Loss: 0.2613776922225952\n",
      "Epoch 787, Loss: 0.26136866211891174\n",
      "Epoch 788, Loss: 0.26135995984077454\n",
      "Epoch 789, Loss: 0.26135116815567017\n",
      "Epoch 790, Loss: 0.2613426148891449\n",
      "Epoch 791, Loss: 0.261333703994751\n",
      "Epoch 792, Loss: 0.2613253891468048\n",
      "Epoch 793, Loss: 0.26131612062454224\n",
      "Epoch 794, Loss: 0.26130735874176025\n",
      "Epoch 795, Loss: 0.261298805475235\n",
      "Epoch 796, Loss: 0.2612898349761963\n",
      "Epoch 797, Loss: 0.26128146052360535\n",
      "Epoch 798, Loss: 0.26127293705940247\n",
      "Epoch 799, Loss: 0.2612638771533966\n",
      "Epoch 800, Loss: 0.2612552344799042\n",
      "Epoch 801, Loss: 0.2612467110157013\n",
      "Epoch 802, Loss: 0.26123836636543274\n",
      "Epoch 803, Loss: 0.26122918725013733\n",
      "Epoch 804, Loss: 0.2612205147743225\n",
      "Epoch 805, Loss: 0.2612121105194092\n",
      "Epoch 806, Loss: 0.2612032890319824\n",
      "Epoch 807, Loss: 0.2611945867538452\n",
      "Epoch 808, Loss: 0.2611863315105438\n",
      "Epoch 809, Loss: 0.26117751002311707\n",
      "Epoch 810, Loss: 0.2611694633960724\n",
      "Epoch 811, Loss: 0.26116055250167847\n",
      "Epoch 812, Loss: 0.2611522674560547\n",
      "Epoch 813, Loss: 0.26114338636398315\n",
      "Epoch 814, Loss: 0.26113465428352356\n",
      "Epoch 815, Loss: 0.261126309633255\n",
      "Epoch 816, Loss: 0.2611180543899536\n",
      "Epoch 817, Loss: 0.261109322309494\n",
      "Epoch 818, Loss: 0.2611006498336792\n",
      "Epoch 819, Loss: 0.26109224557876587\n",
      "Epoch 820, Loss: 0.26108381152153015\n",
      "Epoch 821, Loss: 0.2610754370689392\n",
      "Epoch 822, Loss: 0.26106706261634827\n",
      "Epoch 823, Loss: 0.26105839014053345\n",
      "Epoch 824, Loss: 0.26105019450187683\n",
      "Epoch 825, Loss: 0.26104113459587097\n",
      "Epoch 826, Loss: 0.2610331177711487\n",
      "Epoch 827, Loss: 0.26102471351623535\n",
      "Epoch 828, Loss: 0.2610164284706116\n",
      "Epoch 829, Loss: 0.2610076367855072\n",
      "Epoch 830, Loss: 0.2609994411468506\n",
      "Epoch 831, Loss: 0.26099109649658203\n",
      "Epoch 832, Loss: 0.26098260283470154\n",
      "Epoch 833, Loss: 0.2609741687774658\n",
      "Epoch 834, Loss: 0.26096591353416443\n",
      "Epoch 835, Loss: 0.2609579861164093\n",
      "Epoch 836, Loss: 0.26094919443130493\n",
      "Epoch 837, Loss: 0.26094087958335876\n",
      "Epoch 838, Loss: 0.2609327733516693\n",
      "Epoch 839, Loss: 0.26092442870140076\n",
      "Epoch 840, Loss: 0.26091593503952026\n",
      "Epoch 841, Loss: 0.2609077990055084\n",
      "Epoch 842, Loss: 0.26089948415756226\n",
      "Epoch 843, Loss: 0.2608911693096161\n",
      "Epoch 844, Loss: 0.26088282465934753\n",
      "Epoch 845, Loss: 0.26087483763694763\n",
      "Epoch 846, Loss: 0.26086652278900146\n",
      "Epoch 847, Loss: 0.2608579695224762\n",
      "Epoch 848, Loss: 0.2608499526977539\n",
      "Epoch 849, Loss: 0.2608414590358734\n",
      "Epoch 850, Loss: 0.2608332931995392\n",
      "Epoch 851, Loss: 0.2608250379562378\n",
      "Epoch 852, Loss: 0.26081693172454834\n",
      "Epoch 853, Loss: 0.2608085870742798\n",
      "Epoch 854, Loss: 0.2608007490634918\n",
      "Epoch 855, Loss: 0.2607927620410919\n",
      "Epoch 856, Loss: 0.2607845366001129\n",
      "Epoch 857, Loss: 0.2607761323451996\n",
      "Epoch 858, Loss: 0.2607681453227997\n",
      "Epoch 859, Loss: 0.2607600688934326\n",
      "Epoch 860, Loss: 0.26075178384780884\n",
      "Epoch 861, Loss: 0.2607438862323761\n",
      "Epoch 862, Loss: 0.26073557138442993\n",
      "Epoch 863, Loss: 0.26072758436203003\n",
      "Epoch 864, Loss: 0.2607191503047943\n",
      "Epoch 865, Loss: 0.2607114315032959\n",
      "Epoch 866, Loss: 0.26070329546928406\n",
      "Epoch 867, Loss: 0.26069459319114685\n",
      "Epoch 868, Loss: 0.2606869041919708\n",
      "Epoch 869, Loss: 0.26067912578582764\n",
      "Epoch 870, Loss: 0.26067090034484863\n",
      "Epoch 871, Loss: 0.26066267490386963\n",
      "Epoch 872, Loss: 0.26065483689308167\n",
      "Epoch 873, Loss: 0.26064687967300415\n",
      "Epoch 874, Loss: 0.260638564825058\n",
      "Epoch 875, Loss: 0.2606307864189148\n",
      "Epoch 876, Loss: 0.26062270998954773\n",
      "Epoch 877, Loss: 0.2606147527694702\n",
      "Epoch 878, Loss: 0.26060670614242554\n",
      "Epoch 879, Loss: 0.2605985701084137\n",
      "Epoch 880, Loss: 0.26059064269065857\n",
      "Epoch 881, Loss: 0.260582834482193\n",
      "Epoch 882, Loss: 0.2605748176574707\n",
      "Epoch 883, Loss: 0.2605670690536499\n",
      "Epoch 884, Loss: 0.26055908203125\n",
      "Epoch 885, Loss: 0.2605510950088501\n",
      "Epoch 886, Loss: 0.26054295897483826\n",
      "Epoch 887, Loss: 0.26053503155708313\n",
      "Epoch 888, Loss: 0.26052722334861755\n",
      "Epoch 889, Loss: 0.2605193853378296\n",
      "Epoch 890, Loss: 0.26051148772239685\n",
      "Epoch 891, Loss: 0.26050353050231934\n",
      "Epoch 892, Loss: 0.26049551367759705\n",
      "Epoch 893, Loss: 0.2604878544807434\n",
      "Epoch 894, Loss: 0.26047977805137634\n",
      "Epoch 895, Loss: 0.26047220826148987\n",
      "Epoch 896, Loss: 0.26046425104141235\n",
      "Epoch 897, Loss: 0.2604560852050781\n",
      "Epoch 898, Loss: 0.2604486346244812\n",
      "Epoch 899, Loss: 0.2604408860206604\n",
      "Epoch 900, Loss: 0.2604326903820038\n",
      "Epoch 901, Loss: 0.26042479276657104\n",
      "Epoch 902, Loss: 0.26041749119758606\n",
      "Epoch 903, Loss: 0.260409414768219\n",
      "Epoch 904, Loss: 0.2604016065597534\n",
      "Epoch 905, Loss: 0.2603938579559326\n",
      "Epoch 906, Loss: 0.2603859603404999\n",
      "Epoch 907, Loss: 0.26037830114364624\n",
      "Epoch 908, Loss: 0.26037073135375977\n",
      "Epoch 909, Loss: 0.260362446308136\n",
      "Epoch 910, Loss: 0.2603549361228943\n",
      "Epoch 911, Loss: 0.26034724712371826\n",
      "Epoch 912, Loss: 0.260339617729187\n",
      "Epoch 913, Loss: 0.26033201813697815\n",
      "Epoch 914, Loss: 0.2603238523006439\n",
      "Epoch 915, Loss: 0.2603166103363037\n",
      "Epoch 916, Loss: 0.26030856370925903\n",
      "Epoch 917, Loss: 0.2603010833263397\n",
      "Epoch 918, Loss: 0.26029354333877563\n",
      "Epoch 919, Loss: 0.2602856755256653\n",
      "Epoch 920, Loss: 0.2602781653404236\n",
      "Epoch 921, Loss: 0.26026999950408936\n",
      "Epoch 922, Loss: 0.260262668132782\n",
      "Epoch 923, Loss: 0.26025450229644775\n",
      "Epoch 924, Loss: 0.2602473497390747\n",
      "Epoch 925, Loss: 0.26023972034454346\n",
      "Epoch 926, Loss: 0.26023218035697937\n",
      "Epoch 927, Loss: 0.2602243721485138\n",
      "Epoch 928, Loss: 0.260216623544693\n",
      "Epoch 929, Loss: 0.26020893454551697\n",
      "Epoch 930, Loss: 0.26020148396492004\n",
      "Epoch 931, Loss: 0.260193794965744\n",
      "Epoch 932, Loss: 0.26018625497817993\n",
      "Epoch 933, Loss: 0.26017871499061584\n",
      "Epoch 934, Loss: 0.2601713240146637\n",
      "Epoch 935, Loss: 0.26016369462013245\n",
      "Epoch 936, Loss: 0.26015567779541016\n",
      "Epoch 937, Loss: 0.26014867424964905\n",
      "Epoch 938, Loss: 0.26014095544815063\n",
      "Epoch 939, Loss: 0.2601335644721985\n",
      "Epoch 940, Loss: 0.26012566685676575\n",
      "Epoch 941, Loss: 0.26011842489242554\n",
      "Epoch 942, Loss: 0.2601107060909271\n",
      "Epoch 943, Loss: 0.2601030170917511\n",
      "Epoch 944, Loss: 0.26009559631347656\n",
      "Epoch 945, Loss: 0.2600880563259125\n",
      "Epoch 946, Loss: 0.2600805461406708\n",
      "Epoch 947, Loss: 0.260073184967041\n",
      "Epoch 948, Loss: 0.2600656747817993\n",
      "Epoch 949, Loss: 0.2600581645965576\n",
      "Epoch 950, Loss: 0.26005053520202637\n",
      "Epoch 951, Loss: 0.26004326343536377\n",
      "Epoch 952, Loss: 0.2600357234477997\n",
      "Epoch 953, Loss: 0.26002806425094604\n",
      "Epoch 954, Loss: 0.26002055406570435\n",
      "Epoch 955, Loss: 0.2600131630897522\n",
      "Epoch 956, Loss: 0.2600058913230896\n",
      "Epoch 957, Loss: 0.259998619556427\n",
      "Epoch 958, Loss: 0.25999099016189575\n",
      "Epoch 959, Loss: 0.2599833607673645\n",
      "Epoch 960, Loss: 0.2599762976169586\n",
      "Epoch 961, Loss: 0.2599687874317169\n",
      "Epoch 962, Loss: 0.25996124744415283\n",
      "Epoch 963, Loss: 0.2599537968635559\n",
      "Epoch 964, Loss: 0.25994664430618286\n",
      "Epoch 965, Loss: 0.2599392533302307\n",
      "Epoch 966, Loss: 0.25993192195892334\n",
      "Epoch 967, Loss: 0.25992417335510254\n",
      "Epoch 968, Loss: 0.25991711020469666\n",
      "Epoch 969, Loss: 0.2599096894264221\n",
      "Epoch 970, Loss: 0.2599020302295685\n",
      "Epoch 971, Loss: 0.25989508628845215\n",
      "Epoch 972, Loss: 0.2598879039287567\n",
      "Epoch 973, Loss: 0.25988027453422546\n",
      "Epoch 974, Loss: 0.2598731517791748\n",
      "Epoch 975, Loss: 0.259865403175354\n",
      "Epoch 976, Loss: 0.25985848903656006\n",
      "Epoch 977, Loss: 0.2598510682582855\n",
      "Epoch 978, Loss: 0.259843647480011\n",
      "Epoch 979, Loss: 0.2598363161087036\n",
      "Epoch 980, Loss: 0.25982919335365295\n",
      "Epoch 981, Loss: 0.2598218321800232\n",
      "Epoch 982, Loss: 0.2598145306110382\n",
      "Epoch 983, Loss: 0.25980740785598755\n",
      "Epoch 984, Loss: 0.2598003149032593\n",
      "Epoch 985, Loss: 0.2597924768924713\n",
      "Epoch 986, Loss: 0.2597855031490326\n",
      "Epoch 987, Loss: 0.25977811217308044\n",
      "Epoch 988, Loss: 0.2597707211971283\n",
      "Epoch 989, Loss: 0.25976377725601196\n",
      "Epoch 990, Loss: 0.2597567141056061\n",
      "Epoch 991, Loss: 0.25974923372268677\n",
      "Epoch 992, Loss: 0.2597419321537018\n",
      "Epoch 993, Loss: 0.2597348392009735\n",
      "Epoch 994, Loss: 0.2597277760505676\n",
      "Epoch 995, Loss: 0.2597203254699707\n",
      "Epoch 996, Loss: 0.25971320271492004\n",
      "Epoch 997, Loss: 0.2597058415412903\n",
      "Epoch 998, Loss: 0.25969868898391724\n",
      "Epoch 999, Loss: 0.2596920132637024\n"
     ]
    }
   ],
   "source": [
    "alphaset = torch.randn((datasize, d), dtype=torch.float32, requires_grad=True)\n",
    "lambda_ = 0.001\n",
    "\n",
    "def objective(alphaset: torch.Tensor):\n",
    "    U = torch.zeros((datasize, d), dtype=torch.float32)\n",
    "    U = torch.einsum(\"ijkl, jk -> il\", K_kernel, alphaset)\n",
    "\n",
    "    l = loss(U)\n",
    "    r = reg(alphaset)\n",
    "\n",
    "    return l + lambda_ * r\n",
    "\n",
    "\n",
    "\n",
    "def loss(U: torch.Tensor):\n",
    "\n",
    "    loss_value = 0.0\n",
    "    for i in range(datasize):\n",
    "\n",
    "        p_vec = torch.zeros((d, 1), dtype=torch.float32)\n",
    "\n",
    "        hS_i = torch.tensor(\n",
    "            in_sample_transactions.iloc[i][\"offered_product_one_hot\"],\n",
    "            dtype=torch.float32,\n",
    "        ).view(-1, 1)\n",
    "\n",
    "        y_i = torch.tensor(\n",
    "            in_sample_transactions.iloc[i][\"choice_one_hot\"], dtype=torch.float32\n",
    "        ).view(-1, 1)\n",
    "\n",
    "        utility_hSi = U[i].view(-1, 1)\n",
    "        exp_utility = torch.exp(utility_hSi)\n",
    "        sum_exp_utility = torch.sum(exp_utility) + 1\n",
    "\n",
    "        for j in range(d):\n",
    "\n",
    "            if hS_i[j] == 1:\n",
    "\n",
    "                p_vec[j] = torch.exp(utility_hSi[j]) / sum_exp_utility\n",
    "            else:\n",
    "\n",
    "                p_vec[j] = 0\n",
    "\n",
    "        loss_value += cross_entropy_loss(p_vec, y_i)\n",
    "    return loss_value / datasize\n",
    "\n",
    "\n",
    "def cross_entropy_loss(p_vec: torch.Tensor, y_vec: torch.Tensor):\n",
    "    # print('y_vec',y_vec)\n",
    "    for i in range(d):\n",
    "        if y_vec[i] == 1:\n",
    "            return -torch.log(p_vec[i])\n",
    "    return 0\n",
    "\n",
    "\n",
    "def squared_loss(p_vec: torch.Tensor, y_vec: torch.Tensor):\n",
    "    return torch.sum((p_vec - y_vec) ** 2)\n",
    "\n",
    "\n",
    "def reg(alphaset: torch.Tensor):\n",
    "\n",
    "    alphaset = alphaset.unsqueeze(2)  # 增加一个维度以便执行einsum\n",
    "\n",
    "    # 正则化项的计算\n",
    "    result = torch.einsum(\"ikd,ijkl,jle->\", alphaset, K_kernel, alphaset)\n",
    "\n",
    "    return result\n",
    "\n",
    "def compute_gradient():\n",
    "    objective_value = objective(alphaset)\n",
    "    objective_value.backward()  # 计算梯度\n",
    "    return alphaset.grad\n",
    "\n",
    "optimizer = torch.optim.Adam([alphaset], lr=0.01)\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()  \n",
    "    loss_value = objective(alphaset)\n",
    "    loss_value.backward()  \n",
    "    optimizer.step()  \n",
    "\n",
    "    print(f\"Epoch {epoch}, Loss: {loss_value.item()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
