{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create a folder in the root directory\n",
        "!mkdir -p \"/content/drive/My Drive/HOTEL_log_Sep26\"\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMMQBtJrGvSK",
        "outputId": "808d540a-4c3c-48f7-ab94-7e8930e19b80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "root_dir = \"/content/drive/My Drive/HOTEL_log_Sep26/\""
      ],
      "metadata": {
        "id": "ObAlwmfDJyCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUVunDyt7_iS"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "import math\n",
        "from scipy import stats\n",
        "import copy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "# import matplotlib.tri as tri\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# upload berbeglia's generated data\n",
        "!git clone https://github.com/Layneww/choice-models-sushi-hotel.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvcEa30Z5MFa",
        "outputId": "fb1a8023-fdde-4fc5-ac77-b10599a6590e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'choice-models-sushi-hotel'...\n",
            "remote: Enumerating objects: 346, done.\u001b[K\n",
            "remote: Counting objects: 100% (143/143), done.\u001b[K\n",
            "remote: Compressing objects: 100% (134/134), done.\u001b[K\n",
            "remote: Total 346 (delta 100), reused 21 (delta 7), pack-reused 203 (from 1)\u001b[K\n",
            "Receiving objects: 100% (346/346), 97.79 MiB | 30.28 MiB/s, done.\n",
            "Resolving deltas: 100% (176/176), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd choice-models-sushi-hotel/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIKXa6W47n6y",
        "outputId": "3da52c88-6520-41e3-e6e6-ae19a59fdf0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/choice-models-sushi-hotel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python setup.py install"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYrvcyUA6ZNl",
        "outputId": "73f7c5a0-fd45-4eb2-f1a8-05e4148a2d10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running install\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating python_choice_models.egg-info\n",
            "writing python_choice_models.egg-info/PKG-INFO\n",
            "writing dependency_links to python_choice_models.egg-info/dependency_links.txt\n",
            "writing requirements to python_choice_models.egg-info/requires.txt\n",
            "writing top-level names to python_choice_models.egg-info/top_level.txt\n",
            "writing manifest file 'python_choice_models.egg-info/SOURCES.txt'\n",
            "reading manifest file 'python_choice_models.egg-info/SOURCES.txt'\n",
            "writing manifest file 'python_choice_models.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/python_choice_models\n",
            "copying python_choice_models/settings.py -> build/lib/python_choice_models\n",
            "copying python_choice_models/__init__.py -> build/lib/python_choice_models\n",
            "copying python_choice_models/utils.py -> build/lib/python_choice_models\n",
            "copying python_choice_models/profiler.py -> build/lib/python_choice_models\n",
            "creating build/lib/python_choice_models/estimation\n",
            "copying python_choice_models/estimation/ranked_list.py -> build/lib/python_choice_models/estimation\n",
            "copying python_choice_models/estimation/__init__.py -> build/lib/python_choice_models/estimation\n",
            "creating build/lib/python_choice_models/optimization\n",
            "copying python_choice_models/optimization/non_linear.py -> build/lib/python_choice_models/optimization\n",
            "copying python_choice_models/optimization/__init__.py -> build/lib/python_choice_models/optimization\n",
            "copying python_choice_models/optimization/linear.py -> build/lib/python_choice_models/optimization\n",
            "creating build/lib/python_choice_models/transactions\n",
            "copying python_choice_models/transactions/__init__.py -> build/lib/python_choice_models/transactions\n",
            "copying python_choice_models/transactions/base.py -> build/lib/python_choice_models/transactions\n",
            "creating build/lib/python_choice_models/models\n",
            "copying python_choice_models/models/multinomial_logit.py -> build/lib/python_choice_models/models\n",
            "copying python_choice_models/models/exponomial.py -> build/lib/python_choice_models/models\n",
            "copying python_choice_models/models/ranked_list.py -> build/lib/python_choice_models/models\n",
            "copying python_choice_models/models/markov_chain.py -> build/lib/python_choice_models/models\n",
            "copying python_choice_models/models/random_choice.py -> build/lib/python_choice_models/models\n",
            "copying python_choice_models/models/__init__.py -> build/lib/python_choice_models/models\n",
            "copying python_choice_models/models/latent_class.py -> build/lib/python_choice_models/models\n",
            "copying python_choice_models/models/mixed_logit.py -> build/lib/python_choice_models/models\n",
            "copying python_choice_models/models/markov_chain_rank_2.py -> build/lib/python_choice_models/models\n",
            "copying python_choice_models/models/nested_logit.py -> build/lib/python_choice_models/models\n",
            "copying python_choice_models/models/generalized_stochastic_preference.py -> build/lib/python_choice_models/models\n",
            "creating build/lib/python_choice_models/estimation/maximum_likelihood\n",
            "copying python_choice_models/estimation/maximum_likelihood/ranked_list.py -> build/lib/python_choice_models/estimation/maximum_likelihood\n",
            "copying python_choice_models/estimation/maximum_likelihood/random_choice.py -> build/lib/python_choice_models/estimation/maximum_likelihood\n",
            "copying python_choice_models/estimation/maximum_likelihood/__init__.py -> build/lib/python_choice_models/estimation/maximum_likelihood\n",
            "copying python_choice_models/estimation/maximum_likelihood/latent_class.py -> build/lib/python_choice_models/estimation/maximum_likelihood\n",
            "creating build/lib/python_choice_models/estimation/expectation_maximization\n",
            "copying python_choice_models/estimation/expectation_maximization/ranked_list.py -> build/lib/python_choice_models/estimation/expectation_maximization\n",
            "copying python_choice_models/estimation/expectation_maximization/markov_chain.py -> build/lib/python_choice_models/estimation/expectation_maximization\n",
            "copying python_choice_models/estimation/expectation_maximization/__init__.py -> build/lib/python_choice_models/estimation/expectation_maximization\n",
            "copying python_choice_models/estimation/expectation_maximization/latent_class.py -> build/lib/python_choice_models/estimation/expectation_maximization\n",
            "creating build/lib/python_choice_models/estimation/market_explore\n",
            "copying python_choice_models/estimation/market_explore/ranked_list.py -> build/lib/python_choice_models/estimation/market_explore\n",
            "copying python_choice_models/estimation/market_explore/__init__.py -> build/lib/python_choice_models/estimation/market_explore\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/python_choice_models\n",
            "copying build/lib/python_choice_models/settings.py -> build/bdist.linux-x86_64/egg/python_choice_models\n",
            "copying build/lib/python_choice_models/__init__.py -> build/bdist.linux-x86_64/egg/python_choice_models\n",
            "creating build/bdist.linux-x86_64/egg/python_choice_models/estimation\n",
            "creating build/bdist.linux-x86_64/egg/python_choice_models/estimation/maximum_likelihood\n",
            "copying build/lib/python_choice_models/estimation/maximum_likelihood/ranked_list.py -> build/bdist.linux-x86_64/egg/python_choice_models/estimation/maximum_likelihood\n",
            "copying build/lib/python_choice_models/estimation/maximum_likelihood/random_choice.py -> build/bdist.linux-x86_64/egg/python_choice_models/estimation/maximum_likelihood\n",
            "copying build/lib/python_choice_models/estimation/maximum_likelihood/__init__.py -> build/bdist.linux-x86_64/egg/python_choice_models/estimation/maximum_likelihood\n",
            "copying build/lib/python_choice_models/estimation/maximum_likelihood/latent_class.py -> build/bdist.linux-x86_64/egg/python_choice_models/estimation/maximum_likelihood\n",
            "copying build/lib/python_choice_models/estimation/ranked_list.py -> build/bdist.linux-x86_64/egg/python_choice_models/estimation\n",
            "creating build/bdist.linux-x86_64/egg/python_choice_models/estimation/expectation_maximization\n",
            "copying build/lib/python_choice_models/estimation/expectation_maximization/ranked_list.py -> build/bdist.linux-x86_64/egg/python_choice_models/estimation/expectation_maximization\n",
            "copying build/lib/python_choice_models/estimation/expectation_maximization/markov_chain.py -> build/bdist.linux-x86_64/egg/python_choice_models/estimation/expectation_maximization\n",
            "copying build/lib/python_choice_models/estimation/expectation_maximization/__init__.py -> build/bdist.linux-x86_64/egg/python_choice_models/estimation/expectation_maximization\n",
            "copying build/lib/python_choice_models/estimation/expectation_maximization/latent_class.py -> build/bdist.linux-x86_64/egg/python_choice_models/estimation/expectation_maximization\n",
            "copying build/lib/python_choice_models/estimation/__init__.py -> build/bdist.linux-x86_64/egg/python_choice_models/estimation\n",
            "creating build/bdist.linux-x86_64/egg/python_choice_models/estimation/market_explore\n",
            "copying build/lib/python_choice_models/estimation/market_explore/ranked_list.py -> build/bdist.linux-x86_64/egg/python_choice_models/estimation/market_explore\n",
            "copying build/lib/python_choice_models/estimation/market_explore/__init__.py -> build/bdist.linux-x86_64/egg/python_choice_models/estimation/market_explore\n",
            "creating build/bdist.linux-x86_64/egg/python_choice_models/optimization\n",
            "copying build/lib/python_choice_models/optimization/non_linear.py -> build/bdist.linux-x86_64/egg/python_choice_models/optimization\n",
            "copying build/lib/python_choice_models/optimization/__init__.py -> build/bdist.linux-x86_64/egg/python_choice_models/optimization\n",
            "copying build/lib/python_choice_models/optimization/linear.py -> build/bdist.linux-x86_64/egg/python_choice_models/optimization\n",
            "copying build/lib/python_choice_models/utils.py -> build/bdist.linux-x86_64/egg/python_choice_models\n",
            "creating build/bdist.linux-x86_64/egg/python_choice_models/transactions\n",
            "copying build/lib/python_choice_models/transactions/__init__.py -> build/bdist.linux-x86_64/egg/python_choice_models/transactions\n",
            "copying build/lib/python_choice_models/transactions/base.py -> build/bdist.linux-x86_64/egg/python_choice_models/transactions\n",
            "creating build/bdist.linux-x86_64/egg/python_choice_models/models\n",
            "copying build/lib/python_choice_models/models/multinomial_logit.py -> build/bdist.linux-x86_64/egg/python_choice_models/models\n",
            "copying build/lib/python_choice_models/models/exponomial.py -> build/bdist.linux-x86_64/egg/python_choice_models/models\n",
            "copying build/lib/python_choice_models/models/ranked_list.py -> build/bdist.linux-x86_64/egg/python_choice_models/models\n",
            "copying build/lib/python_choice_models/models/markov_chain.py -> build/bdist.linux-x86_64/egg/python_choice_models/models\n",
            "copying build/lib/python_choice_models/models/random_choice.py -> build/bdist.linux-x86_64/egg/python_choice_models/models\n",
            "copying build/lib/python_choice_models/models/__init__.py -> build/bdist.linux-x86_64/egg/python_choice_models/models\n",
            "copying build/lib/python_choice_models/models/latent_class.py -> build/bdist.linux-x86_64/egg/python_choice_models/models\n",
            "copying build/lib/python_choice_models/models/mixed_logit.py -> build/bdist.linux-x86_64/egg/python_choice_models/models\n",
            "copying build/lib/python_choice_models/models/markov_chain_rank_2.py -> build/bdist.linux-x86_64/egg/python_choice_models/models\n",
            "copying build/lib/python_choice_models/models/nested_logit.py -> build/bdist.linux-x86_64/egg/python_choice_models/models\n",
            "copying build/lib/python_choice_models/models/generalized_stochastic_preference.py -> build/bdist.linux-x86_64/egg/python_choice_models/models\n",
            "copying build/lib/python_choice_models/profiler.py -> build/bdist.linux-x86_64/egg/python_choice_models\n",
            "byte-compiling build/bdist.linux-x86_64/egg/python_choice_models/settings.py to settings.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/python_choice_models/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/python_choice_models/estimation/maximum_likelihood/ranked_list.py to ranked_list.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/python_choice_models/estimation/maximum_likelihood/random_choice.py to random_choice.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/python_choice_models/estimation/maximum_likelihood/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/python_choice_models/estimation/maximum_likelihood/latent_class.py to latent_class.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/python_choice_models/estimation/ranked_list.py to ranked_list.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/python_choice_models/estimation/expectation_maximization/ranked_list.py to ranked_list.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/python_choice_models/estimation/expectation_maximization/markov_chain.py to markov_chain.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/python_choice_models/estimation/expectation_maximization/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/python_choice_models/estimation/expectation_maximization/latent_class.py to latent_class.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/python_choice_models/estimation/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/python_choice_models/estimation/market_explore/ranked_list.py to ranked_list.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/python_choice_models/estimation/market_explore/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/python_choice_models/optimization/non_linear.py to non_linear.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/python_choice_models/optimization/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/python_choice_models/optimization/linear.py to linear.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/python_choice_models/utils.py to utils.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/python_choice_models/transactions/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/python_choice_models/transactions/base.py to base.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/python_choice_models/models/multinomial_logit.py to multinomial_logit.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/python_choice_models/models/exponomial.py to exponomial.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/python_choice_models/models/ranked_list.py to ranked_list.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/python_choice_models/models/markov_chain.py to markov_chain.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/python_choice_models/models/random_choice.py to random_choice.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/python_choice_models/models/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/python_choice_models/models/latent_class.py to latent_class.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/python_choice_models/models/mixed_logit.py to mixed_logit.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/python_choice_models/models/markov_chain_rank_2.py to markov_chain_rank_2.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/python_choice_models/models/nested_logit.py to nested_logit.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/python_choice_models/models/generalized_stochastic_preference.py to generalized_stochastic_preference.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/python_choice_models/profiler.py to profiler.cpython-310.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying python_choice_models.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying python_choice_models.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying python_choice_models.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying python_choice_models.egg-info/not-zip-safe -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying python_choice_models.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying python_choice_models.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "creating dist\n",
            "creating 'dist/python_choice_models-0.0.1-py3.10.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing python_choice_models-0.0.1-py3.10.egg\n",
            "creating /usr/local/lib/python3.10/dist-packages/python_choice_models-0.0.1-py3.10.egg\n",
            "Extracting python_choice_models-0.0.1-py3.10.egg to /usr/local/lib/python3.10/dist-packages\n",
            "Adding python-choice-models 0.0.1 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.10/dist-packages/python_choice_models-0.0.1-py3.10.egg\n",
            "Processing dependencies for python-choice-models==0.0.1\n",
            "Searching for matplotlib==3.7.1\n",
            "Best match: matplotlib 3.7.1\n",
            "Adding matplotlib 3.7.1 to easy-install.pth file\n",
            "detected new path './python_choice_models-0.0.1-py3.10.egg'\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for scipy==1.13.1\n",
            "Best match: scipy 1.13.1\n",
            "Adding scipy 1.13.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for numba==0.60.0\n",
            "Best match: numba 0.60.0\n",
            "Adding numba 0.60.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for numpy==1.26.4\n",
            "Best match: numpy 1.26.4\n",
            "Adding numpy 1.26.4 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for python-dateutil==2.8.2\n",
            "Best match: python-dateutil 2.8.2\n",
            "Adding python-dateutil 2.8.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for pyparsing==3.1.4\n",
            "Best match: pyparsing 3.1.4\n",
            "Adding pyparsing 3.1.4 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for pillow==10.4.0\n",
            "Best match: pillow 10.4.0\n",
            "Adding pillow 10.4.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for packaging==24.1\n",
            "Best match: packaging 24.1\n",
            "Adding packaging 24.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages/setuptools/_vendor\n",
            "Searching for kiwisolver==1.4.7\n",
            "Best match: kiwisolver 1.4.7\n",
            "Adding kiwisolver 1.4.7 to easy-install.pth file\n",
            "detected new path './setuptools/_vendor'\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for fonttools==4.53.1\n",
            "Best match: fonttools 4.53.1\n",
            "Adding fonttools 4.53.1 to easy-install.pth file\n",
            "Installing fonttools script to /usr/local/bin\n",
            "Installing pyftmerge script to /usr/local/bin\n",
            "Installing pyftsubset script to /usr/local/bin\n",
            "Installing ttx script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for cycler==0.12.1\n",
            "Best match: cycler 0.12.1\n",
            "Adding cycler 0.12.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for contourpy==1.3.0\n",
            "Best match: contourpy 1.3.0\n",
            "Adding contourpy 1.3.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for llvmlite==0.43.0\n",
            "Best match: llvmlite 0.43.0\n",
            "Adding llvmlite 0.43.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for six==1.16.0\n",
            "Best match: six 1.16.0\n",
            "Adding six 1.16.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Finished processing dependencies for python-choice-models==0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -o data_sets/hotel/hotel.zip -d data_sets/hotel/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exRozjne67fQ",
        "outputId": "c4ecc494-937e-4edc-b47e-816228043d78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  data_sets/hotel/hotel.zip\n",
            "   creating: data_sets/hotel/hotel/\n",
            "  inflating: data_sets/hotel/hotel/instance_1.json  \n",
            "  inflating: data_sets/hotel/hotel/instance_2.json  \n",
            "  inflating: data_sets/hotel/hotel/instance_3.json  \n",
            "  inflating: data_sets/hotel/hotel/instance_4.json  \n",
            "  inflating: data_sets/hotel/hotel/instance_5.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def read_hotel_instance(file_name):\n",
        "    with open(file_name, 'r') as f:\n",
        "        data = json.loads(f.read())\n",
        "    #ground_truth = Model.from_data(data['ground_model'])\n",
        "    product_labels = data['product_labels']\n",
        "    transactions = Transaction.from_json(data['transactions']['in_sample_transactions'])\n",
        "    out_of_sample_transactions = Transaction.from_json(data['transactions']['out_of_sample_transactions'])\n",
        "    return product_labels, transactions, out_of_sample_transactions\n",
        "\n",
        "\n",
        "def read_sushi_instance(file_name):\n",
        "  with open(file_name, 'r') as f:\n",
        "        data = json.loads(f.read())\n",
        "  ground_truth = Model.from_data(data['ground_model'])\n",
        "  in_sample_transactions = Transaction.from_json(data['transactions']['in_sample_transactions'])\n",
        "  out_of_sample_transactions = Transaction.from_json(data['transactions']['out_of_sample_transactions'])\n",
        "  return ground_truth, in_sample_transactions, out_of_sample_transactions\n",
        "\n",
        "\n",
        "def read_synthetic_instance(file_name):\n",
        "    with open(file_name, 'r') as f:\n",
        "        data = json.loads(f.read())\n",
        "    ground_truth = Model.from_data(data['ground_model'])\n",
        "    transactions = Transaction.from_json(data['transactions']['in_sample_transactions'])\n",
        "    return ground_truth, transactions"
      ],
      "metadata": {
        "id": "2O7E6lO45squ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# utils\n",
        "def choice2target(choice,offer_set,padding=True, padding_size=6):\n",
        "  target = np.equal(offer_set, choice).astype(int)\n",
        "  if not padding:\n",
        "    return target\n",
        "  else:\n",
        "    padded_target = np.zeros(padding_size)\n",
        "    padded_target[:len(offer_set)] = target\n",
        "    return padded_target\n",
        "\n",
        "def offer_set_to_one_hot_features_with_padding(offered_products, total_products,\n",
        "                                               padding=True, padding_size=6):\n",
        "  # padding size refers to the input size (0) of each transaction\n",
        "\n",
        "  offer_set_size = len(offered_products)\n",
        "  if not padding:\n",
        "    features = np.zeros((offer_set_size, len(total_products)))\n",
        "  else:\n",
        "    features = np.zeros((padding_size, len(total_products)))\n",
        "    # set padding features to be (-10e6, 0, ..., 0)\n",
        "    features[offer_set_size:padding_size, 0] = -1e6\n",
        "  features[np.arange(len(offered_products)), offered_products] = 1\n",
        "\n",
        "  # set feature for product 0 to be zero\n",
        "  if (0 in offered_products):\n",
        "    features[0][0]=0\n",
        "\n",
        "  return features\n",
        "\n",
        "# apply softmax to a vector\n",
        "def softmax(u):\n",
        "  return np.exp(u)/np.sum(np.exp(u))\n",
        "\n",
        "def prob2target(x):\n",
        "  # probabilistic choice\n",
        "  choice = np.random.choice(np.arange(len(x)), size=1, p=x)\n",
        "  target = np.zeros(len(x))\n",
        "  target[choice]=1\n",
        "  return target\n"
      ],
      "metadata": {
        "id": "pn2Qt8jC-ZSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls data_sets/hotel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmdHSwG3CbNy",
        "outputId": "2d518e0d-b927-466a-bb7e-0ee4b633ca4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mhotel\u001b[0m/  hotel.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Generator for General Json data (Berbeglia)\n",
        "Preprocess Json data\n",
        "use one-hot feature\n",
        "only run this part for NN implementation"
      ],
      "metadata": {
        "id": "2GU5714G5lIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transaction2feature(products, transaction, padding=True, padding_size=6):\n",
        "  offered_set = transaction.offered_products\n",
        "  #print(offered_set)\n",
        "  features = offer_set_to_one_hot_features_with_padding(offered_set, products,\n",
        "                                                          padding=padding, padding_size=padding_size)\n",
        "  #features = torch.from_numpy(np.array([features])).float()\n",
        "  features = np.array([features])\n",
        "  return features\n",
        "\n",
        "class JSONChoiceHotelDataset(Dataset):\n",
        "  def __init__(self, input_file, padding=True, padding_size=6):\n",
        "    product_labels, transactions,out_of_sample_transactions = read_hotel_instance(input_file)\n",
        "\n",
        "\n",
        "    self.products = [0]+ sorted([int(i) for i in list(product_labels.keys())])\n",
        "    self.data = transactions\n",
        "    self.test_data = out_of_sample_transactions\n",
        "    self.padding = padding\n",
        "    self.padding_size = padding_size\n",
        "    self.feature_dim = len(self.products)\n",
        "    self.nchoices = len(transactions[0].offered_products)\n",
        "    if padding:\n",
        "      self.nchoices = padding_size\n",
        "\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    transaction = self.data[index]\n",
        "    offered_set = transaction.offered_products\n",
        "\n",
        "    features = transaction2feature(self.products, transaction, padding=self.padding, padding_size = self.padding_size)\n",
        "    features = torch.from_numpy(features).float()[0] # make features to be tensor\n",
        "    choices = torch.from_numpy(choice2target(transaction.product,offered_set,\n",
        "                                                          padding=self.padding, padding_size=self.padding_size))\n",
        "\n",
        "    return features, choices\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n"
      ],
      "metadata": {
        "id": "IsAD_D758Ozs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model\n",
        "This part outputs the model using the genereated data above as training data"
      ],
      "metadata": {
        "id": "-gjyEF1fSTXG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### NN"
      ],
      "metadata": {
        "id": "3T8n5Pq50Xiq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NN Model\n",
        "# this model has not adapted to\n",
        "# - different offer set size at time t\n",
        "# - different feature lengths\n",
        "\n",
        "class MixedLogitChoice(nn.Module):\n",
        "    def __init__(self, d, nchoices, K, **kwargs):\n",
        "        super(MixedLogitChoice, self).__init__()\n",
        "\n",
        "        self.d = d # number of features\n",
        "        self.n = nchoices # number of choices\n",
        "        self.K = K # number of mixtures\n",
        "\n",
        "        # initialize parameters\n",
        "        self.lin = nn.Linear(in_features=self.d, out_features=K, bias=False)\n",
        "        #self.lin.weight.data.uniform_(0.0, 2.0)\n",
        "\n",
        "        self.act = nn.LogSoftmax(dim=1)\n",
        "        #self.p = nn.Parameter(torch.ones(K)/K)\n",
        "\n",
        "\n",
        "    def forward(self,x,**kwargs):\n",
        "        #x = x.reshape((x.size()[0],self.n ,self.d))\n",
        "        #p = F.softmax(self.p,dim=0)\n",
        "\n",
        "        output = self.lin(x)\n",
        "\n",
        "        # apply softmax\n",
        "        output = self.act(output)\n",
        "        #print(output.shape)\n",
        "        output = torch.sum(torch.exp(output), dim=2)/self.K\n",
        "        #print(output.shape)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "0_KdlRy6Sq-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def torch_log_ignore_zero(x):\n",
        "  # x is a tensor\n",
        "  #output = torch.zeros(size = x.size())\n",
        "  idx = x>0\n",
        "  x[idx] = torch.log(x[idx])\n",
        "  return x\n",
        "\n",
        "def NLL_loss(target, out):\n",
        "  return torch.mul(torch.mul(target, torch_log_ignore_zero(out)).sum(),-1)"
      ],
      "metadata": {
        "id": "Qd92VsOwVO1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train one instance\n",
        "After running this part to make sure dataloader and neural network function normally, go to **Performance matrix** for evaluations.\n"
      ],
      "metadata": {
        "id": "RH5kllFT-hmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating data indices for training and validation splits:\n",
        "def train_test_split(dataset, batch_size=50, validation_split=.2, shuffle_dataset=True, random_seed=42):\n",
        "\n",
        "  dataset_size = len(dataset)\n",
        "  indices = list(range(dataset_size))\n",
        "  split = int(np.floor(validation_split * dataset_size))\n",
        "  if shuffle_dataset :\n",
        "      np.random.seed(random_seed)\n",
        "      np.random.shuffle(indices)\n",
        "  train_indices, val_indices = indices[split:], indices[:split]\n",
        "\n",
        "  # Creating data samplers and loaders:\n",
        "  train_sampler = SubsetRandomSampler(train_indices)\n",
        "  valid_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
        "                                            sampler=train_sampler)\n",
        "  validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
        "                                                  sampler=valid_sampler)\n",
        "  return train_loader, validation_loader\n"
      ],
      "metadata": {
        "id": "uxGCXXafkrKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NNChoiceModel(object):\n",
        "\n",
        "  def __init__(self, d, nchoices,nmixture,dataset,\n",
        "               device=torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")):\n",
        "\n",
        "    #initialize NN model\n",
        "    self.model = MixedLogitChoice(d, nchoices,nmixtures).to(device)\n",
        "    self.d = d\n",
        "    self.nchoices = nchoices\n",
        "    self.nmixtures = nmixtures\n",
        "\n",
        "    # best model path\n",
        "    self.saved_model_path_by_val = None\n",
        "    self.saved_model_path_by_train = None\n",
        "\n",
        "    # record training losses\n",
        "    self.train_losses = []\n",
        "    self.train_accurs = []\n",
        "    self.test_losses = []\n",
        "    self.test_accurs = []\n",
        "    self.train_time = 0\n",
        "\n",
        "    # record dataset\n",
        "    self.dataset = dataset\n",
        "\n",
        "    # record server\n",
        "    self.device = device\n",
        "    print('device', device)\n",
        "\n",
        "\n",
        "\n",
        "  def train(self, model_name, batch_size, train_loader, validation_loader,\n",
        "            num_epochs=1000, lr=1e-3, weight_decay = 0,\n",
        "            loss_type = \"NLL\",padding=False, padding_size=6,\n",
        "            upper_bound=np.log(1e6), lower_bound=-np.log(1e6),\n",
        "            eval=False, opt='adam', noise=False,\n",
        "            verbose=True,\n",
        "            learning_rate_decay_step = 100000, gamma=1, stopping_train_loss = 1e-10, use_val=False):\n",
        "      # training setup\n",
        "    gmodel = self.model\n",
        "    device = self.device\n",
        "    if opt=='adam' or 'noisy-adam':\n",
        "      optimizer = torch.optim.Adam(gmodel.parameters(),lr=lr, weight_decay=weight_decay)\n",
        "    elif opt=='sgd':\n",
        "      optimizer = torch.optim.SGD(gmodel.parameters(),lr=lr, weight_decay=weight_decay)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=gamma,last_epoch=-1)\n",
        "    dataset_size = len(self.dataset.data)\n",
        "\n",
        "    stop_train = False\n",
        "\n",
        "    best_model_param_by_train = None\n",
        "    best_model_param_by_val = None\n",
        "\n",
        "    # Train\n",
        "    best_vloss = 100_00_000\n",
        "    best_tloss = 100_00_000\n",
        "    # best_epoch = None\n",
        "    default_start = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "      # Train:\n",
        "      gmodel.train()\n",
        "      running_tloss = 0\n",
        "      # running_taccur = 0\n",
        "      for batch_index, (features, choices) in enumerate(train_loader):\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        input = features\n",
        "        out = gmodel(input)\n",
        "        target = choices\n",
        "\n",
        "        if loss_type == 'NLL':\n",
        "          tloss = NLL_loss(target, out)/ choices.size()[0]\n",
        "        elif loss_type == 'SQ':\n",
        "          tloss = torch.square(out - target).sum()/ choices.size()[0]\n",
        "\n",
        "        if epoch==0 and batch_index ==0 and len(self.train_losses)==0:\n",
        "          print('initial loss', round(tloss.item(),2))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        tloss.backward()\n",
        "        optimizer.step()\n",
        "        end_step_time = time.time()\n",
        "        #print('step time', end_step_time-start_time)\n",
        "        if noise:\n",
        "          lr_ = lr_scheduler.get_last_lr()[0]\n",
        "          beta = 2*lr_ / ((1*lr_)**2)\n",
        "\n",
        "          for p in gmodel.parameters():\n",
        "              unit_noise = torch.autograd.Variable(p.data.new(p.size()).normal_())\n",
        "              p.data.add_(unit_noise, alpha=(2*lr_/beta)**0.5)\n",
        "\n",
        "        with torch.no_grad():\n",
        "          for param in gmodel.parameters():\n",
        "              param.clamp_(lower_bound, upper_bound)\n",
        "\n",
        "              if padding:\n",
        "                param.data[:,0]=1\n",
        "        #print('padding time', time.time()-end_step_time)\n",
        "        if not (epoch*dataset_size + batch_index*batch_size) % learning_rate_decay_step:\n",
        "          lr_scheduler.step()\n",
        "          #print(lr_scheduler.get_last_lr())\n",
        "\n",
        "        # record training time\n",
        "        self.train_time +=  time.time() - start_time\n",
        "\n",
        "        # probs = out.detach().numpy()\n",
        "        # predict = np.apply_along_axis(prob2target, 1, probs)\n",
        "        # train_accuracy = np.average((target.argmax(dim=1).detach().numpy() == predict.argmax(axis=1))*1)\n",
        "\n",
        "\n",
        "        running_tloss += tloss\n",
        "        # running_taccur += train_accuracy\n",
        "        #train_arg_accur = np.average((gt_probs.argmax(axis=1)== probs.argmax(axis=1))*1)\n",
        "\n",
        "\n",
        "\n",
        "      avg_tloss = running_tloss / (batch_index+1)\n",
        "      # avg_taccur = running_taccur / (batch_index+1)\n",
        "\n",
        "\n",
        "      # Test: evaluate after each epoch\n",
        "      avg_vloss = torch.Tensor(1)*0\n",
        "      if eval:\n",
        "        gmodel.eval()\n",
        "        with torch.no_grad():\n",
        "          running_vloss = torch.Tensor(1)*0\n",
        "          # running_vaccur = torch.Tensor(1)*0\n",
        "          # test_accuracy = torch.Tensor(1)*0\n",
        "\n",
        "          for batch_index, (features, choices) in enumerate(validation_loader):\n",
        "            # set to train mode\n",
        "            input = features.to(device)\n",
        "            out = gmodel(input)\n",
        "            target = choices.to(device)\n",
        "\n",
        "            if loss_type == 'NLL':\n",
        "              vloss = NLL_loss(target, out)/ choices.size()[0]\n",
        "            elif loss_type == 'SQ':\n",
        "              vloss = torch.square(out - target).sum()/choices.size()[0]\n",
        "            # probs = out.detach().numpy()\n",
        "\n",
        "            # predict = np.apply_along_axis(prob2target, 1, probs)\n",
        "            # test_accuracy = np.average((target.argmax(dim=1).detach().numpy() == predict.argmax(axis=1))*1)\n",
        "            # test_arg_accur = np.average((gt_probs.argmax(axis=1)== probs.argmax(axis=1))*1)\n",
        "\n",
        "            running_vloss += vloss\n",
        "            # running_vaccur += test_accuracy\n",
        "          avg_vloss = running_vloss/(batch_index+1)\n",
        "        # avg_vaccur = running_vaccur/(batch_index+1)\n",
        "\n",
        "        self.train_losses.append(avg_tloss.item())\n",
        "        # self.train_accurs.append(train_accuracy.item())\n",
        "        self.test_losses.append(avg_vloss.item())\n",
        "        # self.test_accurs.append(test_accuracy.item())\n",
        "\n",
        "      if (not (epoch+1) % 100) and verbose:\n",
        "        print('time', round(self.train_time,1),\n",
        "              'time extra', round(time.time()-default_start, 1),\n",
        "              'epoch', epoch+1,\n",
        "              'train loss', round(avg_tloss.item(),5),\n",
        "              # 'train accur', round(avg_taccur.item(),5),\n",
        "              'test loss', round(avg_vloss.item(),5),\n",
        "              # 'test accur', round(avg_vaccur.item(),5),\n",
        "                      )\n",
        "        # if avg_vloss <= best_vloss and use_val:\n",
        "        #   best_vloss = avg_vloss\n",
        "        #   model_path = 'val_model_{}'.format(model_name)\n",
        "        #   #torch.save(gmodel.state_dict(), model_path)\n",
        "        #   best_model_param_by_val = gmodel.state_dict()\n",
        "        #   self.saved_model_path_by_val=model_path\n",
        "        #stop_train = (abs(best_tloss-avg_tloss)<stopping_train_loss*len(self.dataset))\n",
        "        # if (avg_tloss <= best_tloss) and abs(best_tloss-avg_tloss)>=stopping_train_loss*len(self.dataset):\n",
        "        #   best_tloss = avg_tloss\n",
        "        #   model_path = 'train_model_{}'.format(model_name)\n",
        "        #   #torch.save(gmodel.state_dict(), model_path)\n",
        "        #   best_model_param_by_train = gmodel.state_dict()\n",
        "        #   self.saved_model_path_by_train=model_path\n",
        "\n",
        "        # elif epoch>2:\n",
        "        #   stop_train=True\n",
        "      # if stop_train:\n",
        "      #   break\n",
        "    # if self.saved_model_path_by_val is not None:\n",
        "    #   torch.save(best_model_param_by_val, self.saved_model_path_by_val)\n",
        "    # if self.saved_model_path_by_train is not None:\n",
        "    #   torch.save(best_model_param_by_train, self.saved_model_path_by_train)\n",
        "    # print('total train time:', round(self.train_time,1))\n",
        "    # print('best train loss', best_tloss.item())\n",
        "\n",
        "#   def getBestModelByVal(self):\n",
        "#     saved_model = MixedLogitChoice(d=self.d, nchoices=self.nchoices,K=self.nmixtures)\n",
        "#     saved_model.load_state_dict(torch.load(self.saved_model_path_by_val))\n",
        "#     return saved_model\n",
        "\n",
        "  # def getBestModelByTrain(self):\n",
        "  #   saved_model = MixedLogitChoice(d=self.d, nchoices=self.nchoices,K=self.nmixtures).to(self.device)\n",
        "  #   saved_model.load_state_dict(torch.load(self.saved_model_path_by_train))\n",
        "\n",
        "# # model = saved_model\n",
        "# # pred_weights = model.lin.weight.detach().numpy()\n",
        "#     return saved_model\n",
        "\n",
        "  # def loadBestModel(self):\n",
        "  #   self.model = self.getBestModel()\n",
        "\n",
        "  def getPredictedWeights(self):\n",
        "    return self.model.lin.weight.detach().numpy()\n",
        "\n",
        "  # def cdf_of(self,x):\n",
        "  #   xk = self.getPredictedWeights()\n",
        "  #   pk = np.ones(self.nmixtures)/self.nmixtures\n",
        "  #   return cdf_discrete_rv_vec(x, xk, pk)\n",
        "\n",
        "  def plotLoss(self, MA=100, se=1):\n",
        "    epochs = range(se,len(self.train_losses)+1-MA+1)\n",
        "    plt.plot(epochs, moving_average(self.train_losses,n=MA)[se-1:], 'grey', label='Training loss')\n",
        "    plt.plot(epochs, moving_average(self.test_losses,n=MA)[se-1:], 'b', label='validation loss')\n",
        "    plt.plot(epochs, moving_average(self.train_accurs,n=MA)[se-1:], 'grey',label='Training accur', linestyle='--')\n",
        "    plt.plot(epochs, moving_average(self.test_accurs,n=MA)[se-1:], 'b',label='validation accur', linestyle='--')\n",
        "    plt.title('Training and Validation loss/accur of {} with assume K={}'.format(self.gtype, self.nmixtures))\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss/Accur')\n",
        "    #plt.ylim(bottom=0,top=2)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def moving_average(a, n=10):\n",
        "    b = np.array(a)\n",
        "    ret = np.cumsum(b, dtype=float)\n",
        "    ret[n:] = ret[n:] - ret[:-n]\n",
        "    return ret[n - 1:] / n"
      ],
      "metadata": {
        "id": "LPkp7t3Wfn3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance Matrix"
      ],
      "metadata": {
        "id": "WCy4iK2Y57xF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RMSE - for Json dataset\n",
        "$$\\operatorname{RMSE}_{soft} = \\sqrt{ \\frac{1}{T} \\sum_{t=1}^T \\sum_{j\\in S_t} \\left(\\Pr(j\\mid S_t,\\beta_0) - \\Pr(j\\mid S_t,\\hat\\beta)\\right)^2}$$\n",
        "\n",
        "$$\\operatorname{RMSE}_{hard} = \\sqrt{ \\frac{1}{T} \\sum_{t=1}^T \\sum_{j\\in S_t} \\left(\\mathbf{1}(N_{jt}==1) - \\Pr(j\\mid S_t,\\hat\\beta)\\right)^2}$$\n",
        "\n",
        "For synthetic, as we know the ground truth, use the soft RMSE.\n",
        "\n",
        "To be consistent with the `estimate.py` in the package `python-choice-models`, we show the following version of soft RMSE\n",
        "$$\\operatorname{RMSE}_{soft} = \\sqrt{ \\frac{1}{T} \\sum_{t=1}^T  \\left(Pr(j_t\\mid S_t, \\beta_0) - \\Pr(j\\mid S_t,\\hat\\beta)\\right)^2}$$\n",
        "where $j_t$ is the choice made at time $t$ with offered set $S_t$.\n",
        "\n"
      ],
      "metadata": {
        "id": "t_pRs-n-fQNB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preliminary test"
      ],
      "metadata": {
        "id": "JyUyv0b_VAAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def prob_of_NN(model, prod, t,products,padding=True, padding_size=10,\n",
        "               device=torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")):\n",
        "  features = transaction2feature(products, t, padding=True, padding_size=padding_size)\n",
        "  #print(features)\n",
        "  features = torch.from_numpy(features).float().to(device)\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    out = model(features)\n",
        "    probs = out.cpu().detach().numpy()\n",
        "  #print(probs.shape)\n",
        "  #print(probs)\n",
        "  index = t.offered_products.index(prod)\n",
        "  return probs[0][index]\n",
        "\n",
        "from python_choice_models.transactions.base import Transaction\n",
        "def soft_rmse_known_model(model, dataset, ground_truth_model,padding=True, padding_size=10):\n",
        "  rmse = 0.0\n",
        "  amount_terms = 0.0\n",
        "  products = ground_truth_model.products\n",
        "  for t in dataset.data:\n",
        "    for prod in t.offered_products:\n",
        "      #print(prob_of_NN(t,products,padding=True, padding_size=6), ground_truth_model.probability_of(t))\n",
        "      prob1 = prob_of_NN(model, prod, t,products,padding=padding, padding_size=padding_size)\n",
        "      prob2 = ground_truth_model.probability_of(Transaction(prod, t.offered_products))\n",
        "      #print(prob1, prob2)\n",
        "      rmse += (( prob1 - prob2) ** 2)\n",
        "      amount_terms += 1\n",
        "  return np.sqrt(rmse / float(amount_terms))\n",
        "\n",
        "\n",
        "def soft_rmse(model, dataset, ground_truth_model,padding=True, padding_size=6):\n",
        "  rmse = 0.0\n",
        "  amount_terms = 0.0\n",
        "  products = ground_truth_model.products\n",
        "  for t in dataset.data:\n",
        "      #print(prob_of_NN(t,products,padding=True, padding_size=6), ground_truth_model.probability_of(t))\n",
        "    prob1 = prob_of_NN(model, t.product, t,products,padding=padding, padding_size=padding_size)\n",
        "    prob2 = ground_truth_model.probability_of(t)\n",
        "    rmse += (( prob1 - prob2) ** 2)\n",
        "    amount_terms += 1\n",
        "  return np.sqrt(rmse / float(amount_terms))\n",
        "\n",
        "\n",
        "def hard_rmse(model, out_of_sample_transactions, products, padding=True, padding_size=6):\n",
        "  rmse = 0.0\n",
        "  amount_terms = 0.0\n",
        "  for t in out_of_sample_transactions:\n",
        "    for prod in t.offered_products:\n",
        "      prob1 = prob_of_NN(model, prod, t,products,padding=padding, padding_size=padding_size)\n",
        "      prob2 = int(prod==t.product)\n",
        "      rmse += (( prob1 - prob2) ** 2)\n",
        "      amount_terms += 1\n",
        "  return np.sqrt(rmse / float(amount_terms))\n",
        "\n",
        "def hard_nll(model, out_of_sample_transactions, products, padding=True, padding_size=6):\n",
        "  nll = 0.0\n",
        "  amount_terms = 0.0\n",
        "  for t in out_of_sample_transactions:\n",
        "    for prod in t.offered_products:\n",
        "      prob1 = prob_of_NN(model, prod, t,products,padding=padding, padding_size=padding_size)\n",
        "      prob2 = int(prod==t.product)\n",
        "      if prob2 == 1:\n",
        "        nll += np.log(prob1)\n",
        "        amount_terms += 1\n",
        "  return nll / float(amount_terms)\n",
        "\n"
      ],
      "metadata": {
        "id": "b68aLFQrfhtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class TensorDataset(Dataset):\n",
        "    def __init__(self, feature_tensor, target_tensor):\n",
        "        self.feature_tensor = feature_tensor\n",
        "        self.target_tensor = target_tensor\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.feature_tensor[index], self.target_tensor[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.feature_tensor.size(0)\n",
        "\n"
      ],
      "metadata": {
        "id": "RP7RMDyHTcn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "10UvL0BK7kfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = [11, 11, 9, 5, 7]\n",
        "for instance_index in range(1,6):\n",
        "  torch.manual_seed(0)\n",
        "  print('---------- Hotel {} ------------'.format(instance_index))\n",
        "  json_file_name='data_sets/hotel/hotel/instance_{}.json'.format(instance_index)\n",
        "  max_padding_size = ds[instance_index-1]\n",
        "  dataset = JSONChoiceHotelDataset(json_file_name, padding_size=max_padding_size)\n",
        "  dataloader = torch.utils.data.DataLoader(dataset, batch_size=len(dataset))\n",
        "  features, labels = next(iter(dataloader))\n",
        "  device=torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
        "  features = features.to(device)\n",
        "  labels = labels.to(device)\n",
        "  tensor_dataset = TensorDataset(features, labels)\n",
        "  T = len(dataset)\n",
        "  validation_size = 0.0\n",
        "  batch_size = 8\n",
        "  print('batch size:', batch_size)\n",
        "  train_loader, test_loader = train_test_split(tensor_dataset, batch_size=batch_size, validation_split=validation_size, shuffle_dataset=True, random_seed=42)\n",
        "\n",
        "  d=dataset.feature_dim\n",
        "  nmixtures = 10000\n",
        "  print('number of hidden nodes:', nmixtures)\n",
        "  nn_model = NNChoiceModel(d, dataset.nchoices, nmixtures, dataset)\n",
        "  for i in range(20):\n",
        "    nn_model.train(model_name='hotel_{}_instance'.format(instance_index),\n",
        "                   loss_type='SQ',\n",
        "                  batch_size = batch_size, lr=5e-3, opt='adam', #noise=True, weight_decay=1e-9,\n",
        "                  train_loader=train_loader, upper_bound=8, lower_bound=-8,\n",
        "                  learning_rate_decay_step=500000,\n",
        "                  validation_loader = test_loader,\n",
        "                  num_epochs=200, gamma=1, padding=True, padding_size=max_padding_size,use_val=False)\n",
        "\n",
        "    saved_model = nn_model.model\n",
        "\n",
        "    #print('soft rmse', soft_rmse(saved_model, test_dataset, dataset.ground_truth, padding_size=max_padding_size))  # sensitive to suitable batch size & learning rate\n",
        "    rmse = hard_rmse(saved_model, dataset.test_data, dataset.products, padding=True, padding_size=max_padding_size)\n",
        "    in_rmse = hard_rmse(saved_model, dataset.data, dataset.products, padding=True, padding_size = max_padding_size)\n",
        "    saved_model.eval()\n",
        "    with torch.no_grad():\n",
        "      out = saved_model(features)\n",
        "      in_loss = NLL_loss(labels, out).item() / T\n",
        "    loss = hard_nll(saved_model, dataset.test_data, dataset.products, padding=True, padding_size = max_padding_size)\n",
        "\n",
        "\n",
        "    print('out-of-sample rmse', rmse, 'in-sample rmse', in_rmse, 'out-of-sample nll', loss, 'in-sample rmse', in_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9LM8vOyY7G3",
        "outputId": "9121a458-bfde-4fd1-f85e-093f0b6573aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- Hotel 1 ------------\n",
            "batch size: 8\n",
            "number of hidden nodes: 10000\n",
            "device cuda:0\n",
            "initial loss 0.91\n",
            "time 84.0 time extra 102.2 epoch 100 train loss 0.34952 test loss 0.0\n",
            "time 166.2 time extra 203.0 epoch 200 train loss 0.34892 test loss 0.0\n",
            "out-of-sample rmse 0.22182799816534898 in-sample rmse 0.21529225101034863 out-of-sample nll -0.7903745007514954 in-sample rmse 0.8018766097963871\n",
            "initial loss 0.44\n",
            "time 248.0 time extra 100.4 epoch 100 train loss 0.34801 test loss 0.0\n",
            "time 329.2 time extra 200.0 epoch 200 train loss 0.34883 test loss 0.0\n",
            "out-of-sample rmse 0.22180609553633893 in-sample rmse 0.21526564693327485 out-of-sample nll -0.7904871785865639 in-sample rmse 0.8013043631790716\n",
            "initial loss 0.59\n",
            "time 411.1 time extra 100.3 epoch 100 train loss 0.34887 test loss 0.0\n",
            "time 492.9 time extra 200.6 epoch 200 train loss 0.34878 test loss 0.0\n",
            "out-of-sample rmse 0.2217987058893189 in-sample rmse 0.2152607154531956 out-of-sample nll -0.7903729420338037 in-sample rmse 0.8009776789772488\n",
            "initial loss 0.05\n",
            "time 576.0 time extra 102.0 epoch 100 train loss 0.3487 test loss 0.0\n",
            "time 660.4 time extra 206.3 epoch 200 train loss 0.34796 test loss -0.0\n",
            "out-of-sample rmse 0.22180028723862183 in-sample rmse 0.2152585411136687 out-of-sample nll -0.7904379358831441 in-sample rmse 0.8009490133120567\n",
            "initial loss 0.41\n",
            "time 743.6 time extra 102.5 epoch 100 train loss 0.34959 test loss 0.0\n",
            "time 826.6 time extra 204.7 epoch 200 train loss 0.34795 test loss 0.0\n",
            "out-of-sample rmse 0.2218022207619526 in-sample rmse 0.2152581017338009 out-of-sample nll -0.7905641895869993 in-sample rmse 0.8010345466871568\n",
            "initial loss 0.24\n",
            "time 910.7 time extra 103.6 epoch 100 train loss 0.34881 test loss -0.0\n",
            "time 995.0 time extra 207.5 epoch 200 train loss 0.34797 test loss 0.0\n",
            "out-of-sample rmse 0.22179701230003768 in-sample rmse 0.21525712981841716 out-of-sample nll -0.7902791996721952 in-sample rmse 0.8009015950566204\n",
            "initial loss 0.61\n",
            "time 1079.3 time extra 103.8 epoch 100 train loss 0.34796 test loss -0.0\n",
            "time 1163.3 time extra 207.5 epoch 200 train loss 0.34796 test loss -0.0\n",
            "out-of-sample rmse 0.22179573555411702 in-sample rmse 0.21525766307909158 out-of-sample nll -0.7903644374631486 in-sample rmse 0.8009496423157041\n",
            "initial loss 0.23\n",
            "time 1247.4 time extra 103.6 epoch 100 train loss 0.34796 test loss -0.0\n",
            "time 1331.8 time extra 207.5 epoch 200 train loss 0.34796 test loss 0.0\n",
            "out-of-sample rmse 0.22180139604109755 in-sample rmse 0.21525706952096102 out-of-sample nll -0.7905230585584101 in-sample rmse 0.8009841400982076\n",
            "initial loss 0.44\n",
            "time 1416.4 time extra 104.3 epoch 100 train loss 0.34968 test loss 0.0\n",
            "time 1500.5 time extra 207.9 epoch 200 train loss 0.34796 test loss 0.0\n",
            "out-of-sample rmse 0.22179776207343282 in-sample rmse 0.2152570552427112 out-of-sample nll -0.7902942935025917 in-sample rmse 0.8009919415687569\n",
            "initial loss 0.23\n",
            "time 1584.9 time extra 104.0 epoch 100 train loss 0.34881 test loss 0.0\n",
            "time 1669.3 time extra 208.0 epoch 200 train loss 0.34795 test loss 0.0\n",
            "out-of-sample rmse 0.22179997224240525 in-sample rmse 0.21525624786005612 out-of-sample nll -0.7904525607037094 in-sample rmse 0.8009028285347446\n",
            "initial loss 0.05\n",
            "time 1751.5 time extra 101.2 epoch 100 train loss 0.34795 test loss 0.0\n",
            "time 1834.0 time extra 203.0 epoch 200 train loss 0.34885 test loss 0.0\n",
            "out-of-sample rmse 0.22179139254096375 in-sample rmse 0.2152570182903404 out-of-sample nll -0.7901698025217596 in-sample rmse 0.8009837236769484\n",
            "initial loss 0.62\n",
            "time 1918.1 time extra 103.6 epoch 100 train loss 0.34796 test loss 0.0\n",
            "time 2002.7 time extra 207.7 epoch 200 train loss 0.34877 test loss 0.0\n",
            "out-of-sample rmse 0.22179687055793607 in-sample rmse 0.21525635288226144 out-of-sample nll -0.7903564713586052 in-sample rmse 0.800896216171225\n",
            "initial loss 0.05\n",
            "time 2087.3 time extra 104.2 epoch 100 train loss 0.34795 test loss -0.0\n",
            "time 2172.0 time extra 208.8 epoch 200 train loss 0.34795 test loss 0.0\n",
            "out-of-sample rmse 0.22179547305180897 in-sample rmse 0.21525629165107918 out-of-sample nll -0.7903487582476634 in-sample rmse 0.8008567925774983\n",
            "initial loss 0.44\n",
            "time 2255.7 time extra 102.9 epoch 100 train loss 0.34878 test loss 0.0\n",
            "time 2339.8 time extra 206.7 epoch 200 train loss 0.34884 test loss 0.0\n",
            "out-of-sample rmse 0.22180214605843188 in-sample rmse 0.21525627717151946 out-of-sample nll -0.7905817649499426 in-sample rmse 0.8010249087513056\n",
            "initial loss 0.23\n",
            "time 2424.0 time extra 103.7 epoch 100 train loss 0.34797 test loss 0.0\n",
            "time 2507.9 time extra 207.2 epoch 200 train loss 0.34796 test loss -0.0\n",
            "out-of-sample rmse 0.22179734771634516 in-sample rmse 0.21525549793484614 out-of-sample nll -0.7903570606123727 in-sample rmse 0.8008211094850853\n",
            "initial loss 0.05\n",
            "time 2592.6 time extra 104.5 epoch 100 train loss 0.34885 test loss 0.0\n",
            "time 2676.8 time extra 208.0 epoch 200 train loss 0.34796 test loss 0.0\n",
            "out-of-sample rmse 0.22178966722613075 in-sample rmse 0.21525660365014973 out-of-sample nll -0.7898742852570876 in-sample rmse 0.8009170335610106\n",
            "initial loss 0.63\n",
            "time 2760.6 time extra 103.2 epoch 100 train loss 0.3488 test loss 0.0\n",
            "time 2846.3 time extra 209.1 epoch 200 train loss 0.34795 test loss 0.0\n",
            "out-of-sample rmse 0.22179762068165784 in-sample rmse 0.21525545631208898 out-of-sample nll -0.79029005010173 in-sample rmse 0.8008539178646354\n",
            "initial loss 0.21\n",
            "time 2937.1 time extra 111.2 epoch 100 train loss 0.34884 test loss -0.0\n",
            "time 3021.0 time extra 214.8 epoch 200 train loss 0.34795 test loss 0.0\n",
            "out-of-sample rmse 0.22179592434753156 in-sample rmse 0.21525599289358963 out-of-sample nll -0.7903324503718682 in-sample rmse 0.8009282682590989\n",
            "initial loss 0.4\n",
            "time 3105.3 time extra 104.0 epoch 100 train loss 0.34795 test loss -0.0\n",
            "time 3189.3 time extra 207.4 epoch 200 train loss 0.34795 test loss 0.0\n",
            "out-of-sample rmse 0.22179679723206597 in-sample rmse 0.2152553851383725 out-of-sample nll -0.7903081585326285 in-sample rmse 0.8008632271636862\n",
            "initial loss 0.05\n",
            "time 3272.4 time extra 102.3 epoch 100 train loss 0.34876 test loss 0.0\n",
            "time 3357.6 time extra 207.5 epoch 200 train loss 0.34795 test loss 0.0\n",
            "out-of-sample rmse 0.22179506850623334 in-sample rmse 0.21525604966903816 out-of-sample nll -0.7902066912291185 in-sample rmse 0.80092204232297\n",
            "---------- Hotel 2 ------------\n",
            "batch size: 8\n",
            "number of hidden nodes: 10000\n",
            "device cuda:0\n",
            "initial loss 1.04\n",
            "time 28.7 time extra 35.2 epoch 100 train loss 0.34716 test loss 0.0\n",
            "time 57.6 time extra 71.0 epoch 200 train loss 0.34545 test loss -0.0\n",
            "out-of-sample rmse 0.20923827611681547 in-sample rmse 0.2082980423134844 out-of-sample nll -0.7733803488234038 in-sample rmse 0.7700888064334063\n",
            "initial loss 0.06\n",
            "time 87.6 time extra 36.8 epoch 100 train loss 0.3458 test loss -0.0\n",
            "time 116.1 time extra 71.9 epoch 200 train loss 0.34433 test loss 0.0\n",
            "out-of-sample rmse 0.20888558752914485 in-sample rmse 0.2078231624825631 out-of-sample nll -0.7612461496424932 in-sample rmse 0.7576666747651449\n",
            "initial loss 0.59\n",
            "time 145.2 time extra 35.7 epoch 100 train loss 0.34447 test loss -0.0\n",
            "time 173.7 time extra 70.7 epoch 200 train loss 0.34347 test loss -0.0\n",
            "out-of-sample rmse 0.20881069045053235 in-sample rmse 0.20772133492645845 out-of-sample nll -0.75769868358489 in-sample rmse 0.7534668268226996\n",
            "initial loss 0.42\n",
            "time 202.8 time extra 35.8 epoch 100 train loss 0.34384 test loss 0.0\n",
            "time 231.4 time extra 70.9 epoch 200 train loss 0.34387 test loss -0.0\n",
            "out-of-sample rmse 0.2087960840903424 in-sample rmse 0.20769497263838588 out-of-sample nll -0.7569494466627797 in-sample rmse 0.7524331475661052\n",
            "initial loss 0.05\n",
            "time 259.9 time extra 35.1 epoch 100 train loss 0.3443 test loss -0.0\n",
            "time 288.7 time extra 70.5 epoch 200 train loss 0.34432 test loss 0.0\n",
            "out-of-sample rmse 0.2087819791817444 in-sample rmse 0.2076819715727875 out-of-sample nll -0.756098275030813 in-sample rmse 0.7519015800662157\n",
            "initial loss 0.43\n",
            "time 317.3 time extra 35.1 epoch 100 train loss 0.34379 test loss -0.0\n",
            "time 346.5 time extra 71.2 epoch 200 train loss 0.34331 test loss -0.0\n",
            "out-of-sample rmse 0.2087758649285776 in-sample rmse 0.2076750812239799 out-of-sample nll -0.7556325185683466 in-sample rmse 0.7515610343395533\n",
            "initial loss 0.42\n",
            "time 375.6 time extra 35.9 epoch 100 train loss 0.34377 test loss 0.0\n",
            "time 404.4 time extra 71.3 epoch 200 train loss 0.34331 test loss 0.0\n",
            "out-of-sample rmse 0.20877817478928676 in-sample rmse 0.2076708854573667 out-of-sample nll -0.755503390168631 in-sample rmse 0.7513791789530415\n",
            "initial loss 0.45\n",
            "time 433.2 time extra 35.4 epoch 100 train loss 0.34376 test loss 0.0\n",
            "time 462.0 time extra 70.7 epoch 200 train loss 0.34329 test loss 0.0\n",
            "out-of-sample rmse 0.20877281700731376 in-sample rmse 0.20766838086217926 out-of-sample nll -0.7553479158750145 in-sample rmse 0.751341168085734\n",
            "initial loss 0.06\n",
            "time 490.6 time extra 35.2 epoch 100 train loss 0.34329 test loss -0.0\n",
            "time 519.3 time extra 70.4 epoch 200 train loss 0.34468 test loss 0.0\n",
            "out-of-sample rmse 0.20877068165910656 in-sample rmse 0.2076667385521855 out-of-sample nll -0.7552053545110969 in-sample rmse 0.7512792311063627\n",
            "initial loss 0.77\n",
            "time 547.7 time extra 34.8 epoch 100 train loss 0.34468 test loss 0.0\n",
            "time 576.4 time extra 70.4 epoch 200 train loss 0.34431 test loss 0.0\n",
            "out-of-sample rmse 0.208779694283407 in-sample rmse 0.20766499359754198 out-of-sample nll -0.7555499501125786 in-sample rmse 0.7511966399384061\n",
            "initial loss 0.41\n",
            "time 605.8 time extra 36.1 epoch 100 train loss 0.34378 test loss 0.0\n",
            "time 634.7 time extra 71.6 epoch 200 train loss 0.34328 test loss 0.0\n",
            "out-of-sample rmse 0.2087716907971862 in-sample rmse 0.2076640150402241 out-of-sample nll -0.7551012132757453 in-sample rmse 0.7511651684598225\n",
            "initial loss 0.42\n",
            "time 663.5 time extra 35.4 epoch 100 train loss 0.34468 test loss -0.0\n",
            "time 692.2 time extra 71.0 epoch 200 train loss 0.34371 test loss 0.0\n",
            "out-of-sample rmse 0.20877523761914682 in-sample rmse 0.2076635887468747 out-of-sample nll -0.7552013989417784 in-sample rmse 0.7511280357030026\n",
            "initial loss 0.4\n",
            "time 721.7 time extra 36.3 epoch 100 train loss 0.34378 test loss 0.0\n",
            "time 750.0 time extra 71.0 epoch 200 train loss 0.34327 test loss 0.0\n",
            "out-of-sample rmse 0.20877050504542857 in-sample rmse 0.2076626394442797 out-of-sample nll -0.7552440104946013 in-sample rmse 0.7511111137989737\n",
            "initial loss 0.25\n",
            "time 778.6 time extra 35.0 epoch 100 train loss 0.34327 test loss 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "with open(os.path.join(root_dir, 'nn-adam_1.txt'), 'w') as f:\n",
        "  f.write('908.7\\t0.22178966722613075\\t0.21525660365014973\\t0.7898742852570876\\t0.8009170335610106')\n"
      ],
      "metadata": {
        "id": "1as-1YMBairA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(os.path.join(root_dir, 'nn-adam_2.txt'), 'w') as f:\n",
        "  f.write('613.7\\t0.20875867282307645\\t0.2076561415263884\\t0.7569169343158763\\t0.7523185712858267')\n"
      ],
      "metadata": {
        "id": "7t-mWDQnfcM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(os.path.join(root_dir, 'nn-adam_3.txt'), 'w') as f:\n",
        "  f.write('1707.8\\t0.20732514439508717\\t0.20637711925974497\\t0.7145531167664866\\t0.7127658555963806')\n"
      ],
      "metadata": {
        "id": "71UL4Bb8grR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(os.path.join(root_dir, 'nn-adam_5.txt'), 'w') as f:\n",
        "  f.write('336.8\\t0.23868402388601653\\t0.2375867348327292\\t0.7879139946956261\\t0.7479081176519394')"
      ],
      "metadata": {
        "id": "a4ng8Woti75n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "with open(os.path.join(root_dir, 'nn-noisy-adam_1.txt'), 'w') as f:\n",
        "  f.write('1585.7\\t0.22178382004681163\\t0.21526709048827364\\t0.7890873348487998\\t0.8007987188481653')\n",
        "with open(os.path.join(root_dir, 'nn-noisy-adam_2.txt'), 'w') as f:\n",
        "  f.write('688.7\\t0.2087715198102344\\t0.2076730836105634\\t0.7581730581098988\\t0.753329997495584')\n",
        "with open(os.path.join(root_dir, 'nn-noisy-adam_3.txt'), 'w') as f:\n",
        "  f.write('1886.0\\t0.20733967102055684\\t0.20639045504789366\\t0.7158754984224875\\t0.7137613038457122')\n",
        "with open(os.path.join(root_dir, 'nn-noisy-adam_4.txt'), 'w') as f:\n",
        "  f.write('370.2\\t0.2820037780788805\\t0.27643864661214\\t0.7313284973664718\\t0.698735443462025')\n",
        "with open(os.path.join(root_dir, 'nn-noisy-adam_5.txt'), 'w') as f:\n",
        "  f.write('373.2\\t0.23867869728584454\\t0.2375867845695361\\t0.7867977072210873\\t0.7478160259127616')"
      ],
      "metadata": {
        "id": "IbkSUcSrjhXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = [11, 11, 9, 5, 7]\n",
        "for instance_index in range(4,6):\n",
        "  torch.manual_seed(0)\n",
        "  print('---------- Hotel {} ------------'.format(instance_index))\n",
        "  json_file_name='data_sets/hotel/hotel/instance_{}.json'.format(instance_index)\n",
        "  max_padding_size = ds[instance_index-1]\n",
        "  dataset = JSONChoiceHotelDataset(json_file_name, padding_size=max_padding_size)\n",
        "  dataloader = torch.utils.data.DataLoader(dataset, batch_size=len(dataset))\n",
        "  features, labels = next(iter(dataloader))\n",
        "  device=torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
        "  features = features.to(device)\n",
        "  labels = labels.to(device)\n",
        "  tensor_dataset = TensorDataset(features, labels)\n",
        "  T = len(dataset)\n",
        "  validation_size = 0.0\n",
        "  batch_size = 8\n",
        "  print('batch size:', batch_size)\n",
        "  train_loader, test_loader = train_test_split(tensor_dataset, batch_size=batch_size, validation_split=validation_size, shuffle_dataset=True, random_seed=42)\n",
        "\n",
        "  d=dataset.feature_dim\n",
        "  nmixtures = 10000\n",
        "  print('number of hidden nodes:', nmixtures)\n",
        "  nn_model = NNChoiceModel(d, dataset.nchoices, nmixtures, dataset)\n",
        "  for i in range(20):\n",
        "    nn_model.train(model_name='hotel_{}_instance'.format(instance_index),\n",
        "                   loss_type='SQ',\n",
        "                  batch_size = batch_size, lr=1e-3, opt='adam', #noise=True, weight_decay=1e-9,\n",
        "                  train_loader=train_loader, upper_bound=8, lower_bound=-8,\n",
        "                  learning_rate_decay_step=500000,\n",
        "                  validation_loader = test_loader,\n",
        "                  num_epochs=200, gamma=1, padding=True, padding_size=max_padding_size,use_val=False)\n",
        "\n",
        "    saved_model = nn_model.model\n",
        "\n",
        "    #print('soft rmse', soft_rmse(saved_model, test_dataset, dataset.ground_truth, padding_size=max_padding_size))  # sensitive to suitable batch size & learning rate\n",
        "    rmse = hard_rmse(saved_model, dataset.test_data, dataset.products, padding=True, padding_size=max_padding_size)\n",
        "    in_rmse = hard_rmse(saved_model, dataset.data, dataset.products, padding=True, padding_size = max_padding_size)\n",
        "    saved_model.eval()\n",
        "    with torch.no_grad():\n",
        "      out = saved_model(features)\n",
        "      in_loss = NLL_loss(labels, out).item() / T\n",
        "    loss = hard_nll(saved_model, dataset.test_data, dataset.products, padding=True, padding_size = max_padding_size)\n",
        "\n",
        "\n",
        "    print('out-of-sample rmse', rmse, 'in-sample rmse', in_rmse, 'out-of-sample nll', loss, 'in-sample rmse', in_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKgGLXN9CT-w",
        "outputId": "9e5b4110-25fa-479a-ee98-03ead9902525"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- Hotel 4 ------------\n",
            "batch size: 8\n",
            "number of hidden nodes: 500\n",
            "device cuda:0\n",
            "initial loss 0.92\n",
            "time 19.0 time extra 23.3 epoch 100 train loss 0.34333 test loss 0.0\n",
            "time 37.1 time extra 45.5 epoch 200 train loss 0.34333 test loss -0.0\n",
            "out-of-sample rmse 0.28246863215401485 in-sample rmse 0.2771753662461098 out-of-sample nll -0.7311925578117371 in-sample rmse 0.703739235970107\n",
            "initial loss 0.58\n",
            "time 55.6 time extra 22.7 epoch 100 train loss 0.34604 test loss -0.0\n",
            "time 74.1 time extra 45.4 epoch 200 train loss 0.34333 test loss 0.0\n",
            "out-of-sample rmse 0.2824713847485076 in-sample rmse 0.27716633983908273 out-of-sample nll -0.7317001538926905 in-sample rmse 0.7037131351774389\n",
            "initial loss 0.25\n",
            "time 92.5 time extra 22.9 epoch 100 train loss 0.3433 test loss 0.0\n",
            "time 110.5 time extra 44.9 epoch 200 train loss 0.34326 test loss 0.0\n",
            "out-of-sample rmse 0.28244978833898826 in-sample rmse 0.27714776520674345 out-of-sample nll -0.7315935923836449 in-sample rmse 0.7035960224270821\n",
            "initial loss 0.42\n",
            "time 128.8 time extra 22.4 epoch 100 train loss 0.3445 test loss 0.0\n",
            "time 147.3 time extra 45.2 epoch 200 train loss 0.34573 test loss 0.0\n",
            "out-of-sample rmse 0.2824088994722424 in-sample rmse 0.2771026542557574 out-of-sample nll -0.7313031919436022 in-sample rmse 0.7032443918152289\n",
            "initial loss 0.42\n",
            "time 165.9 time extra 22.9 epoch 100 train loss 0.3443 test loss 0.0\n",
            "time 184.0 time extra 45.1 epoch 200 train loss 0.34287 test loss 0.0\n",
            "out-of-sample rmse 0.2823181445938831 in-sample rmse 0.27699741885519213 out-of-sample nll -0.7309819828380238 in-sample rmse 0.702484937581149\n",
            "initial loss 0.06\n",
            "time 202.3 time extra 22.4 epoch 100 train loss 0.34402 test loss -0.0\n",
            "time 221.2 time extra 45.7 epoch 200 train loss 0.34248 test loss -0.0\n",
            "out-of-sample rmse 0.28219711600229475 in-sample rmse 0.27683705874226266 out-of-sample nll -0.7309850897572258 in-sample rmse 0.7013572732155974\n",
            "initial loss 0.43\n",
            "time 240.0 time extra 23.2 epoch 100 train loss 0.34227 test loss -0.0\n",
            "time 258.7 time extra 46.1 epoch 200 train loss 0.34329 test loss -0.0\n",
            "out-of-sample rmse 0.2820964856698617 in-sample rmse 0.27667843539194326 out-of-sample nll -0.7311545012213967 in-sample rmse 0.7002645426988602\n",
            "initial loss 0.59\n",
            "time 276.9 time extra 22.4 epoch 100 train loss 0.34316 test loss 0.0\n",
            "time 295.6 time extra 45.3 epoch 200 train loss 0.34304 test loss -0.0\n",
            "out-of-sample rmse 0.28203732767364526 in-sample rmse 0.2765600630945929 out-of-sample nll -0.7314042895490473 in-sample rmse 0.6995110934430903\n",
            "initial loss 0.23\n",
            "time 314.4 time extra 23.2 epoch 100 train loss 0.34311 test loss 0.0\n",
            "time 333.2 time extra 46.2 epoch 200 train loss 0.34282 test loss 0.0\n",
            "out-of-sample rmse 0.28201796756784725 in-sample rmse 0.27648128592725335 out-of-sample nll -0.7316530143130909 in-sample rmse 0.6990160092440518\n",
            "initial loss 0.41\n",
            "time 351.6 time extra 22.9 epoch 100 train loss 0.34151 test loss 0.0\n",
            "time 370.3 time extra 45.9 epoch 200 train loss 0.34271 test loss -0.0\n",
            "out-of-sample rmse 0.28201825924806784 in-sample rmse 0.2764290322738004 out-of-sample nll -0.7319163341955706 in-sample rmse 0.698704224283045\n",
            "---------- Hotel 5 ------------\n",
            "batch size: 8\n",
            "number of hidden nodes: 500\n",
            "device cuda:0\n",
            "initial loss 0.86\n",
            "time 17.1 time extra 21.1 epoch 100 train loss 0.34898 test loss 0.0\n",
            "time 33.7 time extra 41.5 epoch 200 train loss 0.3489 test loss 0.0\n",
            "out-of-sample rmse 0.2394514884343012 in-sample rmse 0.2385639474799965 out-of-sample nll -0.7972656003400391 in-sample rmse 0.7645355229079723\n",
            "initial loss 0.27\n",
            "time 50.4 time extra 20.5 epoch 100 train loss 0.34889 test loss -0.0\n",
            "time 67.4 time extra 41.4 epoch 200 train loss 0.34888 test loss 0.0\n",
            "out-of-sample rmse 0.23945882022967693 in-sample rmse 0.238557426872384 out-of-sample nll -0.7983734635745778 in-sample rmse 0.7644134320318698\n",
            "initial loss 0.05\n",
            "time 84.0 time extra 20.4 epoch 100 train loss 0.34886 test loss 0.0\n",
            "time 101.1 time extra 41.4 epoch 200 train loss 0.34885 test loss 0.0\n",
            "out-of-sample rmse 0.23944465469557935 in-sample rmse 0.23854473885471766 out-of-sample nll -0.7984247178423638 in-sample rmse 0.7642989656031132\n",
            "initial loss 0.24\n",
            "time 118.5 time extra 21.5 epoch 100 train loss 0.34881 test loss 0.0\n",
            "time 135.4 time extra 42.3 epoch 200 train loss 0.34875 test loss -0.0\n",
            "out-of-sample rmse 0.239413542938205 in-sample rmse 0.2385107286542923 out-of-sample nll -0.798047263248294 in-sample rmse 0.7638747235238552\n",
            "initial loss 0.43\n",
            "time 152.5 time extra 20.9 epoch 100 train loss 0.34864 test loss 0.0\n",
            "time 169.7 time extra 42.2 epoch 200 train loss 0.34849 test loss 0.0\n",
            "out-of-sample rmse 0.23933732723041928 in-sample rmse 0.23842371399134574 out-of-sample nll -0.7969592072215734 in-sample rmse 0.7626760915219783\n",
            "initial loss 0.06\n",
            "time 186.4 time extra 20.5 epoch 100 train loss 0.34829 test loss 0.0\n",
            "time 203.3 time extra 41.4 epoch 200 train loss 0.34804 test loss 0.0\n",
            "out-of-sample rmse 0.23920308915039157 in-sample rmse 0.23826737661088324 out-of-sample nll -0.7954438719094968 in-sample rmse 0.7604811154007912\n",
            "initial loss 0.25\n",
            "time 220.5 time extra 21.3 epoch 100 train loss 0.34777 test loss 0.0\n",
            "time 237.1 time extra 41.6 epoch 200 train loss 0.34753 test loss 0.0\n",
            "out-of-sample rmse 0.2390597308629558 in-sample rmse 0.23809197142346653 out-of-sample nll -0.794137267739165 in-sample rmse 0.7580630629658699\n",
            "initial loss 0.42\n",
            "time 254.1 time extra 20.9 epoch 100 train loss 0.34731 test loss 0.0\n",
            "time 270.6 time extra 41.2 epoch 200 train loss 0.34712 test loss 0.0\n",
            "out-of-sample rmse 0.23896699466988816 in-sample rmse 0.23795535903946186 out-of-sample nll -0.7935948453697503 in-sample rmse 0.7561843813061714\n",
            "initial loss 0.8\n",
            "time 287.2 time extra 20.4 epoch 100 train loss 0.34697 test loss -0.0\n",
            "time 304.2 time extra 41.3 epoch 200 train loss 0.34687 test loss 0.0\n",
            "out-of-sample rmse 0.23890492742679498 in-sample rmse 0.23786808171216559 out-of-sample nll -0.7932134022899703 in-sample rmse 0.75502408349514\n",
            "initial loss 0.6\n",
            "time 320.8 time extra 20.5 epoch 100 train loss 0.34678 test loss 0.0\n",
            "time 337.7 time extra 41.2 epoch 200 train loss 0.3467 test loss 0.0\n",
            "out-of-sample rmse 0.23886910168674597 in-sample rmse 0.23781301638283509 out-of-sample nll -0.7929544577411577 in-sample rmse 0.754236062169075\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(os.path.join(root_dir, 'nn-adam_4.txt'), 'w') as f:\n",
        "  f.write('370.3\\t0.28201825924806784\\t0.2764290322738004\\t0.7319163341955706\\t0.698704224283045')"
      ],
      "metadata": {
        "id": "-rjmfccXjWQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-jxKgtsbGL_",
        "outputId": "9ee62928-ff45-4c98-9e02-b21708f008e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mbuild\u001b[0m/      estimate.py            \u001b[01;34mpython_choice_models.egg-info\u001b[0m/  \u001b[01;32mrun_all.sh\u001b[0m*\n",
            "\u001b[01;34mdata_sets\u001b[0m/  \u001b[01;34mexamples\u001b[0m/              README.md                       setup.py\n",
            "\u001b[01;34mdist\u001b[0m/       \u001b[01;34mpython_choice_models\u001b[0m/  requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "saved_model = nn_model.model\n",
        "\n",
        "#print('soft rmse', soft_rmse(saved_model, test_dataset, dataset.ground_truth, padding_size=max_padding_size))  # sensitive to suitable batch size & learning rate\n",
        "rmse = hard_rmse(saved_model, dataset.test_data, dataset.products, padding=True, padding_size=max_padding_size)\n",
        "in_rmse = hard_rmse(saved_model, dataset.data, dataset.products, padding=True, padding_size = max_padding_size)\n",
        "\n",
        "print('out-of-sample rmse', rmse, 'in-sample rmse', in_rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSRclFs4IpxW",
        "outputId": "130fd5cf-9ed1-46bf-bb7e-6944b3504698"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "out-of-sample rmse 0.22180479764750835 in-sample rmse 0.21527642486415077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rmse = hard_rmse(saved_model, dataset.test_data, dataset.products, padding=True, padding_size=max_padding_size)\n",
        "in_rmse = hard_rmse(saved_model, dataset.data, dataset.products, padding=True, padding_size = max_padding_size)\n",
        "print(rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zp4pPIYxTedp",
        "outputId": "67b8a152-6ab8-4854-edc8-8f1c385c3e5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.22214996105063317\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds = [11, 11, 9, 5, 7]\n",
        "for instance_index in range(2, 6):\n",
        "  torch.manual_seed(0)\n",
        "  print('---------- Hotel {} ------------'.format(instance_index))\n",
        "  json_file_name='data_sets/hotel/hotel/instance_{}.json'.format(instance_index)\n",
        "  max_padding_size = ds[instance_index-1]\n",
        "  dataset = JSONChoiceHotelDataset(json_file_name, padding_size=max_padding_size)\n",
        "  dataloader = torch.utils.data.DataLoader(dataset, batch_size=len(dataset))\n",
        "  features, labels = next(iter(dataloader))\n",
        "  device=torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
        "  tensor_dataset = TensorDataset(features.to(device), labels.to(device))\n",
        "  T = len(dataset)\n",
        "  validation_size = 0.0\n",
        "  batch_size = 8\n",
        "  print('batch size:', batch_size)\n",
        "  train_loader, test_loader = train_test_split(tensor_dataset, batch_size=batch_size, validation_split=validation_size, shuffle_dataset=True, random_seed=42)\n",
        "\n",
        "  d=dataset.feature_dim\n",
        "  nmixtures = 500\n",
        "  print('number of hidden nodes:', nmixtures)\n",
        "  nn_model = NNChoiceModel(d, dataset.nchoices, nmixtures, dataset)\n",
        "  nn_model.train(model_name='hotel_{}_instance'.format(instance_index),\n",
        "                batch_size = batch_size, lr=3, opt='sgd', #noise=True, weight_decay=1e-9,\n",
        "                train_loader=train_loader, upper_bound=8, lower_bound=-8,\n",
        "                learning_rate_decay_step=500000,\n",
        "                validation_loader = test_loader,\n",
        "                num_epochs=3000, gamma=0.99, padding=True, padding_size=max_padding_size,use_val=False)\n",
        "\n",
        "  saved_model = nn_model.model\n",
        "\n",
        "  #print('soft rmse', soft_rmse(saved_model, test_dataset, dataset.ground_truth, padding_size=max_padding_size))  # sensitive to suitable batch size & learning rate\n",
        "  rmse = hard_rmse(saved_model, dataset.test_data, dataset.products, padding=True, padding_size=max_padding_size)\n",
        "  print('soft rmse known model', rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2J0ok50qZpFl",
        "outputId": "643b6b4f-f028-40a3-bc37-ceedf157974f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- Hotel 2 ------------\n",
            "batch size: 8\n",
            "number of hidden nodes: 500\n",
            "device cuda:0\n",
            "initial loss 2.67\n",
            "time 39.3 time extra 45.4 epoch 100 train loss 0.79876 test loss 0.0\n",
            "time 79.0 time extra 91.3 epoch 200 train loss 0.77906 test loss 0.0\n",
            "time 119.2 time extra 137.6 epoch 300 train loss 0.77736 test loss 0.0\n",
            "time 158.3 time extra 182.6 epoch 400 train loss 0.77775 test loss 0.0\n",
            "time 198.1 time extra 228.5 epoch 500 train loss 0.77526 test loss 0.0\n",
            "time 238.5 time extra 275.0 epoch 600 train loss 0.77276 test loss 0.0\n",
            "time 277.6 time extra 320.1 epoch 700 train loss 0.77259 test loss 0.0\n",
            "time 317.2 time extra 365.7 epoch 800 train loss 0.77179 test loss 0.0\n",
            "time 356.9 time extra 411.5 epoch 900 train loss 0.77178 test loss 0.0\n",
            "time 396.5 time extra 457.3 epoch 1000 train loss 0.77281 test loss 0.0\n",
            "time 436.2 time extra 503.1 epoch 1100 train loss 0.76958 test loss 0.0\n",
            "time 475.6 time extra 548.6 epoch 1200 train loss 0.76937 test loss 0.0\n",
            "time 515.1 time extra 594.1 epoch 1300 train loss 0.7671 test loss 0.0\n",
            "time 554.3 time extra 639.3 epoch 1400 train loss 0.76697 test loss -0.0\n",
            "time 594.2 time extra 685.5 epoch 1500 train loss 0.76572 test loss 0.0\n",
            "time 633.8 time extra 731.1 epoch 1600 train loss 0.76532 test loss 0.0\n",
            "time 672.7 time extra 776.0 epoch 1700 train loss 0.76484 test loss 0.0\n",
            "time 712.8 time extra 822.3 epoch 1800 train loss 0.76458 test loss -0.0\n",
            "time 752.0 time extra 867.4 epoch 1900 train loss 0.76554 test loss -0.0\n",
            "time 791.0 time extra 912.3 epoch 2000 train loss 0.76419 test loss 0.0\n",
            "time 830.5 time extra 958.0 epoch 2100 train loss 0.76466 test loss -0.0\n",
            "time 869.8 time extra 1003.2 epoch 2200 train loss 0.76384 test loss 0.0\n",
            "time 909.4 time extra 1049.0 epoch 2300 train loss 0.76373 test loss 0.0\n",
            "time 948.4 time extra 1093.9 epoch 2400 train loss 0.76357 test loss 0.0\n",
            "time 988.4 time extra 1140.1 epoch 2500 train loss 0.76456 test loss 0.0\n",
            "time 1027.3 time extra 1184.8 epoch 2600 train loss 0.76322 test loss 0.0\n",
            "time 1066.6 time extra 1230.2 epoch 2700 train loss 0.76357 test loss 0.0\n",
            "time 1106.6 time extra 1276.4 epoch 2800 train loss 0.76379 test loss -0.0\n",
            "time 1145.6 time extra 1321.3 epoch 2900 train loss 0.76189 test loss 0.0\n",
            "time 1185.1 time extra 1366.8 epoch 3000 train loss 0.76099 test loss 0.0\n",
            "soft rmse known model 0.20919190134503096\n",
            "---------- Hotel 3 ------------\n",
            "batch size: 8\n",
            "number of hidden nodes: 500\n",
            "device cuda:0\n",
            "initial loss 2.46\n",
            "time 108.2 time extra 124.9 epoch 100 train loss 0.73578 test loss -0.0\n",
            "time 216.6 time extra 250.0 epoch 200 train loss 0.73053 test loss 0.0\n",
            "time 325.0 time extra 375.2 epoch 300 train loss 0.72713 test loss -0.0\n",
            "time 433.1 time extra 500.0 epoch 400 train loss 0.72504 test loss 0.0\n",
            "time 541.5 time extra 625.2 epoch 500 train loss 0.72261 test loss 0.0\n",
            "time 649.3 time extra 749.6 epoch 600 train loss 0.72097 test loss 0.0\n",
            "time 757.0 time extra 874.0 epoch 700 train loss 0.71971 test loss -0.0\n",
            "time 865.1 time extra 998.8 epoch 800 train loss 0.71836 test loss -0.0\n",
            "time 972.8 time extra 1123.2 epoch 900 train loss 0.71772 test loss -0.0\n",
            "time 1081.5 time extra 1248.7 epoch 1000 train loss 0.71743 test loss -0.0\n",
            "time 1190.6 time extra 1374.7 epoch 1100 train loss 0.71689 test loss -0.0\n",
            "time 1302.3 time extra 1503.7 epoch 1200 train loss 0.71656 test loss 0.0\n",
            "time 1414.4 time extra 1633.3 epoch 1300 train loss 0.71577 test loss -0.0\n",
            "time 1525.6 time extra 1761.9 epoch 1400 train loss 0.71572 test loss -0.0\n",
            "time 1638.3 time extra 1892.2 epoch 1500 train loss 0.71561 test loss 0.0\n",
            "time 1748.9 time extra 2020.0 epoch 1600 train loss 0.7154 test loss 0.0\n",
            "time 1858.3 time extra 2146.4 epoch 1700 train loss 0.71542 test loss 0.0\n",
            "time 1967.9 time extra 2273.1 epoch 1800 train loss 0.71584 test loss 0.0\n",
            "time 2077.9 time extra 2400.2 epoch 1900 train loss 0.71504 test loss 0.0\n",
            "time 2187.7 time extra 2527.1 epoch 2000 train loss 0.71509 test loss 0.0\n",
            "time 2297.8 time extra 2654.3 epoch 2100 train loss 0.71493 test loss 0.0\n",
            "time 2406.7 time extra 2780.0 epoch 2200 train loss 0.71477 test loss 0.0\n",
            "time 2516.1 time extra 2906.4 epoch 2300 train loss 0.71494 test loss 0.0\n",
            "time 2625.3 time extra 3032.5 epoch 2400 train loss 0.71443 test loss 0.0\n",
            "time 2734.3 time extra 3158.4 epoch 2500 train loss 0.7142 test loss 0.0\n",
            "time 2843.7 time extra 3284.8 epoch 2600 train loss 0.71429 test loss 0.0\n",
            "time 2952.9 time extra 3411.0 epoch 2700 train loss 0.71422 test loss 0.0\n",
            "time 3063.7 time extra 3539.1 epoch 2800 train loss 0.7142 test loss 0.0\n",
            "time 3174.5 time extra 3667.2 epoch 2900 train loss 0.71414 test loss 0.0\n",
            "time 3285.6 time extra 3795.5 epoch 3000 train loss 0.71438 test loss 0.0\n",
            "soft rmse known model 0.2074594171047411\n",
            "---------- Hotel 4 ------------\n",
            "batch size: 8\n",
            "number of hidden nodes: 500\n",
            "device cuda:0\n",
            "initial loss 1.83\n",
            "time 24.8 time extra 28.7 epoch 100 train loss 0.70518 test loss -0.0\n",
            "time 48.8 time extra 56.4 epoch 200 train loss 0.70216 test loss 0.0\n",
            "time 72.6 time extra 83.9 epoch 300 train loss 0.70565 test loss -0.0\n",
            "time 96.5 time extra 111.4 epoch 400 train loss 0.70185 test loss 0.0\n",
            "time 120.3 time extra 138.8 epoch 500 train loss 0.70178 test loss 0.0\n",
            "time 144.1 time extra 166.5 epoch 600 train loss 0.70164 test loss -0.0\n",
            "time 169.0 time extra 195.3 epoch 700 train loss 0.70348 test loss 0.0\n",
            "time 192.9 time extra 222.8 epoch 800 train loss 0.70457 test loss 0.0\n",
            "time 216.7 time extra 250.3 epoch 900 train loss 0.70269 test loss 0.0\n",
            "time 240.6 time extra 277.8 epoch 1000 train loss 0.70016 test loss -0.0\n",
            "time 264.5 time extra 305.4 epoch 1100 train loss 0.70212 test loss 0.0\n",
            "time 289.2 time extra 334.0 epoch 1200 train loss 0.69946 test loss 0.0\n",
            "time 312.7 time extra 361.2 epoch 1300 train loss 0.69927 test loss 0.0\n",
            "time 336.3 time extra 388.4 epoch 1400 train loss 0.70074 test loss 0.0\n",
            "time 360.0 time extra 415.7 epoch 1500 train loss 0.70067 test loss 0.0\n",
            "time 383.7 time extra 443.0 epoch 1600 train loss 0.70055 test loss 0.0\n",
            "time 407.5 time extra 470.5 epoch 1700 train loss 0.70164 test loss -0.0\n",
            "time 432.0 time extra 498.8 epoch 1800 train loss 0.70016 test loss 0.0\n",
            "time 455.8 time extra 526.3 epoch 1900 train loss 0.69833 test loss -0.0\n",
            "time 479.7 time extra 553.9 epoch 2000 train loss 0.70002 test loss -0.0\n",
            "time 503.5 time extra 581.3 epoch 2100 train loss 0.69809 test loss -0.0\n",
            "time 527.3 time extra 608.9 epoch 2200 train loss 0.6997 test loss 0.0\n",
            "time 552.1 time extra 637.6 epoch 2300 train loss 0.70137 test loss 0.0\n",
            "time 575.8 time extra 664.9 epoch 2400 train loss 0.69961 test loss 0.0\n",
            "time 599.7 time extra 692.5 epoch 2500 train loss 0.70014 test loss 0.0\n",
            "time 623.5 time extra 719.9 epoch 2600 train loss 0.69772 test loss 0.0\n",
            "time 647.3 time extra 747.4 epoch 2700 train loss 0.69758 test loss 0.0\n",
            "time 671.3 time extra 775.1 epoch 2800 train loss 0.69763 test loss 0.0\n",
            "time 695.9 time extra 803.5 epoch 2900 train loss 0.70236 test loss 0.0\n",
            "time 720.1 time extra 831.4 epoch 3000 train loss 0.69918 test loss 0.0\n",
            "soft rmse known model 0.2820973331415316\n",
            "---------- Hotel 5 ------------\n",
            "batch size: 8\n",
            "number of hidden nodes: 500\n",
            "device cuda:0\n",
            "initial loss 1.97\n",
            "time 21.6 time extra 24.9 epoch 100 train loss 0.78094 test loss 0.0\n",
            "time 43.2 time extra 49.9 epoch 200 train loss 0.76573 test loss 0.0\n",
            "time 64.6 time extra 74.6 epoch 300 train loss 0.76401 test loss 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds = [11, 11, 9, 5, 7]\n",
        "for instance_index in range(1, 6):\n",
        "  torch.manual_seed(0)\n",
        "  print('---------- Hotel {} ------------'.format(instance_index))\n",
        "  json_file_name='data_sets/hotel/hotel/instance_{}.json'.format(instance_index)\n",
        "  max_padding_size = ds[instance_index-1]\n",
        "  dataset = JSONChoiceHotelDataset(json_file_name, padding_size=max_padding_size)\n",
        "  dataloader = torch.utils.data.DataLoader(dataset, batch_size=len(dataset))\n",
        "  features, labels = next(iter(dataloader))\n",
        "  tensor_dataset = TensorDataset(features, labels)\n",
        "  T = len(dataset)\n",
        "  validation_size = 0.0\n",
        "  batch_size = 8\n",
        "  print('batch size:', batch_size)\n",
        "  train_loader, test_loader = train_test_split(tensor_dataset, batch_size=batch_size, validation_split=validation_size, shuffle_dataset=True, random_seed=42)\n",
        "\n",
        "  d=dataset.feature_dim\n",
        "  nmixtures = 500\n",
        "  print('number of hidden nodes:', nmixtures)\n",
        "  nn_model = NNChoiceModel(d, dataset.nchoices, nmixtures, dataset)\n",
        "  nn_model.train(model_name='hotel_{}_instance'.format(instance_index),\n",
        "                batch_size = batch_size, lr=3, opt='sgd', noise=True, weight_decay=1e-9,\n",
        "                train_loader=train_loader, upper_bound=8, lower_bound=-8,\n",
        "                learning_rate_decay_step=500000,\n",
        "                validation_loader = test_loader,\n",
        "                num_epochs=3000, gamma=0.99, padding=True, padding_size=max_padding_size,use_val=False)\n",
        "\n",
        "  saved_model = nn_model.model\n",
        "\n",
        "  #print('soft rmse', soft_rmse(saved_model, test_dataset, dataset.ground_truth, padding_size=max_padding_size))  # sensitive to suitable batch size & learning rate\n",
        "  rmse = hard_rmse(saved_model, dataset.test_data, dataset.products, padding=True, padding_size=max_padding_size)\n",
        "  print('soft rmse known model', rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNczND_ms8QF",
        "outputId": "d570affe-c9c5-4bb4-fef7-fbc4c8f1aa11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- Hotel 1 ------------\n",
            "batch size: 8\n",
            "number of hidden nodes: 500\n",
            "device cpu\n",
            "initial loss 2.39\n",
            "time 130.0 time extra 148.4 epoch 100 train loss 0.83343 test loss 0.0\n",
            "time 259.6 time extra 295.9 epoch 200 train loss 0.82214 test loss -0.0\n",
            "time 388.4 time extra 442.5 epoch 300 train loss 0.81736 test loss 0.0\n",
            "time 516.2 time extra 588.2 epoch 400 train loss 0.81627 test loss -0.0\n",
            "time 643.9 time extra 733.8 epoch 500 train loss 0.8142 test loss -0.0\n",
            "time 771.9 time extra 879.6 epoch 600 train loss 0.81377 test loss -0.0\n",
            "time 901.4 time extra 1027.4 epoch 700 train loss 0.80942 test loss -0.0\n",
            "time 1029.7 time extra 1173.6 epoch 800 train loss 0.80884 test loss 0.0\n",
            "time 1158.2 time extra 1319.9 epoch 900 train loss 0.80802 test loss -0.0\n",
            "time 1285.9 time extra 1465.5 epoch 1000 train loss 0.80875 test loss -0.0\n",
            "time 1413.6 time extra 1611.0 epoch 1100 train loss 0.80858 test loss 0.0\n",
            "time 1541.7 time extra 1757.2 epoch 1200 train loss 0.80708 test loss -0.0\n",
            "time 1668.9 time extra 1902.4 epoch 1300 train loss 0.80676 test loss 0.0\n",
            "time 1796.3 time extra 2047.4 epoch 1400 train loss 0.80756 test loss 0.0\n",
            "time 1925.3 time extra 2194.3 epoch 1500 train loss 0.80963 test loss -0.0\n",
            "time 2053.7 time extra 2341.0 epoch 1600 train loss 0.80776 test loss -0.0\n",
            "time 2182.0 time extra 2487.0 epoch 1700 train loss 0.80596 test loss -0.0\n",
            "time 2308.0 time extra 2630.5 epoch 1800 train loss 0.80604 test loss 0.0\n",
            "time 2436.2 time extra 2776.8 epoch 1900 train loss 0.80575 test loss -0.0\n",
            "time 2567.7 time extra 2926.8 epoch 2000 train loss 0.80557 test loss -0.0\n",
            "time 2697.8 time extra 3075.2 epoch 2100 train loss 0.80571 test loss -0.0\n",
            "time 2827.7 time extra 3223.6 epoch 2200 train loss 0.80555 test loss -0.0\n",
            "time 2956.5 time extra 3370.3 epoch 2300 train loss 0.80519 test loss -0.0\n",
            "time 3085.2 time extra 3516.9 epoch 2400 train loss 0.80514 test loss -0.0\n",
            "time 3212.9 time extra 3662.7 epoch 2500 train loss 0.80511 test loss 0.0\n",
            "time 3339.7 time extra 3806.9 epoch 2600 train loss 0.80634 test loss 0.0\n",
            "time 3465.8 time extra 3950.4 epoch 2700 train loss 0.80478 test loss -0.0\n",
            "time 3592.5 time extra 4094.9 epoch 2800 train loss 0.8066 test loss -0.0\n",
            "time 3719.1 time extra 4239.0 epoch 2900 train loss 0.80652 test loss -0.0\n",
            "time 3846.1 time extra 4383.5 epoch 3000 train loss 0.80615 test loss -0.0\n",
            "soft rmse known model 0.2222094688763831\n",
            "---------- Hotel 2 ------------\n",
            "batch size: 8\n",
            "number of hidden nodes: 500\n",
            "device cpu\n",
            "initial loss 2.67\n",
            "time 43.0 time extra 48.9 epoch 100 train loss 0.80293 test loss -0.0\n",
            "time 85.1 time extra 96.6 epoch 200 train loss 0.78212 test loss -0.0\n",
            "time 128.4 time extra 145.7 epoch 300 train loss 0.77526 test loss -0.0\n",
            "time 171.6 time extra 194.5 epoch 400 train loss 0.77151 test loss -0.0\n",
            "time 213.6 time extra 242.1 epoch 500 train loss 0.7699 test loss -0.0\n",
            "time 256.6 time extra 290.9 epoch 600 train loss 0.77159 test loss -0.0\n",
            "time 299.2 time extra 339.1 epoch 700 train loss 0.76609 test loss -0.0\n",
            "time 341.5 time extra 387.0 epoch 800 train loss 0.76513 test loss -0.0\n",
            "time 384.3 time extra 435.4 epoch 900 train loss 0.76439 test loss 0.0\n",
            "time 426.3 time extra 482.9 epoch 1000 train loss 0.7649 test loss 0.0\n",
            "time 469.2 time extra 531.9 epoch 1100 train loss 0.76394 test loss -0.0\n",
            "time 512.2 time extra 580.5 epoch 1200 train loss 0.76289 test loss -0.0\n",
            "time 554.1 time extra 627.9 epoch 1300 train loss 0.763 test loss -0.0\n",
            "time 596.8 time extra 676.3 epoch 1400 train loss 0.76272 test loss 0.0\n",
            "time 639.0 time extra 724.1 epoch 1500 train loss 0.76224 test loss -0.0\n",
            "time 681.6 time extra 772.7 epoch 1600 train loss 0.76148 test loss -0.0\n",
            "time 724.9 time extra 821.6 epoch 1700 train loss 0.76244 test loss -0.0\n",
            "time 766.7 time extra 869.0 epoch 1800 train loss 0.76231 test loss 0.0\n",
            "time 809.6 time extra 917.5 epoch 1900 train loss 0.76138 test loss 0.0\n",
            "time 852.3 time extra 966.0 epoch 2000 train loss 0.7617 test loss -0.0\n",
            "time 894.4 time extra 1013.6 epoch 2100 train loss 0.76146 test loss 0.0\n",
            "time 937.5 time extra 1062.4 epoch 2200 train loss 0.76019 test loss -0.0\n",
            "time 979.9 time extra 1110.5 epoch 2300 train loss 0.76258 test loss 0.0\n",
            "time 1022.4 time extra 1158.6 epoch 2400 train loss 0.76033 test loss -0.0\n",
            "time 1065.3 time extra 1207.5 epoch 2500 train loss 0.76047 test loss 0.0\n",
            "time 1107.4 time extra 1255.1 epoch 2600 train loss 0.75895 test loss -0.0\n",
            "time 1150.3 time extra 1303.7 epoch 2700 train loss 0.75972 test loss -0.0\n",
            "time 1193.1 time extra 1352.2 epoch 2800 train loss 0.76102 test loss -0.0\n",
            "time 1235.3 time extra 1400.0 epoch 2900 train loss 0.76038 test loss 0.0\n",
            "time 1278.2 time extra 1448.9 epoch 3000 train loss 0.75862 test loss 0.0\n",
            "soft rmse known model 0.2091033657805257\n",
            "---------- Hotel 3 ------------\n",
            "batch size: 8\n",
            "number of hidden nodes: 500\n",
            "device cpu\n",
            "initial loss 2.46\n",
            "time 94.1 time extra 110.0 epoch 100 train loss 0.73423 test loss 0.0\n",
            "time 189.4 time extra 221.7 epoch 200 train loss 0.72683 test loss -0.0\n",
            "time 283.6 time extra 331.9 epoch 300 train loss 0.72258 test loss 0.0\n",
            "time 378.1 time extra 442.5 epoch 400 train loss 0.72007 test loss -0.0\n",
            "time 473.5 time extra 554.4 epoch 500 train loss 0.71889 test loss 0.0\n",
            "time 567.6 time extra 664.6 epoch 600 train loss 0.71856 test loss -0.0\n",
            "time 661.9 time extra 774.9 epoch 700 train loss 0.71795 test loss 0.0\n",
            "time 757.3 time extra 887.0 epoch 800 train loss 0.71739 test loss 0.0\n",
            "time 851.4 time extra 997.2 epoch 900 train loss 0.71732 test loss 0.0\n",
            "time 945.9 time extra 1107.8 epoch 1000 train loss 0.7174 test loss 0.0\n",
            "time 1042.0 time extra 1220.6 epoch 1100 train loss 0.717 test loss 0.0\n",
            "time 1137.5 time extra 1332.5 epoch 1200 train loss 0.71693 test loss 0.0\n",
            "time 1233.9 time extra 1445.5 epoch 1300 train loss 0.71699 test loss 0.0\n",
            "time 1329.2 time extra 1557.5 epoch 1400 train loss 0.7172 test loss -0.0\n",
            "time 1425.1 time extra 1670.0 epoch 1500 train loss 0.7166 test loss -0.0\n",
            "time 1520.2 time extra 1781.3 epoch 1600 train loss 0.71646 test loss 0.0\n",
            "time 1616.0 time extra 1893.8 epoch 1700 train loss 0.71673 test loss -0.0\n",
            "time 1711.1 time extra 2005.5 epoch 1800 train loss 0.7165 test loss -0.0\n",
            "time 1806.5 time extra 2117.1 epoch 1900 train loss 0.71608 test loss 0.0\n",
            "time 1902.6 time extra 2230.0 epoch 2000 train loss 0.71653 test loss -0.0\n",
            "time 1998.1 time extra 2341.9 epoch 2100 train loss 0.71626 test loss -0.0\n",
            "time 2094.1 time extra 2454.3 epoch 2200 train loss 0.71605 test loss -0.0\n",
            "time 2189.2 time extra 2566.1 epoch 2300 train loss 0.71598 test loss -0.0\n",
            "time 2285.1 time extra 2678.5 epoch 2400 train loss 0.7161 test loss -0.0\n",
            "time 2379.5 time extra 2789.0 epoch 2500 train loss 0.7164 test loss -0.0\n",
            "time 2473.9 time extra 2900.0 epoch 2600 train loss 0.71607 test loss -0.0\n",
            "time 2569.8 time extra 3012.4 epoch 2700 train loss 0.71621 test loss -0.0\n",
            "time 2664.4 time extra 3123.1 epoch 2800 train loss 0.71581 test loss -0.0\n",
            "time 2760.1 time extra 3235.5 epoch 2900 train loss 0.7159 test loss -0.0\n",
            "time 2854.8 time extra 3346.2 epoch 3000 train loss 0.71639 test loss -0.0\n",
            "soft rmse known model 0.20747606946633793\n",
            "---------- Hotel 4 ------------\n",
            "batch size: 8\n",
            "number of hidden nodes: 500\n",
            "device cpu\n",
            "initial loss 1.83\n",
            "time 18.0 time extra 21.2 epoch 100 train loss 0.70629 test loss -0.0\n",
            "time 35.2 time extra 41.6 epoch 200 train loss 0.70539 test loss 0.0\n",
            "time 53.3 time extra 62.9 epoch 300 train loss 0.70168 test loss -0.0\n",
            "time 70.9 time extra 83.8 epoch 400 train loss 0.70314 test loss 0.0\n",
            "time 88.5 time extra 104.6 epoch 500 train loss 0.70068 test loss -0.0\n",
            "time 106.5 time extra 126.1 epoch 600 train loss 0.70197 test loss -0.0\n",
            "time 123.6 time extra 146.4 epoch 700 train loss 0.69954 test loss 0.0\n",
            "time 141.6 time extra 167.5 epoch 800 train loss 0.69921 test loss -0.0\n",
            "time 158.7 time extra 187.8 epoch 900 train loss 0.70084 test loss -0.0\n",
            "time 176.5 time extra 208.9 epoch 1000 train loss 0.70027 test loss -0.0\n",
            "time 193.9 time extra 229.4 epoch 1100 train loss 0.69844 test loss -0.0\n",
            "time 211.5 time extra 250.2 epoch 1200 train loss 0.70126 test loss 0.0\n",
            "time 229.4 time extra 271.3 epoch 1300 train loss 0.69811 test loss -0.0\n",
            "time 246.6 time extra 291.6 epoch 1400 train loss 0.69785 test loss -0.0\n",
            "time 264.5 time extra 312.8 epoch 1500 train loss 0.69946 test loss -0.0\n",
            "time 281.6 time extra 333.0 epoch 1600 train loss 0.70003 test loss 0.0\n",
            "time 299.5 time extra 354.1 epoch 1700 train loss 0.69932 test loss -0.0\n",
            "time 316.8 time extra 374.5 epoch 1800 train loss 0.70084 test loss 0.0\n",
            "time 334.6 time extra 395.6 epoch 1900 train loss 0.6991 test loss -0.0\n",
            "time 352.7 time extra 417.0 epoch 2000 train loss 0.69726 test loss 0.0\n",
            "time 369.9 time extra 437.3 epoch 2100 train loss 0.697 test loss -0.0\n",
            "time 387.9 time extra 458.5 epoch 2200 train loss 0.69707 test loss -0.0\n",
            "time 405.0 time extra 478.7 epoch 2300 train loss 0.69879 test loss -0.0\n",
            "time 422.9 time extra 499.9 epoch 2400 train loss 0.69862 test loss -0.0\n",
            "time 440.4 time extra 520.5 epoch 2500 train loss 0.70033 test loss -0.0\n",
            "time 458.2 time extra 542.1 epoch 2600 train loss 0.69673 test loss -0.0\n",
            "time 476.2 time extra 563.3 epoch 2700 train loss 0.69677 test loss -0.0\n",
            "time 493.3 time extra 583.4 epoch 2800 train loss 0.69666 test loss -0.0\n",
            "time 511.3 time extra 604.7 epoch 2900 train loss 0.70247 test loss 0.0\n",
            "time 528.4 time extra 625.0 epoch 3000 train loss 0.7001 test loss 0.0\n",
            "soft rmse known model 0.281990416476335\n",
            "---------- Hotel 5 ------------\n",
            "batch size: 8\n",
            "number of hidden nodes: 500\n",
            "device cpu\n",
            "initial loss 1.97\n",
            "time 17.8 time extra 20.9 epoch 100 train loss 0.78278 test loss 0.0\n",
            "time 35.0 time extra 40.9 epoch 200 train loss 0.76686 test loss -0.0\n",
            "time 52.8 time extra 61.7 epoch 300 train loss 0.76434 test loss -0.0\n",
            "time 70.1 time extra 81.9 epoch 400 train loss 0.76355 test loss -0.0\n",
            "time 87.5 time extra 102.3 epoch 500 train loss 0.76286 test loss 0.0\n",
            "time 105.0 time extra 122.8 epoch 600 train loss 0.76171 test loss -0.0\n",
            "time 122.1 time extra 142.8 epoch 700 train loss 0.76111 test loss 0.0\n",
            "time 139.9 time extra 163.5 epoch 800 train loss 0.76019 test loss -0.0\n",
            "time 156.7 time extra 183.1 epoch 900 train loss 0.75933 test loss -0.0\n",
            "time 174.4 time extra 203.8 epoch 1000 train loss 0.75834 test loss 0.0\n",
            "time 191.1 time extra 223.3 epoch 1100 train loss 0.75755 test loss -0.0\n",
            "time 208.9 time extra 244.0 epoch 1200 train loss 0.7568 test loss -0.0\n",
            "time 225.8 time extra 263.8 epoch 1300 train loss 0.75628 test loss -0.0\n",
            "time 243.6 time extra 284.9 epoch 1400 train loss 0.75584 test loss -0.0\n",
            "time 260.5 time extra 304.7 epoch 1500 train loss 0.75562 test loss -0.0\n",
            "time 278.2 time extra 325.4 epoch 1600 train loss 0.7552 test loss -0.0\n",
            "time 295.2 time extra 345.2 epoch 1700 train loss 0.75505 test loss 0.0\n",
            "time 312.9 time extra 366.0 epoch 1800 train loss 0.75477 test loss -0.0\n",
            "time 329.6 time extra 385.5 epoch 1900 train loss 0.75458 test loss 0.0\n",
            "time 347.3 time extra 406.2 epoch 2000 train loss 0.75444 test loss -0.0\n",
            "time 364.4 time extra 426.1 epoch 2100 train loss 0.75434 test loss 0.0\n",
            "time 382.0 time extra 446.6 epoch 2200 train loss 0.75416 test loss -0.0\n",
            "time 399.3 time extra 466.9 epoch 2300 train loss 0.75392 test loss -0.0\n",
            "time 416.5 time extra 487.0 epoch 2400 train loss 0.75357 test loss -0.0\n",
            "time 433.9 time extra 507.3 epoch 2500 train loss 0.75339 test loss -0.0\n",
            "time 451.0 time extra 527.2 epoch 2600 train loss 0.75323 test loss 0.0\n",
            "time 468.7 time extra 547.9 epoch 2700 train loss 0.75313 test loss -0.0\n",
            "time 485.7 time extra 567.9 epoch 2800 train loss 0.75302 test loss -0.0\n",
            "time 503.6 time extra 588.8 epoch 2900 train loss 0.75282 test loss 0.0\n",
            "time 520.7 time extra 608.8 epoch 3000 train loss 0.75276 test loss -0.0\n",
            "soft rmse known model 0.23903679429647104\n"
          ]
        }
      ]
    }
  ]
}